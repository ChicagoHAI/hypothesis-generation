{
    "paper_id": "Batista_jmp_generating-hypotheses",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-09-10T13:07:29.721394Z"
    },
    "title": "Words that Work: Using Language to Generate Hypotheses",
    "authors": [
        {
            "first": "Rafael",
            "middle": [
                "M"
            ],
            "last": "Batista",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Chicago Booth School of Business",
                "location": {}
            },
            "email": "rafael.batista@chicagobooth.edu"
        },
        {
            "first": "James",
            "middle": [],
            "last": "Ross",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Chicago Booth School of Business",
                "location": {}
            },
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "In this paper, we examine how specific features of language drive consumer behavior. Our contribution, however, lies not in testing specific hypotheses; rather, it is in demonstrating a data-driven process for generating them. We devise an approach that generates interpretable hypotheses from text by integrating large-language models (LLMs), machine learning (ML), and psychology experiments. Using a dataset with over 60,000 headlines (and over 32,000 A/B tests), we produce human-interpretable hypotheses about what features of language might a\u21b5ect engagement. We then test a subset of these hypotheses out-of-sample using two datasets: one consisting of 1,600 A/B tests and another containing over 5,000 social media posts. Our approach indeed facilitates discovery. For instance, we find that describing physical reactions significantly increases engagement. In contrast, focusing on positive aspects of human behavior decreases it. A third hypothesis posited that referring to multimedia (e.g., GIFs, videos) would influence engagement, and it does, only it significantly increases engagement in one domain while significantly decreasing it in another. This approach extends beyond a single application. In general, it o\u21b5ers a data-driven method for discovery that can convert unstructured text data into insights that are interpretable, novel, testable, and generalizable. It does so while maintaining a transparent role for both human researchers and algorithmic processes. This approach o\u21b5ers a practical tool to researchers, organizations, and policymakers seeking to aggregate insights from multiple marketing experiments.",
    "pdf_parse": {
        "paper_id": "Batista_jmp_generating-hypotheses",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "In this paper, we examine how specific features of language drive consumer behavior. Our contribution, however, lies not in testing specific hypotheses; rather, it is in demonstrating a data-driven process for generating them. We devise an approach that generates interpretable hypotheses from text by integrating large-language models (LLMs), machine learning (ML), and psychology experiments. Using a dataset with over 60,000 headlines (and over 32,000 A/B tests), we produce human-interpretable hypotheses about what features of language might a\u21b5ect engagement. We then test a subset of these hypotheses out-of-sample using two datasets: one consisting of 1,600 A/B tests and another containing over 5,000 social media posts. Our approach indeed facilitates discovery. For instance, we find that describing physical reactions significantly increases engagement. In contrast, focusing on positive aspects of human behavior decreases it. A third hypothesis posited that referring to multimedia (e.g., GIFs, videos) would influence engagement, and it does, only it significantly increases engagement in one domain while significantly decreasing it in another. This approach extends beyond a single application. In general, it o\u21b5ers a data-driven method for discovery that can convert unstructured text data into insights that are interpretable, novel, testable, and generalizable. It does so while maintaining a transparent role for both human researchers and algorithmic processes. This approach o\u21b5ers a practical tool to researchers, organizations, and policymakers seeking to aggregate insights from multiple marketing experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Language is core to consumer behavior; not only as a topic to study consumers' decision processes (Pogacar et al. 2022; Packard and Berger 2024; Berger and Packard 2023 ), but also as data, providing researchers and practitioners alike a rich source of insights about customers and companies (Berger et al. 2020; Humphreys and Wang 2018) . This paper uses language as data to investigate what motivates consumers to engage with a message.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 119,
                        "text": "(Pogacar et al. 2022;",
                        "ref_id": "BIBREF104"
                    },
                    {
                        "start": 120,
                        "end": 144,
                        "text": "Packard and Berger 2024;",
                        "ref_id": "BIBREF99"
                    },
                    {
                        "start": 145,
                        "end": 168,
                        "text": "Berger and Packard 2023",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 292,
                        "end": 312,
                        "text": "(Berger et al. 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 313,
                        "end": 337,
                        "text": "Humphreys and Wang 2018)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "Many papers in marketing have explored the e\u21b5ects of language on engagement. Much of this research examines what features of text draw consumers' attention (e.g., Banerjee and Urminsky 2023; Bruce, Murthi, and Rao 2017; Kanuri, Chen, and Sridhar 2018; Zor, Kim, and Monga 2022; Robertson et al. 2023 ) and sustain it (e.g., Berger, Moe, and Schweidel 2023; Berger, Kim, and Meyer 2021; Deolankar et al. 2024 ). 1 However, these insights often depend on the context, platform, and population in which they are discovered, making it hard for practitioners and marketing researchers to make compelling predictions about engagement in a particular real-world setting (Banerjee and Urminsky 2023) .",
                "cite_spans": [
                    {
                        "start": 163,
                        "end": 190,
                        "text": "Banerjee and Urminsky 2023;",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 191,
                        "end": 219,
                        "text": "Bruce, Murthi, and Rao 2017;",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 220,
                        "end": 251,
                        "text": "Kanuri, Chen, and Sridhar 2018;",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 252,
                        "end": 277,
                        "text": "Zor, Kim, and Monga 2022;",
                        "ref_id": "BIBREF135"
                    },
                    {
                        "start": 278,
                        "end": 299,
                        "text": "Robertson et al. 2023",
                        "ref_id": "BIBREF108"
                    },
                    {
                        "start": 324,
                        "end": 356,
                        "text": "Berger, Moe, and Schweidel 2023;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 357,
                        "end": 385,
                        "text": "Berger, Kim, and Meyer 2021;",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 386,
                        "end": 407,
                        "text": "Deolankar et al. 2024",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 663,
                        "end": 691,
                        "text": "(Banerjee and Urminsky 2023)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "For a given application, the space of possible insights is vast, and discoveries take time (Chu and Evans 2021; Rzhetsky et al. 2015; Fiedler 2018) . The current approach relies on both human ingenuity and trial and error to come up with and then test one hypothesis at a time. When studying consumer language, this task is particularly vexing because language encompasses many dimensions (Aka, Bhatia, and McCoy 2023; Clark 1973 ). Where does one even begin?",
                "cite_spans": [
                    {
                        "start": 91,
                        "end": 111,
                        "text": "(Chu and Evans 2021;",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 112,
                        "end": 133,
                        "text": "Rzhetsky et al. 2015;",
                        "ref_id": "BIBREF110"
                    },
                    {
                        "start": 134,
                        "end": 147,
                        "text": "Fiedler 2018)",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 389,
                        "end": 418,
                        "text": "(Aka, Bhatia, and McCoy 2023;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 419,
                        "end": 429,
                        "text": "Clark 1973",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "Machine learning (ML) can help uncover patterns that humans may miss (Oquendo et al. 2012; Shin et al. 2023; Wang et al. 2023 ), but utilizing these tools for discovery often comes at the cost of interpretability and understanding (Messeri and Crockett 2024) . Therefore, marketing managers and scholars would benefit from an approach that helps uncover insights from existing data and presents those insights in a format that humans could readily 1 While engagement can mean di\u21b5erent things in marketing (see Table 1 in Berger, Moe, and Schweidel 2023, but also Brodie et al. 2011) , we will focus primarily on attracting attention.",
                "cite_spans": [
                    {
                        "start": 69,
                        "end": 90,
                        "text": "(Oquendo et al. 2012;",
                        "ref_id": "BIBREF97"
                    },
                    {
                        "start": 91,
                        "end": 108,
                        "text": "Shin et al. 2023;",
                        "ref_id": "BIBREF114"
                    },
                    {
                        "start": 109,
                        "end": 125,
                        "text": "Wang et al. 2023",
                        "ref_id": "BIBREF130"
                    },
                    {
                        "start": 231,
                        "end": 258,
                        "text": "(Messeri and Crockett 2024)",
                        "ref_id": "BIBREF87"
                    },
                    {
                        "start": 521,
                        "end": 557,
                        "text": "Berger, Moe, and Schweidel 2023, but",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 563,
                        "end": 582,
                        "text": "Brodie et al. 2011)",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 516,
                        "end": 517,
                        "text": "1",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "The goal of the present research is to advance our understanding of the e\u21b5ects of language on engagement while o\u21b5ering a framework to researchers, organizations, and policymakers that they could generate hypotheses about what drives engagement in their specific context with their customers and constituents. Similar to existing work in consumer psychology, we study how modifying the language in a message a\u21b5ects consumers' propensity to engage with it. For instance, does framing a message with an element of surprise, followed by a cli\u21b5hanger, make people more likely to engage with it? Does describing physical reactions make a message more engaging? How does focusing on positive aspects of human behavior a\u21b5ect engagement?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "The main contribution of this paper, however, is not in testing these specific hypotheses (although we do that too); instead, it is in developing the data-driven process that generated them. This paper proposes a framework for generating novel and interpretable hypotheses from text using a combination of large-language models (LLMs), machine learning (ML) tools, and psychology experiments. Large-language models, such as OpenAI's GPT-4, play a crucial role in processing text data and generating coherent hypotheses (Banker et al. 2023; Demszky et al. 2023) . At the same time, o\u21b5-the-shelf machine-learning tools help to uncover meaningful patterns in large volumes of unstructured data (Wang et al. 2023; Ludwig and Mullainathan 2024) . We integrate both technologies and validate the various steps through standard psychology experiments. The framework, therefore, consists of three steps: (1) generating a set of hypotheses from observations in the data, (2) ranking the set of hypotheses using a machine learning algorithm trained on past outcomes, and (3) filtering the set of hypotheses to select the ones most likely to have a meaningful e\u21b5ect.",
                "cite_spans": [
                    {
                        "start": 519,
                        "end": 539,
                        "text": "(Banker et al. 2023;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 540,
                        "end": 560,
                        "text": "Demszky et al. 2023)",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 691,
                        "end": 709,
                        "text": "(Wang et al. 2023;",
                        "ref_id": "BIBREF130"
                    },
                    {
                        "start": 710,
                        "end": 739,
                        "text": "Ludwig and Mullainathan 2024)",
                        "ref_id": "BIBREF78"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "We apply our framework to aggregate insights from several thousand marketing experiments and generate hypotheses for a specific application: how language a\u21b5ects engagement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "For this application, we are particularly interested in what features of language drive consumers to click on a headline. We use the Upworthy Research Archive (Matias et al. 2021) , which contains 32,487 randomized field experiments (\"A/B tests\") that test 64,983 unique headlines across 150,817 experimental arms. For each experimental arm, we also see the click-through rate (CTR), which we use to measure engagement. 2 Overview of the Framework. The first step is to generate hypotheses from observations in the data. We provided OpenAI's GPT with 2,100 unique pairs of headlines written for the same story to produce 2,100 hypotheses. For example, when provided the pair, \"Headline A: I Thought Long And Hard About Sharing This But I Decided I Had To Because These Dogs Need Our Help\" and \"Headline B: Don't Click This If You're Looking For Something That'll Make You Feel Better About The Human Race\" GPT responded with, \"Hypothesis:",
                "cite_spans": [
                    {
                        "start": 159,
                        "end": 179,
                        "text": "(Matias et al. 2021)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Incorporating reverse psychology leads to more engagement with a message.\" 3 To assess the quality of these, we had 79 human participants rate a subset of hypotheses on several dimensions. Most of the hypotheses were perceived to be clear, usable, and generalizable to new contexts (see also Banker et al. 2023) .",
                "cite_spans": [
                    {
                        "start": 292,
                        "end": 311,
                        "text": "Banker et al. 2023)",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "The second step is to rank the hypotheses by their predicted e\u21b5ects. We use an ML algorithm -trained to predict CTR using the high-dimensional information contained in the headlines (i.e., sentence embeddings; Song et al. 2020 ) -to identify which hypothesized features are likely to have an e\u21b5ect when applied to various messages. We start by applying each hypothesis to several di\u21b5erent headlines, again using GPT. For example, for a given hypothesis (e.g., \"Incorporating reverse psychology leads to more engagement with a message.\") applied to an actual Upworthy headline (e.g., \"Folks Who Work In Tipped Jobs Would Like You To Spend A Minute Looking At Something\"), GPT produces an alternative, \"morphed\" headline (e.g., \"You Probably Shouldn't Read This if You Think Tipping Is Optional\"). 4 For each hypothesis, we generated approximately 73 morphed headlines, each one paired to one actual Upworthy headline, producing a total of 250,000 morph-original pairs. We then used the ML algorithm to predict what the CTR would be for the morphed headline relative to the original headline. This approach to morphing and scoring produces a \"predicted treatment e\u21b5ect\" (PTE) for each morph that we can then aggregate at the hypothesis level. We use the average PTE of a hypothesis to rank-order the set. Therefore, we rank hypotheses using a measure that incorporates both the ML signal and an element of generalizability (since we apply each hypothesis to many headlines before estimating an e\u21b5ect).",
                "cite_spans": [
                    {
                        "start": 210,
                        "end": 226,
                        "text": "Song et al. 2020",
                        "ref_id": "BIBREF119"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "The third step is to filter the hypotheses. For nearly every pair, GPT produced a hypothesis that was understandable and plausible. While most were sensible, we cannot expect them all to be discoveries. Many of them overlapped with others in the set. Some were too specific (e.g., \"Hypothesis: using language that humanizes animals a\u21b5ects engagement with a message\") and would not apply to most other messages. To narrow the set, we group similar hypotheses together using a sequential selection strategy. We go through the ordered list, starting at the top, and select unique hypotheses. When we come across a hypothesis similar to one already selected, we exclude it from the list. This grouping reduced our set down to 205 hypotheses. Finally, we calculate a test statistic for each of the remaining hypotheses to determine whether their predicted e\u21b5ects di\u21b5er meaningfully from zero. In the end, 16 hypotheses had average predicted e\u21b5ects that were positive and significant (p < .05).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Among the hypotheses generated are six illustrative hypotheses that we selected to test out of sample.5 Four of these hypotheses were predicted to increase engagement: 1) framing a message with an element of surprise followed by a cli\u21b5hanger, 2) incorporating a concept of parody, 3) incorporating multimedia evidence, and 4) describing physical reactions. Two were predicted to decrease engagement: 5) shortening and simplifying phrases and 6) focusing on positive aspects of human behavior.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "After generating the hypotheses, we test them. We take advantage of the experimental setup of the data to estimate a causal e\u21b5ect using standard approaches. We intentionally left some of the data untouched when training the algorithm and throughout each step of the pipeline so that we could conduct these tests out of sample. We recruited 800 participants to code 3,386 headlines from 1,693 unique pairs of original Upworthy headlines, producing more than 100,000 ratings. Each pair was from the same randomized trial, which allowed us to estimate the e\u21b5ect on CTR of changing a specific feature (e.g., \"reference to multimedia\").",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Of the six hypotheses that we selected and pre-registered for testing, four had a significant e\u21b5ect on engagement (two ps < .001 and two ps < .05), and a fifth had a marginal e\u21b5ect (p = .094). Of these five, all showed e\u21b5ects in the predicted direction. To verify whether the features discovered through this process coincide with insights within the algorithm's \"black box\", we regress the algorithm's prediction on the human ratings for each feature. All six features were significant predictors of the ML algorithm, independently and in a multivariate model (ps < .001).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Finally, we explore whether these hypothesized features predict similar outcomes in other settings. Specifically, we test the same set of hypotheses using a similar modeling specification in a second dataset containing social media posts made by an online entertainment company. The messages resemble Upworthy's headlines in style and content, but the time period, platform, audience, and organizational strategy di\u21b5er. Although these posts were not released as A/B tests, they still provide correlational evidence in support of four of the six hypotheses discovered using the Upworthy dataset.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "For both datasets, we have reserved 40% of the data for us to analyze upon conditional acceptance of the paper. Therefore, the current manuscript serves as a registered report of our method and findings (Nosek and Lakens 2014; Chambers and Tzavella 2021; Urminsky and Dietvorst 2024) .",
                "cite_spans": [
                    {
                        "start": 203,
                        "end": 226,
                        "text": "(Nosek and Lakens 2014;",
                        "ref_id": "BIBREF96"
                    },
                    {
                        "start": 227,
                        "end": 254,
                        "text": "Chambers and Tzavella 2021;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 255,
                        "end": 283,
                        "text": "Urminsky and Dietvorst 2024)",
                        "ref_id": "BIBREF127"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Contributions. This paper is intended to help marketing researchers, organizations, and policymakers generate new insights into what drives consumer behavior. We make several significant contributions: First, we introduce a framework to convert unstructured text into marketing insights. There are several recent papers exploring how researchers can use text to study consumer behavior (e.g., Humphreys and Wang 2018; Berger et al. 2020; Berger and Packard 2023; Hartmann and Netzer 2023; Jackson et al. 2022) . As more of our everyday language is captured -through audio, video, or online communication -there will be more data to explore. One persistent challenge with this unstructured data is interpretability (Hartmann et al. 2019; Hartmann and Netzer 2023) . The framework we propose utilizes various existing technologies to help address this.",
                "cite_spans": [
                    {
                        "start": 393,
                        "end": 417,
                        "text": "Humphreys and Wang 2018;",
                        "ref_id": null
                    },
                    {
                        "start": 418,
                        "end": 437,
                        "text": "Berger et al. 2020;",
                        "ref_id": null
                    },
                    {
                        "start": 438,
                        "end": 462,
                        "text": "Berger and Packard 2023;",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 463,
                        "end": 488,
                        "text": "Hartmann and Netzer 2023;",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 489,
                        "end": 509,
                        "text": "Jackson et al. 2022)",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 714,
                        "end": 736,
                        "text": "(Hartmann et al. 2019;",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 737,
                        "end": 762,
                        "text": "Hartmann and Netzer 2023)",
                        "ref_id": "BIBREF46"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Second, we generate and test actual marketing hypotheses. In doing so, we contribute to the literature studying how language a\u21b5ects engagement (e.g., Banerjee and Urminsky 2023; Lee, Hosanagar, and Nair 2018; Berger, Moe, and Schweidel 2023; Berger, Kim, and Meyer 2021) . Using our framework, we uncover new insights, some adding to existing theories and others inspiring new questions. Although we tested a select set in this paper, our process generated dozens of hypotheses worth examining more closely in future research.",
                "cite_spans": [
                    {
                        "start": 178,
                        "end": 208,
                        "text": "Lee, Hosanagar, and Nair 2018;",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 209,
                        "end": 241,
                        "text": "Berger, Moe, and Schweidel 2023;",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 242,
                        "end": 270,
                        "text": "Berger, Kim, and Meyer 2021)",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "In addition, this paper adds to the literature on organizational learning (Moorman and Day 2016; Day 2011; Gebhardt, Carpenter, and Sherry 2006) . Organizations today continuously run A/B tests to learn how various messages a\u21b5ect consumers' behavior (Lee, Hosanagar, and Nair 2018; Angelopoulos, Lee, and Misra 2024; Matias et al. 2021 ). Nevertheless, many of these tests prioritize learning what works (e.g., by comparing wholesale changes; Koning, Hasan, and Chatterji 2022; Azevedo et al. 2020 ) at the cost of learning why, which typically requires more carefully controlled experiments. This paper demonstrates how to aggregate insights from thousands of A/B tests in the form of specific hypotheses that others can carefully test.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 96,
                        "text": "(Moorman and Day 2016;",
                        "ref_id": "BIBREF89"
                    },
                    {
                        "start": 97,
                        "end": 106,
                        "text": "Day 2011;",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 107,
                        "end": 144,
                        "text": "Gebhardt, Carpenter, and Sherry 2006)",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 250,
                        "end": 281,
                        "text": "(Lee, Hosanagar, and Nair 2018;",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 282,
                        "end": 316,
                        "text": "Angelopoulos, Lee, and Misra 2024;",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 317,
                        "end": 335,
                        "text": "Matias et al. 2021",
                        "ref_id": null
                    },
                    {
                        "start": 443,
                        "end": 477,
                        "text": "Koning, Hasan, and Chatterji 2022;",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 478,
                        "end": 497,
                        "text": "Azevedo et al. 2020",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Finally, this paper contributes to the research on data-driven discovery and hypothesis generation (McGuire 1997; Ludwig and Mullainathan 2024; Banker et al. 2023; Aka, Bhatia, and McCoy 2023; Adolphs et al. 2016) . While marketing researchers are driving some of the innovation in this space (e.g., Aka, Bhatia, and McCoy 2023; Banker et al. 2023 ), a lot is also happening in outside disciplines such as computer science and economics (Ludwig and Mullainathan 2024; Zhou et al. 2024; Manning, Zhu, and Horton 2024) . This work tries to bridge this literature and, in doing so, broaden the reach of our field (MacInnis et al. 2020) .",
                "cite_spans": [
                    {
                        "start": 99,
                        "end": 113,
                        "text": "(McGuire 1997;",
                        "ref_id": "BIBREF86"
                    },
                    {
                        "start": 114,
                        "end": 143,
                        "text": "Ludwig and Mullainathan 2024;",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 144,
                        "end": 163,
                        "text": "Banker et al. 2023;",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 164,
                        "end": 192,
                        "text": "Aka, Bhatia, and McCoy 2023;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 193,
                        "end": 213,
                        "text": "Adolphs et al. 2016)",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 300,
                        "end": 328,
                        "text": "Aka, Bhatia, and McCoy 2023;",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 329,
                        "end": 347,
                        "text": "Banker et al. 2023",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 437,
                        "end": 467,
                        "text": "(Ludwig and Mullainathan 2024;",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 468,
                        "end": 485,
                        "text": "Zhou et al. 2024;",
                        "ref_id": "BIBREF134"
                    },
                    {
                        "start": 486,
                        "end": 516,
                        "text": "Manning, Zhu, and Horton 2024)",
                        "ref_id": "BIBREF83"
                    },
                    {
                        "start": 610,
                        "end": 632,
                        "text": "(MacInnis et al. 2020)",
                        "ref_id": "BIBREF80"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Related Work. Our work is similar to recent work that attempts to generate interpretable hypotheses. Ludwig and Mullainathan (2024) develop a procedure for generating hypotheses from unstructured image data. They leave open the question of whether their procedure could be extended to other high-dimensional datasets, including text. Text, however, is quite di\u21b5erent from images. For instance, where images are continuous, text is discrete. Changing the color of a single pixel maintains much of the image intact -it resembles the original.",
                "cite_spans": [
                    {
                        "start": 101,
                        "end": 131,
                        "text": "Ludwig and Mullainathan (2024)",
                        "ref_id": "BIBREF78"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "In contrast, changing even a letter (\"run\" to \"ran\") or removing a punctuation (\"Let's eat, Grandma\" to \"Let's eat Grandma\") can change the whole meaning of the sentence. Another important distinction is in the nature of the hypothesized features. Features contained in text are mutable in ways that features contained faces (and images, more generally) tend not to be. That is, hypotheses derived from text should be not only interpretable but also usable or able to be applied to new messages written by humans (or LLMs). Our work, therefore, builds on the ideas in Ludwig and Mullainathan (2024) , integrating new technologies to generate hypotheses from a di\u21b5erent data source.",
                "cite_spans": [
                    {
                        "start": 568,
                        "end": 598,
                        "text": "Ludwig and Mullainathan (2024)",
                        "ref_id": "BIBREF78"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "In particular, we rely on LLMs to generate hypotheses similar to the work of Banker et al. (2023) , Manning, Zhu, and Horton (2024), and Zhou et al. (2024) . Banker et al. (2023) demonstrates how one could fine-tune LLMs using published and unpublished papers to produce novel psychological hypotheses. Researchers reviewing the hypotheses produced through this process rated them equal quality to human-generated hypotheses in published papers. However, the paper did not formally test the hypotheses as we do here. Manning, Zhu, and Horton (2024) attempts to automate the social scientific process by using LLMs for generating hypotheses and testing them. The process in that paper relies on structural causal models to propose hypotheses, design experiments, and then test them in a simulated environment. Our process is like theirs in that it is semi-automated, requiring almost no human involvement in generating and interpreting the hypotheses. Our paper deviates from the earlier work in that it starts with real-world data to generate the hypotheses. Then, it tests these hypotheses with real-world outcomes. Zhou et al. (2024) o\u21b5ers an alternative approach to ours, also using the Upworthy data. Their paper introduces an algorithm that starts with an initial set of example hypotheses and leverages an LLM to update and refine the set iteratively. The algorithm employs a reward function inspired by the multi-arm bandit literature to guide the updating process. This method provides an e cient search process. We, instead, opted for an alternative step where we apply each hypothesis to new headlines and use the ML algorithm to estimate a predicted e\u21b5ect for each. Our ranking procedure, therefore, allows us to utilize the full capacity of machine learning algorithms to detect complex patterns in the data (Oquendo et al. 2012; Hutson 2023; Wang et al. 2023) while also taking into account the generalizability of each hypothesis.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 97,
                        "text": "Banker et al. (2023)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 100,
                        "end": 117,
                        "text": "Manning, Zhu, and",
                        "ref_id": "BIBREF83"
                    },
                    {
                        "start": 118,
                        "end": 136,
                        "text": "Horton (2024), and",
                        "ref_id": "BIBREF83"
                    },
                    {
                        "start": 137,
                        "end": 155,
                        "text": "Zhou et al. (2024)",
                        "ref_id": "BIBREF134"
                    },
                    {
                        "start": 158,
                        "end": 178,
                        "text": "Banker et al. (2023)",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 517,
                        "end": 548,
                        "text": "Manning, Zhu, and Horton (2024)",
                        "ref_id": "BIBREF83"
                    },
                    {
                        "start": 1117,
                        "end": 1135,
                        "text": "Zhou et al. (2024)",
                        "ref_id": "BIBREF134"
                    },
                    {
                        "start": 1820,
                        "end": 1841,
                        "text": "(Oquendo et al. 2012;",
                        "ref_id": "BIBREF97"
                    },
                    {
                        "start": 1842,
                        "end": 1854,
                        "text": "Hutson 2023;",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 1855,
                        "end": 1872,
                        "text": "Wang et al. 2023)",
                        "ref_id": "BIBREF130"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "understand.",
                "sec_num": null
            },
            {
                "text": "Importantly, it also o\u21b5ers a general framework that researchers and organizations can use to aggregate marketing insights from text. This framework can be applied whenever there is high-dimensional text data, such as text messages, emails, social media posts, brand slogans, advertising content, and customer service scripts. The data need not be structured, and the process requires little human interpretation. Nevertheless, the output is a set of marketing hypotheses readily interpretable by humans.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Current Paper. The current paper produces new insights into what drives engagement.",
                "sec_num": null
            },
            {
                "text": "To use it, one needs a corpus of text and access to a large-language model (e.g., OpenAI's GPT, Anthropic's Claude, or Google's Gemini). The primary prompt asks the LLM to produce an insight that captures what changed between a pair of messages and respond with a hypothesis, \"Hypothesis: increases [decreases] engagement with a message.\"6 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Current Paper. The current paper produces new insights into what drives engagement.",
                "sec_num": null
            },
            {
                "text": "Notice that this step, the actual generation of hypotheses, can be done by nearly anyone with access to a computer today without needing any outcome variable. And although we use pairs of messages from the same A/B test in this paper, this process is not restricted to experimental data. This means that the first step of our process can be applied, for example, by researchers attempting to come up with alternative explanations for a given set of stimuli or firms hoping to learn more about a competitor's marketing strategy. Our ranking and filtering steps leverage o\u21b5-the-shelf machine learning tools, which others could also use to approximate how di\u21b5erent hypotheses, applied to one's data, could a\u21b5ect one's outcome(s) of interest. A company with multiple brands or diverse sets of customers can train an algorithm (or fine-tune an existing one) for each use case and then re-rank the list of hypotheses based on the predictions for each group. Together, this framework integrates existing technologies to provide others with an accessible method for discovery.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Current Paper. The current paper produces new insights into what drives engagement.",
                "sec_num": null
            },
            {
                "text": "As a guide to the rest of the paper, in the next section (Section 2), we describe the application and data used in this paper. Then, we detail our approach for generating hypotheses (Section 3). After generating hypotheses, we test a subset using human coders, both in a holdout set (Section 4) and in an entirely di\u21b5erent messaging context (Section 5). In the final section, we discuss the process, results, and implications for future research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Current Paper. The current paper produces new insights into what drives engagement.",
                "sec_num": null
            },
            {
                "text": "Data and materials related to the studies conducted with human participants are available on the Open Science Framework (OSF; see bit.ly/headlines-osf)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Current Paper. The current paper produces new insights into what drives engagement.",
                "sec_num": null
            },
            {
                "text": "To illustrate this procedure, we start with a concrete application: what linguistic features of a headline lead people to engage with it? where engagement, conditional on seeing a particular headline, is measured through click-through rates (CTR). This application has broad relevance not only for the consumption of news, but also for other domains where engagement precedes behavior (Petty and Cacioppo 1986) . For example, domains such as advertising (Lee, Hosanagar, and Nair 2018; Phillips and McQuarrie 2010) , influencer marketing (Chung, Ding, and Kalra 2023; Cascio Rizzo et al. 2023) , constituent services (De La Rosa et al. 2021; Linos et al. 2024) , customer communication (Rei\u21b5 et al. 2023; Kaul et al. 2024) , and online education (Nie et al. 2024; Kizilcec, Piech, and Schneider 2013; Deolankar et al. 2024 ).7 ",
                "cite_spans": [
                    {
                        "start": 385,
                        "end": 410,
                        "text": "(Petty and Cacioppo 1986)",
                        "ref_id": "BIBREF101"
                    },
                    {
                        "start": 454,
                        "end": 485,
                        "text": "(Lee, Hosanagar, and Nair 2018;",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 486,
                        "end": 514,
                        "text": "Phillips and McQuarrie 2010)",
                        "ref_id": "BIBREF102"
                    },
                    {
                        "start": 538,
                        "end": 567,
                        "text": "(Chung, Ding, and Kalra 2023;",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 568,
                        "end": 593,
                        "text": "Cascio Rizzo et al. 2023)",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 617,
                        "end": 641,
                        "text": "(De La Rosa et al. 2021;",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 642,
                        "end": 660,
                        "text": "Linos et al. 2024)",
                        "ref_id": "BIBREF75"
                    },
                    {
                        "start": 686,
                        "end": 704,
                        "text": "(Rei\u21b5 et al. 2023;",
                        "ref_id": "BIBREF106"
                    },
                    {
                        "start": 705,
                        "end": 722,
                        "text": "Kaul et al. 2024)",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 746,
                        "end": 763,
                        "text": "(Nie et al. 2024;",
                        "ref_id": "BIBREF95"
                    },
                    {
                        "start": 764,
                        "end": 800,
                        "text": "Kizilcec, Piech, and Schneider 2013;",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 801,
                        "end": 822,
                        "text": "Deolankar et al. 2024",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Value of Click-Throughs",
                "sec_num": null
            },
            {
                "text": "Headlines also represent one form of text where the procedure we propose could prove particularly useful. Countless headlines are created and promoted each day. The text is relatively short, typically between 53 and 100 characters, making it easier to parse and compute possible variations. Multiple headlines could be written for the same story, allowing one to study variations while keeping the theme or topic constant. Furthermore, variations matter -di\u21b5erent headlines drive di\u21b5erent click-through rates; combined with a randomizedcontrolled trial, this reveals that something about the text influences behavior.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Value of Click-Throughs",
                "sec_num": null
            },
            {
                "text": "Our specific application uses the Upworthy Research Archive ( ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Upworthy Research Archive",
                "sec_num": null
            },
            {
                "text": "To clean the data, we applied a few standard steps (e.g., Berger et al. 2020 , Table 3 ).",
                "cite_spans": [
                    {
                        "start": 58,
                        "end": 76,
                        "text": "Berger et al. 2020",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 85,
                        "end": 86,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Data Cleaning",
                "sec_num": "2.3.1"
            },
            {
                "text": "First, we removed one observation where the headline text was missing. Next, we cleaned the raw text by removing non-visible characters (e.g., HTML tags) and replacing non-ASCII characters with ASCII equivalents. For cases where two or more treatment arms in a trial had the same headline (e.g., where the image varied), we collapsed the rows into one, summing the number of clicks and impressions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Cleaning",
                "sec_num": "2.3.1"
            },
            {
                "text": "The original data was released already split for exploratory, confirmatory, and testing analysis. However, because the headlines were sometimes reused across trials, the headlines found in one of these original splits sometimes appeared in another. This kind of \"leakage\" is problematic for machine learning applications and can lead to over-optimistic results (see Kapoor and Narayanan 2023) . Therefore, we resampled the complete set into new splits by \"component\", which we defined so that we could group trials with overlapping headlines.9 ",
                "cite_spans": [
                    {
                        "start": 366,
                        "end": 392,
                        "text": "Kapoor and Narayanan 2023)",
                        "ref_id": "BIBREF62"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "This ensured that headlines repeated within and across trials were contained within the same split. The resulting splits include:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "\u2022 A training set (40% of trials; 12,800 trials). This set is further partitioned for training the machine learning model (N = 11, 535) and tuning the model's hyperparameters (N = 1, 265). This set is also used for generating hypotheses, described in Section 3.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "Note that we did not use the ML algorithm, which is di\u21b5erent to GPT, to generate hypotheses, so we were not concerned about data leakage between these two uses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "\u2022 A morphing set (10% of trials; 3,366 trials). This set was used to produce counterfactual headlines, described in Section 3.2.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "\u2022 A regression set of (10% of trials; 3,136 trials). This set was used as a validation set for testing the hypotheses we uncovered, described in the Hypothesis Testing section.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "We also used this set for benchmarking initial model performance (see the below where we explore the signal in the text).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "\u2022 A lock-box or hold-out set (40% of trials; 13,185 trials). We plan to unlock and analyze this set upon conditional acceptance. 10",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Partitioning",
                "sec_num": "2.3.2"
            },
            {
                "text": "The outcome we care about in this application is the click-through rate (CTR). For each headline, the CTR is defined as CTR = Clicks Impressions . To account for variability in CTRs arising from trials of di\u21b5erent sizes, we employed a shrinkage procedure toward the overall average CTR. Specifically, we adjusted each headline's CTR by adding the overall mean CTR to the numerator and 1 to the denominator. For any headline H a , we define this as the smoothed CTR estimate:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defining the outcome of interest",
                "sec_num": "2.3.3"
            },
            {
                "text": "Smoothed CTR a = Clicks a + CTR Impressions a + 1 (1)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defining the outcome of interest",
                "sec_num": "2.3.3"
            },
            {
                "text": "where CTR was the mean CTR calculated across all headlines. This approach e\u21b5ectively reduced the variance of CTR estimates for headlines with limited data, leveraging the global average as a stabilizing prior. Finally, we defined our outcome of interest to be the di\u21b5erence in CTR:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defining the outcome of interest",
                "sec_num": "2.3.3"
            },
            {
                "text": "CTR a,b = Smoothed CTR b Smoothed CTR a (2)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defining the outcome of interest",
                "sec_num": "2.3.3"
            },
            {
                "text": "for any two headlines H a and H b from the same trial. 11 For simplicity, we refer to Smoothed CTR as CTR in the remainder of this paper.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defining the outcome of interest",
                "sec_num": "2.3.3"
            },
            {
                "text": "10 Once the paper is conditionally accepted for publication, we will have headlines from these trials labeled on the final set of hypothesized features in order to replicate our findings. The current manuscript, therefore, serves as a registered report (Nosek and Lakens 2014; Chambers and Tzavella 2021; Urminsky and Dietvorst 2024) .",
                "cite_spans": [
                    {
                        "start": 253,
                        "end": 276,
                        "text": "(Nosek and Lakens 2014;",
                        "ref_id": "BIBREF96"
                    },
                    {
                        "start": 277,
                        "end": 304,
                        "text": "Chambers and Tzavella 2021;",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 305,
                        "end": 333,
                        "text": "Urminsky and Dietvorst 2024)",
                        "ref_id": "BIBREF127"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defining the outcome of interest",
                "sec_num": "2.3.3"
            },
            {
                "text": "11 Other reasonable approaches would include using a hierarchical Bayesian model to determine the level of mean shrinkage, or a binomial likelihood to handle trial sizes directly. While these approaches could have been used for modeling CTR, we have chose to use a strategy that we felt was easier to understand and readily generalizes to other settings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Defining the outcome of interest",
                "sec_num": "2.3.3"
            },
            {
                "text": "Given the experimental setup of the data, we decided to produce our analysis at the pair level, where each observation consists of a pair of headlines. After cleaning, we collected all pairs of headlines H a and H b that appeared in the same trial. Our data partitioning ensures that all headlines in a trial are allocated to the same partition, and therefore, all pairs of headlines within a trial are also allocated to the same partition. Because a trial with k unique headlines contains k(k 1) unique pairs of headlines (independent of order, e.g.,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "A-B and B-A are two pairs), the number of trials in the pairwise dataset does not match up precisely to the number of trials. For example, 14,729 headlines were dropped because the trial consisted of only one headline (i.e., zero pairs of headlines). Note that while this does constitute 45% of the trials in the entire dataset, it makes up only 16% of the headlines in the entire dataset (as these are, by definition, trials with the fewest headlines).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "The pairwise dataset splits therefore contain:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "\u2022 Training set (40%): 112,350 unique headline pairs from 7,048 trials.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "\u2022 Morphing set (10%): 29,600 unique pairs from 1,807 trials. However, because morphing is done at the headline level, the pairwise dataset is not used for the morphing process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "\u2022 Regression set (10%): 27,206 unique pairs from 1, 701 trials. However, for the actual regression step, we will further sample to a single pair from each trial.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "\u2022 Lock-box set (40%): 112,998 unique pairs from 7,202 unique trials.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "Additional Features: Semantic representation, psycholinguistic features, and human labels",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Formatting",
                "sec_num": "2.3.4"
            },
            {
                "text": "We converted the raw text to its high-dimensional semantic representation or sentence embedding to analyze the text data and train our machine learning algorithm. Sentence embeddings are vector representations of text, which are both fixed length and numeric, meaning they could be used as inputs to various downstream tasks. We extract sentence embeddings using a pre-trained MPNet model (Song et al. 2020) , which converts text into a vector of length 768.12 It produces this embedding using a transformer architecture: the text is first converted into a sequence of \"tokens\", each token is mapped to a numeric vector, the starting sequence of vectors are transformed into a sequence of output vectors by several transformer layers in a neural network, and a final output vector is produced by taking the mean value per index across all output vectors. In addition to training the machine learning algorithm, we used these embeddings for other tasks, such as measuring textual similarity and diversity.",
                "cite_spans": [
                    {
                        "start": 389,
                        "end": 407,
                        "text": "(Song et al. 2020)",
                        "ref_id": "BIBREF119"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Semantic representation",
                "sec_num": "2.4.1"
            },
            {
                "text": "Absent in the Upworthy data are the explicit hypotheses each trial intended to test.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Existing Psycholinguistic Features",
                "sec_num": "2.4.2"
            },
            {
                "text": "Although we cannot impute specific hypotheses from the past, we could examine how features known to a\u21b5ect behavior influence the click-through rate.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Existing Psycholinguistic Features",
                "sec_num": "2.4.2"
            },
            {
                "text": "One advantage of starting with the Upworthy dataset is that we are not the first to use it (e.g., Banerjee and Urminsky 2023; Robertson et al. 2023; Gligori\u0107 et al. 2023; Rathje et al. 2023; Hopkins, Lelkes, and Wolken 2023; Shulman, Markowitz, and Rogers 2024; Zhou et al. 2024) . In particular, Banerjee and Urminsky (2023, \"BU\") creates a set of features representing psychological constructs deemed relevant by previous research and maps them to each headline.",
                "cite_spans": [
                    {
                        "start": 126,
                        "end": 148,
                        "text": "Robertson et al. 2023;",
                        "ref_id": "BIBREF108"
                    },
                    {
                        "start": 149,
                        "end": 170,
                        "text": "Gligori\u0107 et al. 2023;",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 171,
                        "end": 190,
                        "text": "Rathje et al. 2023;",
                        "ref_id": null
                    },
                    {
                        "start": 191,
                        "end": 224,
                        "text": "Hopkins, Lelkes, and Wolken 2023;",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 225,
                        "end": 261,
                        "text": "Shulman, Markowitz, and Rogers 2024;",
                        "ref_id": "BIBREF115"
                    },
                    {
                        "start": 262,
                        "end": 279,
                        "text": "Zhou et al. 2024)",
                        "ref_id": "BIBREF134"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Existing Psycholinguistic Features",
                "sec_num": "2.4.2"
            },
            {
                "text": "For our paper, we replicated the work of BU using their materials (posted on osf.io/ 826jq in September 2022) to check that we could reliably extract their features. We combined the outputs from LIWC (Tausczik and Pennebaker 2010) , TextAnalyzer (Berger, Sherman, and Ungar 2020) , and unique word lists BU compiled from past papers to reconstruct the feature set representing the 51 psychological constructs used in BUs analyses. 13 Notably, the set also includes features that Banerjee and Urminsky (2023) have shown a\u21b5ect click-through rates in this dataset, features such as reading ease, numeric reference, and the use of visual language. 14",
                "cite_spans": [
                    {
                        "start": 200,
                        "end": 230,
                        "text": "(Tausczik and Pennebaker 2010)",
                        "ref_id": "BIBREF122"
                    },
                    {
                        "start": 246,
                        "end": 279,
                        "text": "(Berger, Sherman, and Ungar 2020)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Existing Psycholinguistic Features",
                "sec_num": "2.4.2"
            },
            {
                "text": "Despite the many constructs captured in BU's features, it is possible that some of what is known is not reflected in the features. To address this, we could have enumerated a set of additional constructs based on a broader read of existing research and then created new dictionaries (Humphreys and Wang 2018) or had humans code each headline on each construct. This process is, of course, expensive in both money and time and still runs the risk of not capturing implicit knowledge that humans hold in their heads but cannot articulate (e.g., Malt et al. 1999; Batista et al. 2024) .",
                "cite_spans": [
                    {
                        "start": 283,
                        "end": 308,
                        "text": "(Humphreys and Wang 2018)",
                        "ref_id": null
                    },
                    {
                        "start": 543,
                        "end": 560,
                        "text": "Malt et al. 1999;",
                        "ref_id": "BIBREF82"
                    },
                    {
                        "start": 561,
                        "end": 581,
                        "text": "Batista et al. 2024)",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "Instead, we attempt to capture any remaining information by collecting human guesses (Ludwig and Mullainathan 2024) . We recruited 303 participants through Prolific (www.prolific.com) and incentivized them to choose from a pair of headlines -written for the same storywhich one they believed had performed better in an A/B test. 15 Each participant completed 10 \"training\" rounds and 30 \"test\" rounds, one page at a time. In each round, participants were shown a pair of headlines written for the same story (i.e., from the same A/B test) and asked to choose which headline they thought performed better. During the training rounds, participants received feedback after each guess, where we revealed the correct answer. During the testing rounds, participants received no feedback but",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 115,
                        "text": "(Ludwig and Mullainathan 2024)",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 156,
                        "end": 183,
                        "text": "Prolific (www.prolific.com)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "were incentivized to answer correctly. Specifically, participants received $0.25 for selecting the correct answer in at least 17 out of 30 rounds plus an additional $0.25 for each correct",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "13 At the time of this writing, textanalyzer.org, was no longer online. However, we collected the features for the full dataset prior to this.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "14 Note that while we will be using the same set of features, our data splits and modeling specifications will be di\u21b5erent. Therefore, the results in this paper may be appear inconsistent with BU's work. 15 Half the participants were randomized into a condition that asked them to identify which headline performed worse, but there is no evidence this a\u21b5ected performance, (t(300) = .93, p = .36) response beyond that. 16 Data and materials are available on OSF.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "Participants labeled 1,693 pairs of headlines from the regression set, where each pair received a median of five responses (IQR: 4, 7). The interrater reliability, measured by the ICC1k variation (Revelle 2007) , was above 99% for all labels.",
                "cite_spans": [
                    {
                        "start": 196,
                        "end": 210,
                        "text": "(Revelle 2007)",
                        "ref_id": "BIBREF107"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "The \"known psychological features\" gathered using BU's approach and the human labels we collected play an essential role because they provide a baseline of knowledge. We use these features both to approximate how much of the algorithm's predictions are already explained by existing literature in consumer psychology and to estimate how much of the explainable variation in CTR is still left to uncover. By comparing predictions from a model that uses these known psychological features and the human labels to one that combines these known features and our algorithm's predictions, we can evaluate the extent to which the algorithm is uncovering something new versus rediscovering features already known. We can also use the known features to verify post-hoc whether features discovered using our procedure capture any signal above and beyond the existing set.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "Is there any signal in the text left to discover?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "Textual cues have been shown to motivate engagement. In fact, using this dataset Banerjee and Urminsky (2023) test the e\u21b5ects of more than 50 psychological constructs. Therefore, it is reasonable to ask is there anything left to discover? And if so, how much?",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 109,
                        "text": "Urminsky (2023)",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "For this, we need to compare the signal captured in the models that include the known features and human labels to the model that uses predictions from the machine learning algorithm trained on sentence embeddings (which we describe below).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Labels",
                "sec_num": "2.4.3"
            },
            {
                "text": "Our main set of baseline models are linear models in which we include predictors for the known psychological features extracted using BU's approach ( d BU ), the human labels (G), or both combined. We report all our results out of sample to accurately compare the performance of the various models (see Table 3 ). 17",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 309,
                        "end": 310,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "We formed the BU predictor as follows. For each of the 51 psychological constructs used in BU's analyses (see Section 2.4.2), we take the di\u21b5erence in construct values between the headlines in each pair. The result is 51 features defined as the di\u21b5erence in a psychological construct (such as reading ease, numeric reference, or visual language). We then estimate an OLS regression of the form",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "CTR a,b = 0 + 51 X i=1 i \u2022 Rating i a,b + \" a,b ,",
                        "eq_num": "(3)"
                    }
                ],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "and estimate the coe cient values on the full training set consisting of 112,350 unique headline pairs. For robustness, we also fit a non-linear model using XGBoost which can better account for complex relationships, such as interactions, between the known features. For this model, we first train the model using a subset of 99,670 pairs of headlines, and use the remaining 12,680 pairs as a tuning population for finding ideal hyperparameter values for the XGBoost model. For both of these models, we then use the estimated coe cients to extract predictions on the regression partition of the pairwise data. These predictions, which we call the \"BU predictor\" and the \"BU predictor (non-linear)\", can then be used as features in regressions on the regression partition not used in training any of the models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "For the human guess predictor, no additional transformations were required since the guesses collected were already at the pair level. There was also no need to estimate any coe cients on an independent training sample since only a single feature exists. Instead, the feature itself (the proportion of people guessing H B instead of H A for the pair of headlines) is used as the human guess predictor.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "One way to examine the predictive accuracy of these models is to look at their Adjusted R 2 , 17 Out of sample (OOS) predictions are a standard way to evaluate model performance in machine learning tasks (Mullainathan and Spiess 2017) . To obtain an OOS prediction, we first fit a regression on the training set and use that model to predict the outcome in the regression (validation) set. The known features model, therefore, represents a model in which we regress the CTR on the predicted outcome given the known features.",
                "cite_spans": [
                    {
                        "start": 204,
                        "end": 234,
                        "text": "(Mullainathan and Spiess 2017)",
                        "ref_id": "BIBREF91"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "which captures the proportion of the variation in CTR explained by the predictors. The model with only the human labels has an Adjusted R 2 = .008 . The model with only the predictor of known psychological features has an Adjusted R 2 = .042. The model containing both the known features and the human labels has an Adjusted R 2 = .049. When comparing the linear and non-linear BU models, we find that performance tends to improve, but results are qualitatively similar (see Table 3 ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 481,
                        "end": 482,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "Another way to assess performance is to leverage the experimental setup of these data to ask: how well do humans (or a model that includes the known psychological features) pick the winning headline? All three baseline models perform modestly; each picks the winner significantly better than chance (50% ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting click-through rates using known features and human labels",
                "sec_num": "2.5.1"
            },
            {
                "text": "How well does the machine learning algorithm do? To answer this, we train an ML algorithm to predict CTR a,b . We employ a Siamese network architecture (Bromley et al. 1993) , which in our case works by first transforming headlines H a and H b into vectors using a text embedding model (see Section 2.4.1 for an explanation of an example of such a model), then taking the di\u21b5erence between these vectors, and using that di\u21b5erence as input to a linear regression which outputs a single value. To initialize the model, we again use the pre-trained MPNet architecture as a sentence embedding model (Song et al. 2020) , and use a single, randomly-initialized, fully-connected linear layer for the regression. The underlying embedding model and the final regression layer are then simultaneously fine-tuned using a standard gradient descent approach, to improve the performance in predicting CTR. We call the fully trained model m, and write b m for the algorithm's prediction.",
                "cite_spans": [
                    {
                        "start": 152,
                        "end": 173,
                        "text": "(Bromley et al. 1993)",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 595,
                        "end": 613,
                        "text": "(Song et al. 2020)",
                        "ref_id": "BIBREF119"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting click-through rates using machine learning algorithm",
                "sec_num": "2.5.2"
            },
            {
                "text": "To evaluate the performance of the ML algorithm, we again refer to the regression set, which contains headlines which the model has not seen during training. As we did above, we consider the proportion of variance explained (Adjusted R 2 ). Regressing CTR a,b on the algorithm's prediction, b m a,b , results in an Adjusted R 2 = .130. Treating the outcome as a binary measure, our algorithm correctly picks the winner 63.9% of the time (compared to 50% guess; 95% CI [61.5%, 66.1%]).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Predicting click-through rates using machine learning algorithm",
                "sec_num": "2.5.2"
            },
            {
                "text": "Using the known features and human labels models as benchmarks, we see that the algorithm provides a significant improvement on every measure. In Table 3 , we examine whether the algorithm's prediction captures any signal beyond what is known. Regressing CTR on the algorithm's prediction results in an Adjusted R 2 = .130. This is noticeably higher than the model of known features (Adjusted R 2 = .042) and human guesses alone (Adjusted R 2 = .008). Combining the known features, the human labels, and the algorithm's predictions lifts the Adjusted R 2 to .136, outperforming any of the models on their own. 18",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 152,
                        "end": 153,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Comparing performance",
                "sec_num": "2.5.3"
            },
            {
                "text": "In R 2 terms, the ML algorithm captures .130 .136 = 95.6% of the predictive signal. A similar pattern could be seen with the binary measure. A model that includes known features, human labels, and the ML algorithm correctly picks the winning headline 62.9% (95% CI [60.6%, 65.2%]). Noticeably better than the model reported above that only includes known features and human labels, but marginally worse than the ML predictions on their own.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparing performance",
                "sec_num": "2.5.3"
            },
            {
                "text": "To get a sense of how much of the ML prediction is captured by the known features, we regressed b m a,b on the known features, d BU , and the human labels. The Adjusted R 2 = .197, suggesting there is a lot in the ML predictions not accounted for in what is already known.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparing performance",
                "sec_num": "2.5.3"
            },
            {
                "text": "18 Adding the ML predictor to a model of known features and human labels significantly increases the proportion of variance explained, F (1, 1689) = 171.14, p < .001. Adding known features and human labels to the ML-only model also improves the performance of the base model, F (2, 1689) = 6.70, p = .001.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparing performance",
                "sec_num": "2.5.3"
            },
            {
                "text": "In the next section, we explore these predictions further through a series of steps designed to uncover hypotheses from text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Comparing performance",
                "sec_num": "2.5.3"
            },
            {
                "text": "The algorithm is picking up signals that humans fail to see and that past research in marketing and psychology may not yet have discovered. It has, in a sense, made a discovery.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "But the discovery remains unknown; the signal is uninterpretable to human researchers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "For a particular class of applications where prediction is the main objective (Kleinberg et al. 2015) , this may be enough. However, stopping here would leave a lot to be desired when the aim is to uncover novel insights. If the predictors were a set of pre-specified features, one could e\u21b5ectively \"read out\" the significant predictors and use these to form hypotheses (e.g., Guenoun and Zlatev 2023; Netzer, Lemaire, and Herzenstein 2019; Sheetal, Feng, and Savani 2020, but see Mullainathan and Spiess 2017). In the current approach, the algorithm is trained using embeddings that are uninterpretable to humans, even if one were to use regression methods.",
                "cite_spans": [
                    {
                        "start": 78,
                        "end": 101,
                        "text": "(Kleinberg et al. 2015)",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 377,
                        "end": 401,
                        "text": "Guenoun and Zlatev 2023;",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 402,
                        "end": 440,
                        "text": "Netzer, Lemaire, and Herzenstein 2019;",
                        "ref_id": "BIBREF94"
                    },
                    {
                        "start": 441,
                        "end": 459,
                        "text": "Sheetal, Feng, and",
                        "ref_id": "BIBREF113"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "This section describes the steps we devised to recover some of these insights in the form of human-interpretable hypotheses. The goal is to set up a pipeline where one could input text, and the output would be a set of hypotheses to test. Throughout the process, it should be apparent where the algorithm played a role (and where the human did). Our process consists of three steps: generating, ranking, and filtering hypotheses. For each step, we explain the procedure, the output, and any additional checks we did to validate our approach. Figure 1 presents an overview of the framework.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 549,
                        "end": 550,
                        "text": "1",
                        "ref_id": "FIGREF6"
                    }
                ],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "All the code for the steps described are available upon request and will be made public when the paper is accepted for publication. Supplemental materials, such as prompts used with the LLMs and additional figures, are described in the Appendix and posted on OSF (bit.ly/headlines-osf).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Step 1: Generating Hypotheses",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Our process begins with generating hypotheses. This step is designed to mimic the behavior of a careful researcher who systematically writes down hypotheses as they comb through a dataset. Row by row, this researcher might examine a pair of messages and jot down an insight based on what they observed. Each insight forms the basis of a \"hypothesis\" that could later be tested. 19",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Although humans could, presumably, do this task, existing evidence suggests they may not do it well. For instance, humans may be limited by what they can see, noticing some changes more easily than others (Adams et al. 2021) . They also tend to search for evidence consistent with their preferred hypothesis (Hartzmark, Hirshman, and Imas 2021; Bhatia 2014; Jerath and Ren 2021; Klayman and Ha 1987; Piezunka and Dahlander 2015) . Third, the average person may be limited by their belief that their creativity is finite (Lucas and Nordgren 2020), thus undersampling the space of possible discoveries.",
                "cite_spans": [
                    {
                        "start": 205,
                        "end": 224,
                        "text": "(Adams et al. 2021)",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 308,
                        "end": 344,
                        "text": "(Hartzmark, Hirshman, and Imas 2021;",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 345,
                        "end": 357,
                        "text": "Bhatia 2014;",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 358,
                        "end": 378,
                        "text": "Jerath and Ren 2021;",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 379,
                        "end": 399,
                        "text": "Klayman and Ha 1987;",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 400,
                        "end": 428,
                        "text": "Piezunka and Dahlander 2015)",
                        "ref_id": "BIBREF103"
                    },
                    {
                        "start": 520,
                        "end": 530,
                        "text": "(Lucas and",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "LLMs, instead, o\u21b5er a way to do this task at scale while generating a diverse set of hypotheses (for an analysis comparing the semantic diversity of human-generated versus LLM-generated hypotheses, see Appendix Section 4). 20",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Procedure. We used OpenAI's GPT-4-Turbo (\"GPT\") to generate a total of 2,100 hypotheses from 2,100 unique pairs of headlines. To select these pairs, we started with the full set of 282,154 headline pairs, where the headlines in a pair were always from the same trial. 21 We then selected the top quartile of the absolute value of b m a,b (the entire dataset was used to determine quartiles). Next, we restricted the sample to pairs from the training 19 This is analogous to \"divergent\" or \"fanning out\" approaches found in the creativity literature, where the aim is to come up with an expansive list of ideas (e.g., Vanden Bergh, Reid, and Schorin 1983; Kilgour and Koslow 2009; Rosengren et al. 2020; Toubia and Netzer 2017 ). 20 While we do not test whether LLMs have the same biased tendencies as humans (though see Hagendor\u21b5, Fabi, and Kosinski 2023) , we do gather hypotheses from human participants using a similar approach to that used with GPT and find that human hypotheses as diverse as those uncovered by GPT. See Appendix Section 4. set to minimize leakage in later steps. 22 From this subset, we randomly drew one pair per \"component\" (which also meant at most one pair per trial; see Section 2.3).",
                "cite_spans": [
                    {
                        "start": 617,
                        "end": 654,
                        "text": "Vanden Bergh, Reid, and Schorin 1983;",
                        "ref_id": "BIBREF129"
                    },
                    {
                        "start": 655,
                        "end": 679,
                        "text": "Kilgour and Koslow 2009;",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 680,
                        "end": 702,
                        "text": "Rosengren et al. 2020;",
                        "ref_id": "BIBREF109"
                    },
                    {
                        "start": 703,
                        "end": 725,
                        "text": "Toubia and Netzer 2017",
                        "ref_id": "BIBREF125"
                    },
                    {
                        "start": 820,
                        "end": 855,
                        "text": "Hagendor\u21b5, Fabi, and Kosinski 2023)",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Each pair was assigned to one of five model temperatures -.4, .6, .8, 1.0, 1.2 -and one of 288 prompt combinations, minimizing the chance our results were due to a specific prompt. 23",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "The set of prompts was created by combining 4 \"instructions\" x 9 \"roles\" x 8 \"hypothesis formats\". All prompts began with assigning a role; i.e., \"Assume you are {role}...\" where {role} was replaced with the assigned role, such as \"an editorial strategist focused on digital content optimization.\" All prompts also included a specific hypothesis format that the response should be in. For instance, \"produce [an] insight as a single sentence that begins and ends in this exact format...\" where the format was always one of eight possible formats that started with \"Hypothesis:\" and ended with a reference to the feature's e\u21b5ect on engagement. To illustrate, one of the prompt formats reads: \"Hypothesis: leads to {direction} engagement with a message.\" where {direction} was filled in with the value of another variable -typically \"more [less]\" or \"increases [decreases]\" -depending on whether b m a,b was positive or negative. Thirdly, prompts varied in the information presented as part of the instructions. For example, one of the prompts listed all the constructs from Banerjee and Urminsky (2023) and asked GPT to \"look for patterns not yet known.\" Three of the four instruction templates included the pair of headlines that were meant to be used to generate a hypothesis. The fourth template did not refer to any Upworthy headlines and was included simply as a Control to later assess whether hypotheses generated by GPT with access to our dataset di\u21b5ered from those generated by GPT without any specific headlines. 24 Consistent across all prompts was a list of five criteria we wanted each hypothesis to meet.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Specifically, each hypothesis should be: (i) clear, (ii) generalizable, (iii) empirically plausible,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "22 Note, this is out of abundance of caution. We use only one pair for each hypothesis. It would have been just as feasible to use one pair to generate hypotheses and use di\u21b5erent pairs from the same set for the ranking step below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "23 \"Temperature\" refers to a parameter, ranging from 0-2, that determines the randomness of responses. Lower values produce more consistent outputs while higher values produce responses that are more diverse or 'creative'. \"Prompts\" are the conversational input used query an LLM. For more on prompting see www.promptingguide.ai. 24 Since we planned to exclude these from the rest of the pipeline, prompts that had Control instructions were undersampled before being matched to a pair.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "(iv) unidimensional, and (v) usable (see Appendix 1 for more details and OSF for exact wording).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Output. This step produced 2,100 hypotheses, which appear to be clear and coherent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Examples of hypotheses are included in Table 4 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 45,
                        "end": 46,
                        "text": "4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Additional Checks. To further assess the quality, we recruited 79 participants using Prolific (prolific.com) [June 2024] and had them rate 106 hypotheses on several dimensions (e.g., \"clarity\"; for details, see Appendix 3). On every dimension, the average rating for most hypotheses was above the scale's midpoint. In that survey, we also asked participants whether they believed a given hypothesis could be applied to other contexts, such as \"product descriptions\" or \"billboard advertisements\". Every hypothesis rated seemed as though it could be applied to at least one other context (M = 3.72 out of 7; Mdn = 3.76).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "At least on the surface, these appear to be good quality hypotheses. Each hypothesis is based on an observation made in the dataset, but it is not clear at this point whether the feature that has been identified is representative of a general insight or specific to the single pair of headlines. Furthermore, this task was mainly a creative exercise. Past work supports our findings that LLMs are capable of producing high-quality hypotheses (Banker et al. 2023 ), but it remains uncertain whether these hypotheses lead to a larger discovery.",
                "cite_spans": [
                    {
                        "start": 442,
                        "end": 461,
                        "text": "(Banker et al. 2023",
                        "ref_id": "BIBREF8"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "The next step takes a hypothesis generated from one observation and applies it to a di\u21b5erent set of headlines. It then scores these pairs using the ML algorithm, b m a,b .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Step 2: Ranking Hypotheses",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "This step aims to rank-order hypotheses such that insights most likely to a\u21b5ect the outcome (CTR) are prioritized. To achieve this, we want to leverage the ML algorithm to help identify insights that are predicted to have an e\u21b5ect when applied to several new messages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Ranking is therefore done in two parts, which we refer to as \"morphing\" and \"scoring\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Morphing applies each hypothesis to various headlines. Scoring uses the ML algorithm to predict the di\u21b5erence in CTR between a morphed headline and the original target headline.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Aggregating these \"predicted treatment e\u21b5ects\" (PTEs) at the hypothesis level provides a measure that incorporates both the ML signal and an element of generalizability (since each hypothesis is applied to a random set of headlines). This measure is then used to rank-order the hypotheses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "DATA-DRIVEN HYPOTHESIS DISCOVERY",
                "sec_num": null
            },
            {
                "text": "Procedure. We began with a set of actual headlines from the Upworthy dataset, drawing only from the morphing set to avoid overlapping with the headlines used to generate hypotheses. We randomly selected 120 headlines from this subset, each from a unique \"component\", to be morphed according to each hypothesis.25 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "To generate the morphs, we first matched each of the 2,100 hypotheses to each of the 120 original headlines sampled, producing an intermediate dataset containing 252,000 rows. We then paired each row with one of the three prompts and randomly set the model temperature to .75 or .9. Unlike the prompts used to generate hypotheses, we did not vary the role; instead, all the prompts began with \"Assume you are a copywriter for an online news platform. Here are some examples of recent headlines from your company...\" We then provided three example headlines randomly drawn from the same subset of headlines (excluding those that belonged to the same component as the one we were morphing) to allow for \"few-shot learning\" of what headlines look like in this distribution (e.g., Min et al. 2022; Brown et al. 2020 ). The prompts then explicitly stated, \"You need to rewrite Headline A below according to the given instructions. Keep the content of the story as similar as possible. Respond by writing out Headline B.\" All prompts included an original headline to be rewritten and one hypothesis as the given \"instruction\" for how the headline should be modified (see Appendix 1 for more details). The specific wording of the di\u21b5erent prompts is available on OSF. 26 We used OpenAI's GPT-4-Turbo to generate the morphed headlines.",
                "cite_spans": [
                    {
                        "start": 777,
                        "end": 793,
                        "text": "Min et al. 2022;",
                        "ref_id": "BIBREF88"
                    },
                    {
                        "start": 794,
                        "end": 811,
                        "text": "Brown et al. 2020",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "Output. The output contained 252,156 new headlines. 27 We then applied a heavy filter to remove anomalous responses from this set. First, we removed 37,430 morphs (17.85 per hypothesis) that were longer than 100 characters. Upworthy seems to have used a strict character limit, so any morphed headline with more than 100 characters was considered \"out of distribution\" and removed from the set of morphs to be scored. Second, we removed 956 morphs corresponding to the eight hypotheses produced by the \"Control\" prompt; that is, hypotheses produced without referring to any Upworthy headlines. Third, we removed 63,119 morphs (or 30.1 per hypothesis) generated in response to the prompt that instructed GPT to \"dial down\" the hypothesized feature. 28",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "In the end, we had an average of 72.63 morphed headlines (SD = 11.24; Med = 74)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "for each of the 2,092 hypotheses (excluding those generated using the \"Control\" prompts),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "where each morphed headline is associated with an original Upworthy headline and a specific hypothesis. Table 5 provides a set of examples.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 110,
                        "end": 111,
                        "text": "5",
                        "ref_id": "TABREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "Additional Checks. Although LLMs can produce new coherent text from past examples, it is unclear whether they can perform the current task well. This process assumes that morphs are not only coherent but that they also incorporate the hypothesized feature while keeping the content similar to the original. To check whether the morphs varied according 26 Two of the three prompts produced a single headline. These prompts stated that the aim was to rewrite the headline to maximize engagement, i.e., emphasizing [minimizing] the hypothesized feature when it was hypothesized to increase [decrease] engagement. The third prompt produced two headlines, one in which the feature was supposed to be \"dialed up\" and the other in which it was meant to be \"dialed down\". Although morphed headlines generally varied the hypothesized feature, the direction of the responses was sometimes inconsistent. One reason could be that when a feature is not already present, it is easier to emphasize it than to minimize it. 27 In generating these morphs, we encountered an error partway through the procedure, causing us to terminate the process early. We originally anticipated approximately 336,000 new headlines (since one-third of the prompts produced two headlines). Nevertheless, we randomized the order in which we generated the morphs, which resulted in a uniform sample of morphs for each hypothesis. 28 In a post-hoc analysis, morphed headlines corresponding to the \"Control\" hypotheses and those meant to \"dial down\" a feature were predicted by the ML algorithm to be consistently worse than the matched original headlines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "to the hypothesized feature, we again used GPT, this time to label how much of each feature was present in a given headline on a scale of 0 to 7. Through this assessment, we saw that 53% of morphs were rated as having more of the hypothesized feature compared to the original used to create it (40% of morphs had the same value as the original headline, and only 7% had less of the hypothesized feature). In contrast, only 24% of the morphs had a higher score for the feature of interest when that feature was not one prompted to change, with 57% remaining unchanged and 19% decreasing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "The next part involves using the ML algorithm to score these morphs. However, for the algorithm's predictions to serve as a useful measure, two assumptions must be satisfied:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "First, the algorithm should be reliable. In this case, it should be able to predict CTR out of sample. Second, the morphs generated by GPT must be similar to those found in the Upworthy dataset on which the algorithm was trained (i.e., they should be \"in-distribution\").",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "The first assumption is supported by the results reported above, in the section where we describe the predictive performance of the algorithm. To support the second assumption, we conducted a series of comparisons to check whether the morphs were, in fact, similar to the Upworthy headlines. These checks included a computational check and three pre-registered human assessment experiments. We briefly describe these here, but see the Appendix, Section 3 for more details.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "To validate whether the morphs were similar in meaning to the original used to generate them, we gathered the sentence embedding for the morphed headlines and compared them to embeddings of the original. We also compared the original headlines used to generate morphs to other original headlines found in the same trial and original headlines found in other trials. Morphed headlines were semantically more similar to the original Upworthy headline used to generate it than two original Upworthy pairs were to each other; this was true of original-original pairs from the same trial (i.e., for same story) and from separate trials (i.e., di\u21b5erent stories).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "We also conducted three pre-registered online experiments that asked participants to as-sess a mix of original and morphed headlines. Methods and results for these experiments are provided in more detail in the Appendix (Section 3). On multiple measures of attitude (n = 120), participants considered the morphed headlines equivalent to the Upworthy headlines. When incentivized to identify which headlines were generated by AI (n = 101), participants did not think that the morphs or the Upworthy headlines were AI-generated. Finally, when incentivized to identify which headlines were produced by Upworthy (n = 100),",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "participants believed writers at Upworthy.com wrote both the original headlines and the morph. 29 Together, these findings suggest that the morphs were similar to the originals used to create them, both in content and style.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Morphing",
                "sec_num": "3.2.1"
            },
            {
                "text": "Procedure. Next, we want to know how each morph might perform against the original headline. We use the ML algorithm (described in Section 2.5.2) to predict the CTR for the morphed headline relative to the original Upworthy headline from which it was generated.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "These are, in e\u21b5ect, predicted treatment e\u21b5ects (PTEs). 30 Here, we are primarily interested in the average e\u21b5ect per hypothesis, so we average PTEs at the hypothesis level.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "Output. Scoring produces an average PTE for each hypothesis. This measure captures the e\u21b5ect a hypothesis is predicted to have on average; in this case, when applied indiscriminately to a random set of headlines written for di\u21b5erent stories. This measure is likely to be a conservative estimate because, in practice, not every hypothesized feature will apply to all topics. For example, \"excessive sensationalism\" may not be appropriate for sensitive topics.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "29 When comparing the two sets of headlines to each other, the morphed headlines were relatively more likely to be perceived as AI generated (p = .045) and relatively less likely to be perceived as written by Upworthy (p < .001).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "In both cases, however, the equivalence tests were also significant given equivalence bounds of half a unit on the scale ( .5, .5; see also Lakens 2017) . For more details, see Appendix Section 3. 30 Note, that we have no \"ground truth\" of either CTR (y) or CTR for the morphed headlines, only the ML prediction ( b m a,b ).",
                "cite_spans": [
                    {
                        "start": 140,
                        "end": 152,
                        "text": "Lakens 2017)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "Additional Checks. As one might expect, not every hypothesis was predicted to have a positive e\u21b5ect. In fact, most hypotheses applied to a random set of headlines were predicted to have a negative e\u21b5ect. Across all hypotheses, the average PTE was .00065 (SD = .00086; Mdn = .00070). This result corresponds to a decrease in CTR of approximately 4% relative to the average CTR in the original dataset (see Table 2 ). The predicted di\u21b5erences at the morph-original headline pair level show that the average PTE was .00065 (SD = .00279, Mdn = .00061). Furthermore, the standard deviation of predicted di\u21b5erences at the morph-original headline pair level is .00279, which is 77% the size of the standard deviation in the ML predictor and 44% the size of the standard deviation in CTR. This spread indicates that this process creates morphs with nearly as much variation as we see in predictions from the original dataset.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 411,
                        "end": 412,
                        "text": "2",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "Interpreting these results is di cult because there is no equivalent measure to compare this to in the original dataset. The average PTE is an attempt to estimate the average e\u21b5ect if many A/B tests were conducted to test the same hypotheses applied to many stories. In the original data, each pair presumably tests something di\u21b5erent, if not multiple hypotheses at once (Koning, Hasan, and Chatterji 2022) .",
                "cite_spans": [
                    {
                        "start": 371,
                        "end": 406,
                        "text": "(Koning, Hasan, and Chatterji 2022)",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "Nevertheless, we conducted an additional check, a modified specification curve analysis (Simonsohn, Simmons, and Nelson 2020) , where we consider the complete set of average PTEs jointly and ask, how inconsistent are these results with the null distribution? We construct the null distribution for this data by resampling under-the-null. Specifically, we randomly reshu\u270fe the hypotheses, \"assigning\" them to new morph-original headline pairs, then calculate the average PTEs for each hypothesis. This reshu\u270fing preserves features of the morph-headline pairs; however, now we know the null is true by construction since there is no link between (shu\u270fed) hypotheses and the PTEs. Repeating this exercise many times produces a distribution of average PTEs under the null. Figure 2 shows that the distribution of average PTEs in the observed data is much steeper than those in the null distribution, which suggests that our scoring method is aggregating information about the hypotheses beyond what we might expect by chance.",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 125,
                        "text": "(Simonsohn, Simmons, and Nelson 2020)",
                        "ref_id": "BIBREF118"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 776,
                        "end": 777,
                        "text": "2",
                        "ref_id": "FIGREF7"
                    }
                ],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "Step 3: Filtering Hypotheses",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "The final step narrows the set of hypotheses. 31 We implemented two data-driven techniques that others can use-the first technique involved clustering similar hypotheses, and the second tested whether the average predicted treatment e\u21b5ect for a given hypothesis was meaningfully di\u21b5erent from zero. Beyond these two filters, researchers could apply others. The purpose of the filters is to encourage researchers to transparently document the dimensions they use to select and exclude hypotheses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring",
                "sec_num": "3.2.2"
            },
            {
                "text": "While this process can generate a large number of hypotheses, many of them are similar.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustered Selection",
                "sec_num": "3.3.1"
            },
            {
                "text": "For example, \"an element of surprise followed by a cli\u21b5hanger\" and \"an element of suspense and an unexpected outcome\" are two of the hypotheses in our sample. Even though each hypothesis originated from a unique pair of headlines, di\u21b5erent pairs may have nonetheless varied on the same dimension. An organization like Upworthy may have seen that humor works in one A/B test and decided to try it again in another. It is also possible that copywriters and editors have their own style or deliberate writing strategies that may appear across multiple trials. A third reason for the overlap could be related to the task. By soliciting a single feature that could apply in other contexts, GPT is e\u21b5ectively \"forced to choose\" a salient feature amongst many that may be present.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustered Selection",
                "sec_num": "3.3.1"
            },
            {
                "text": "Procedure. To account for similar hypotheses, we use a sequential selection strategy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustered Selection",
                "sec_num": "3.3.1"
            },
            {
                "text": "First, we calculate a reference vector for each hypothesis by taking the di\u21b5erence in embedding space between a headline and its associated morph (using the embeddings derived from the fine-tuned embedding model mentioned above, see Section 2.5.2) and averaging the di\u21b5erences at the hypothesis level. Next, we order the list of hypotheses according to their average PTE and select the hypothesis at the top of this list. We then use each hypothesis's reference vector to calculate the pairwise distance between the selected hypothesis and every other hypothesis on the list. We exclude hypotheses further down the list if they are similar to one of the ones already selected. For instance, if the first hypothesis suggests that \"Framing a message with an element of surprise followed by a cli\u21b5hanger\" increases engagement, it gets selected because it is first. The second -\"incorporating a personal anecdote or reaction increases engagement with a message.\" -also gets selected for being di\u21b5erent from the first.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustered Selection",
                "sec_num": "3.3.1"
            },
            {
                "text": "The third, however, refers to an \"element of suspense and an unexpected outcome,\" which is similar to the first, so it is not selected.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustered Selection",
                "sec_num": "3.3.1"
            },
            {
                "text": "Whether or not two hypotheses are similar is determined computationally according to a distance parameter, \", which we set at .03. 32 This process is repeated for each hypothesis until a set number of hypotheses are selected or the list is exhausted, and every hypothesis is either selected or grouped with one of the ones selected.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustered Selection",
                "sec_num": "3.3.1"
            },
            {
                "text": "Output. The result is a list of hypotheses, arranged by average PTE, with a pairwise distance of at least \" = .03 between their reference vectors. Clustering reduced our hypothesis set from 2,092 to 205 (for selection and respective clusters, see the \"hypothesis\" dataset on OSF).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Clustered Selection",
                "sec_num": "3.3.1"
            },
            {
                "text": "Procedure. With fewer hypotheses, we turned to test whether PTEs were significantly greater than zero. 33 For each of the 205 hypotheses, we conducted a one-sample, one-sided 32 This parameter determines the distance threshold. Lower values mean hypotheses need to be close together to be counted as the same, which results in fewer observations per cluster and, therefore, more clusters. Higher values cluster more broadly but risk grouping a truly novel or unique hypothesis with more common ones. To select .03, we tried di\u21b5erent values and picked one that we believed was selecting a unique set of hypotheses. 33 We used zero as the null because our algorithm was trained on a normalized outcome measure of di\u21b5erences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Significance Testing",
                "sec_num": "3.3.2"
            },
            {
                "text": "Indeed, when we look at the di\u21b5erence in predicted CTR among pairs of headlines within the validation set, the mean and median \"treatment e\u21b5ect\" are both e\u21b5ectively 0 (< .000001).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Significance Testing",
                "sec_num": "3.3.2"
            },
            {
                "text": "We then applied a False Discovery Rate (FDR) correction using the stats package in R (Benjamini and Hochberg 1995) .",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 114,
                        "text": "(Benjamini and Hochberg 1995)",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "t-test.",
                "sec_num": null
            },
            {
                "text": "Output. Sixteen hypotheses had average PTEs that were positive and significantly greater than zero after correcting for the FDR (p < .05); seven at p < .001. These are displayed in Table 6 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 187,
                        "end": 188,
                        "text": "6",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "t-test.",
                "sec_num": null
            },
            {
                "text": "Reading these hypotheses, it is clear that some overlap despite the clustering in the earlier step. For instance, \"the utilization of multimedia elements such as gifs influences engagement with a message\" is similar to \"incorporating multimedia evidence in a headline results in more engagement with a message.\" 34 Therefore, we hand-picked four to test (from the set of seven at p < .001) out-of-sample, using the regression set that was set aside from the start.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "t-test.",
                "sec_num": null
            },
            {
                "text": "We also selected two hypotheses with average PTEs less than zero for robustness. For this, we repeated the clustering step, starting from the bottom (most negative average PTEs)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "t-test.",
                "sec_num": null
            },
            {
                "text": "and then performed the same significance testing procedure on the 212 hypotheses that remained (testing for whether average PTE was less than zero). There were 114 hypotheses with average PTEs less than zero (p < .001). From these, we hand-picked two.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "t-test.",
                "sec_num": null
            },
            {
                "text": "This process has produced a set of interpretable hypotheses which, according to the ML algorithm, are predicted to a\u21b5ect engagement as measured through CTR. Together, these steps o\u21b5er a systematic approach for aggregating insights from several marketing experiments. Importantly, these steps constitute a data-driven framework for generating hypotheses before any confirmatory tests are conducted. Hypothesis testing comes next.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": null
            },
            {
                "text": "34 A higher \u270f could have prevented this but at the increased risk of clustering a new hypothesis in with the old.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": null
            },
            {
                "text": "To test our hypotheses -and assuage any concerns of overfitting or p-hacking (Simmons, Nelson, and Simonsohn 2021; Wicherts et al. 2016 ) -we pre-registered the six hypotheses and conducted all of our tests out of sample, on data that was intentionally left untouched in all the preceding steps for generating the hypotheses. Hypotheses were generated transparently through the process described above and pre-registered as they came, further restricting our degrees of freedom (Kerr 1998; Schaller 2016; Landy et al. 2020 ). The pre-registration of this analysis is available on AsPredicted.org/S6H ZPF (#172038).",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 114,
                        "text": "(Simmons, Nelson, and Simonsohn 2021;",
                        "ref_id": "BIBREF117"
                    },
                    {
                        "start": 115,
                        "end": 135,
                        "text": "Wicherts et al. 2016",
                        "ref_id": "BIBREF131"
                    },
                    {
                        "start": 478,
                        "end": 489,
                        "text": "(Kerr 1998;",
                        "ref_id": "BIBREF64"
                    },
                    {
                        "start": 490,
                        "end": 504,
                        "text": "Schaller 2016;",
                        "ref_id": "BIBREF112"
                    },
                    {
                        "start": 505,
                        "end": 522,
                        "text": "Landy et al. 2020",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "HYPOTHESIS TESTING USING HOLD-OUT SET",
                "sec_num": null
            },
            {
                "text": "We followed a standard procedure for testing the hypotheses. First, we had humans code di\u21b5erent headlines based on the hypothesized feature. Then, we estimated an OLS regression to test whether varying the feature led to a di\u21b5erence in engagement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "The Upworthy dataset has the advantage of being a dataset of randomized experiments.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "While the experiments were not originally designed to test our hypotheses explicitly, we can still assess whether the features identified in this process a\u21b5ect consumers' propensity to engage with a message. Furthermore, since the pairs of headlines within a trial were written for the same news story, we e\u21b5ectively control for various confounds due to the topic by studying the di\u21b5erences between headlines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "For testing, we used the regression set. This set contains pairs from 1,693 trials. Note that none of these trials (or headlines within the trials) overlap with the trials (headlines) used to train the ML algorithm, generate hypotheses, or generate morphs. Where we used this set before was to gather human labels (see Section 2.4.3). We decided to use the same set of 1,693 pairs (3,386 headlines) so that we could also compare whether these new features captured any signal in the humans' intuition from earlier.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "We pre-registered our procedure and the six selected hypotheses, noting that while the experimental data had already been collected, we had not conducted any coding of the hypothesized features in the regression set. Materials for this survey are available on OSF.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "The plan was to recruit 800 participants. Each participant saw 26 headlines, each on a separate page, randomly drawn from the set of 3,402. For each headline, participants were asked to \"select the level which each trait is featured in this headline, from '1 (Low)' to '7 (High)'.\" There was also an option to select \"0\" to indicate the trait was not present.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "The traits (i.e., features) were listed by their shorthand: (i) includes element of surprise followed by cli\u21b5hanger, (ii) incorporates parody, (iii) refers to multimedia evidence, (iv) describes physical reaction, (v) short and simple phrases, (vi) focus on positive aspects of human behavior. 35To supplement the main set of tests, we also had participants rate an additional four headlines after the 26 from the validation set. The additional four headlines were drawn from the 117 original headlines used for morphing and the 404 morphed headlines corresponding to each of the six selected hypotheses. These ratings were meant to check whether the morphing procedure morphed the headlines on the feature it was meant to. We noted in the pre-registration that this analysis was intended as exploratory and do not report that analysis here (but see Appendix 3.2.5).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "We recruited 800 participants (M age = 41.51, SD = 13.75; 379 Male, 401 Female, 20",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Self-Identified; 62.4% White, 14% Black, 9% Latin American, 5.4% Multi-racial, 9.3% all others) on Prolific. Altogether, participants provided 144,000 labels (124,800 for headlines in the regression set, 4,212 for headlines in the morph set, and 14,988 for morphed headlines).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Each headline was rated on each feature a median of six times (IQR: 4, 8).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "To test each of the six hypotheses, we estimate six OLS regressions:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "CTR a,b = 0 + r \u2022 Rating a,b + \" a,b ,",
                        "eq_num": "(4)"
                    }
                ],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "where Rating represents participants' mean rating of headline H b minus the mean rating of headline H a for each hypothesized feature, and r is the coe cient related to the rated feature.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Table 7 displays the estimated coe cients for each regression. We rescaled the outcome variable, CTR, by dividing it by the standard deviation of CTR and normalized the hypothesized features to have unit variance. Therefore, one standard deviation increase in Rating in any of the hypothesized features produces an estimated change in CTR equivalent to \u02c6 times the standard deviation in CTR.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "7",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "Four of the six hypothesized features were significant predictors (p < .05; two p < .001) of the outcome. A fifth had a marginal e\u21b5ect (p = .094). Of these five, all showed e\u21b5ects in the predicted direction. Furthermore, when we fit a regression with all the predictors included, four of the six are significant (p < .05; two p < .001), suggesting these features capture distinct signals in the text. 36 Through this process, we have made a discovery. A question remains as to whether any of these are novel. Statistically, we estimate another set of regressions where we include the prediction from the \"known features\" derived from Banerjee and Urminsky (2023), which was estimated on the training partition (i.e., d BU The results of these regressions are available in Table 9 . Two of the six features continue to be significant predictors, surprise, cli\u21b5hanger and reference to multimedia evidence (ps < .01). A similar pattern holds when we include all six features plus the d BU in a single model, where three of the six features are significant predictors (p < .05).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 778,
                        "end": 779,
                        "text": "9",
                        "ref_id": "TABREF11"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "In another specification, we compare a baseline model that regresses CTR on all 51",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "36 Though note that parody is not significant on its own but is a significant predictor when controlling for the other features, albeit in the direction opposite the one predicted. In contrast, physical reaction is significant on its own, but not when adjusting for the other features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "features from Banerjee and Urminsky (2023) (see Equation 3) to one that also includes one of the new features. The models which included ratings for surprise, cli\u21b5hanger (F (1, 1640) = 11.37, p < .001) and multimedia evidence (F (1, 1640) = 21.95, p < .001), respectively, significantly outperformed the baseline. Together, this suggests that this process has uncovered at least two features that explain the outcome above and beyond what was known.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "In a third set of regressions, we check whether these features are capturing any signal from the ML algorithm. We regress b m a,b on Rating a,b . All six features on their own were significant predictors of the ML prediction, b m, at p < .001, with Adjusted R 2 ranging from .006 to .025. A model with all six features produced an Adjusted R 2 = .098. The results of these regressions are available in Table 10 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 408,
                        "end": 410,
                        "text": "10",
                        "ref_id": "TABREF12"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "As a follow-up analysis, we consider the e\u21b5ect on CTR if we treat headlines within a pair as being higher or lower on a given feature. Doing so allows us to analyze the data \"as-if\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "we had pre-tested a set of headlines written for the same story and assigned the ones with the higher feature to one group and the lower feature to another. We could then ask: what is the average e\u21b5ect on CTR of seeing a headline from the high feature condition compared to the low? Figure 3 displays the average change in CTR from moving from a headline lower on the feature to one that was higher. The largest e\u21b5ect comes from referencing multimedia evidence, which increases CTR by 4.2% on average compared to the average CTR of the \"low feature\" group.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 290,
                        "end": 291,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "These results provide causal evidence against the null for several hypotheses. Using data from nearly 2,000 digital experiments, we find that varying the feature between the two headlines led to a significant di\u21b5erence in CTR in at least four of the six hypothesized features we tested (a fifth showed a marginal e\u21b5ect). These are, however, only six from a set of dozens of hypotheses generated. In the Appendix, we provide results for the same set of tests conducted on a random set of 400 hypotheses. 37 The pattern of results is similar; among the top decile of hypotheses predicted through our ranking procedure to have meaningful e\u21b5ects on CTR, we find evidence in support of 75% of them (ps < .05 after FDR correction; 28% at ps < .001).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": null
            },
            {
                "text": "Whether these are novel, generalizable, and of general interest remains an open set of questions. On the question of novelty, we provide a partial answer. Statistically, at least two featuressurprise, cli\u21b5hanger and multimedia reference -appear to capture information that is su ciently di\u21b5erent from the 51 psychological constructs derived in Banerjee and Urminsky ( 2023). Nevertheless, one could argue that these features appear similar to insights already known. More empirical work is needed to answer this, so we leave this to future research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": null
            },
            {
                "text": "We explore the generalizability of these hypotheses in the next section and return to the question of whether they are interesting in the General Discussion, where we consider striking a balance between basic theoretical insights and more applied insights.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": null
            },
            {
                "text": "Behavioral interventions that work in one context may not produce the same e\u21b5ects in another (Goswami and Urminsky 2022; Landy et al. 2020) . Nevertheless, researchers are often interested in coming up with and testing hypotheses that are broadly applicable.",
                "cite_spans": [
                    {
                        "start": 93,
                        "end": 120,
                        "text": "(Goswami and Urminsky 2022;",
                        "ref_id": null
                    },
                    {
                        "start": 121,
                        "end": 139,
                        "text": "Landy et al. 2020)",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERALIZING HYPOTHESES TO NEW CONTEXTS",
                "sec_num": null
            },
            {
                "text": "Therefore, to investigate the generalizability of our hypotheses, we partnered with an online entertainment company to test whether the hypotheses generated in one context (Upworthy headlines) could inform interventions in another entirely di\u21b5erent context. 38",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERALIZING HYPOTHESES TO NEW CONTEXTS",
                "sec_num": null
            },
            {
                "text": "37 As described in the Appendix, we used ratings collected using GPT instead of humans for this analysis. Also in the Appendix is an analysis comparing GPT to human ratings (see also Rathje et al. 2023) . 38 In the Appendix, we provide tests for a third dataset consisting of A/B tests conducted by a non-profit organization focused on progressive outreach. The context is su ciently di\u21b5erent from either of the two reported here that we felt it would require a longer discussion to contextualize the hypotheses and the results (Goswami and Urminsky 2020, 2022; Markowitz and Shulman 2021).",
                "cite_spans": [
                    {
                        "start": 183,
                        "end": 202,
                        "text": "Rathje et al. 2023)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERALIZING HYPOTHESES TO NEW CONTEXTS",
                "sec_num": null
            },
            {
                "text": "The data from the online entertainment company consists of social media posts. The posts resemble the Upworthy headlines in style and content. Where Upworthy headlines tend to focus on uplifting stories, the content from this company emphasizes popular culture and entertainment, including sports, movies, music, and celebrities. The data we obtained contains a total of 553,328 di\u21b5erent social media posts for various articles hosted on their website between July 2022 and February 2023. We partitioned the data following a similar process we used for the Upworthy data; here, 5,077 posts were split to test the hypotheses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": null
            },
            {
                "text": "Unlike the Upworthy dataset, the posts were not part of a randomized trial. Therefore, our primary outcome is the CTR (not CTR), defined here as the total clicks divided by the total reach. We applied the same smoothing function used above (see Equation 1). More details about this dataset, including summary statistics of key variables and descriptions of the secondary outcomes we examined, are available in the Appendix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": null
            },
            {
                "text": "For this analysis, we followed a procedure similar to the one above (see Hypothesis Testing section). Again, we pre-registered our procedure and hypotheses on AsPredicted.org/FN5 CNG (#181144), keeping the same six hypotheses uncovered and tested above. We kept the direction consistent for simplicity, but note here that behavioral interventions often have di\u21b5erent e\u21b5ects across di\u21b5erent people and contexts (Goswami and Urminsky 2022; Markowitz and Shulman 2021). Our primary interest was in seeing whether the hypothesized feature(s) influenced the outcome, click-through rate (CTR). That is, whether hypotheses generated in one dataset could predict outcomes in another.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "We planned to recruit 900 participants to rate the 5,077 social media posts. The survey format was the same as the task above. Each participant saw 30 messages, each on a separate page, randomly drawn from a set of 5,077. For each message, participants were asked to \"select the level which each trait is featured in this headline, from '1 (Low)' to '7 (High)'.\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": null
            },
            {
                "text": "Demographic details for this study are reported in the Appendix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "We estimated OLS regressions following a similar specification in Equation 4 to test each of the six hypotheses. One notable exception is that we regressed the CTR on the average rating; we did not di\u21b5er in the variables as we did for Upworthy since the posts were not paired.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "The results are displayed visually in Figure 4 together with the results from the outof-sample Upworthy tests for comparison (for table of coe cients, see Appendix). In the social media posts, four out of the six hypothesized features were significant predictors of CTR (ps < .01), including (1) multimedia evidence, (2) physical reactions, (3) short, simple phrases, and (4) a focus on positive, human behavior. These are consistent with the evidence found in the Upworthy data, except for multimedia, for which the e\u21b5ect is in the opposite direction, and surprise, cli\u21b5hanger, for which there is a null e\u21b5ect.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 45,
                        "end": 46,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": null
            },
            {
                "text": "We tested hypotheses generated in one context using a second dataset. The aim was to explore whether the hypotheses generated through our framework were specific to a time and place or whether they might generalize to other contexts. Of the six hypotheses we selected for testing, four appear to predict the e\u21b5ects of language in another context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discussion",
                "sec_num": null
            },
            {
                "text": "This paper presents a novel framework marketers could use to generate hypotheses from text data. Our approach integrates large-language models, machine-learning tools, and psychology experiments to produce hypotheses that are both novel and interpretable. By starting with unstructured data such as text messages, emails, social media posts, or headlines, our framework outputs hypotheses that are interpretable, novel, testable, and generalizable to other contexts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "We demonstrate how to use this framework by applying it to a specific case: uncovering features of language present in a message that a\u21b5ect consumers' propensity to engage with it. Through our process, we produced dozens of hypotheses and selected six to test in this paper.39 Four were predicted to increase engagement: 1) framing a message with an element of surprise followed by a cli\u21b5hanger, 2) incorporating a concept of parody, 3) incorporating multimedia evidence, and 4) describing physical reactions. Two were predicted to decrease engagement: 5) shortening and simplifying phrases and 6) focusing on positive aspects of human behavior. When we tested these hypotheses out of sample, using pairs of headlines from a hold-out set of A/B tests, we found causal evidence supporting five out of six hypotheses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "The hypotheses derived from our framework have practical implications, serving as meaningful predictors of engagement as measured through click-through rates (CTR). These hypothesized features not only capture variation in CTR in the context in which they were discovered but also predict the CTR in other contexts. For instance, using social media posts from an online entertainment company, we found significant correlational evidence supporting four of the six hypotheses above. The evidence that these hypotheses extend to new contexts suggests that companies with multiple messaging channels or several brands can leverage our framework to inform a broader marketing strategy.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "Beyond the specific hypotheses uncovered for this application, this paper illustrates how marketing researchers and organizations could use these tools to generate new insights into what drives consumers' behavior. We consider a few cases here for how and when others might use these tools. In all three cases, one needs a corpus of text. In two of the three cases, one needs an outcome variable that they can statistically predict. We focused on CTR because it is a meaningful engagement metric for media companies whose revenue models often rely on page visits. However, other companies will have other metrics they care about, and our framework easily accommodates this.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "The first use case looks similar to ours. It starts with a large corpus of text with a corre-sponding outcome of interest. Through the same steps, one could generate new hypotheses, rank them using an algorithm trained on their data, and filter the hypotheses for testing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "Researchers interested in studying consumer language could also apply this approach to existing datasets to explore new research directions. 40",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "As we have demonstrated, this approach is useful for aggregating insights across many generate hypotheses using messages in one domain (or from across them all) while ranking them separately according to each context or group. For this, they will need a set of example messages from the relevant domains for morphing and domain-specific algorithms for scoring.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "Notice that access to the outcome variable is not required for generating hypotheses, meaning an organization could just as easily use messages from elsewhere to generate hypotheses and rank the hypotheses using an internal algorithm (for which they do have access to the outcome). An advantage of mixing and matching datasets for di\u21b5erent steps is that it allows for discovering insights not available in the target dataset. Data-driven approaches, such as the one proposed here, will only uncover insights present in the data. However, one can expand the space of possible discoveries by generating hypotheses in one domain and then using them to morph and score messages in another.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "The third use case involves generating hypotheses on messages without access to the outcome variable. Without an outcome variable, one cannot train a machine-learning algorithm, as we did here. While this limits one's ability to rank hypotheses based on a predicted e\u21b5ect, it does not preclude drawing insights from existing messages in the form of hypotheses using an LLM. For example, companies looking to glean insights into a competitor's strategy could generate hypotheses from others' above-the-line marketing campaigns, such as promotional emails and social media posts. Alternatively, companies could generate hypotheses from customer complaints or product reviews. Finally, companies brainstorming new ideas for what to test can use this approach to broaden their options using data from elsewhere. These uses are similar to others where text mining is used for market research (see also Netzer et al. 2012; Hewett et al. 2016; Brand, Israeli, and Ngwe 2023; Hauser, Tellis, and Gri n 2006) .",
                "cite_spans": [
                    {
                        "start": 897,
                        "end": 916,
                        "text": "Netzer et al. 2012;",
                        "ref_id": "BIBREF93"
                    },
                    {
                        "start": 917,
                        "end": 936,
                        "text": "Hewett et al. 2016;",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 937,
                        "end": 967,
                        "text": "Brand, Israeli, and Ngwe 2023;",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 968,
                        "end": 999,
                        "text": "Hauser, Tellis, and Gri n 2006)",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "In academic settings, researchers (and reviewers) could generate hypotheses using a set of stimuli to explore potential confounds or alternative explanations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "An obvious limitation of any data-driven approach is that they are inherently data-driven (as opposed to theory-driven approaches, which start from existing literature or a standard model of the world). The benefit of data-driven approaches is that one starts with an observation. The e\u21b5ect is there; subsequent questions focus on describing how general it is and examining its causes and consequences. The downside of data-driven approaches is that without any background knowledge, it can be hard to contextualize observed e\u21b5ects or generalize them to new contexts without further testing. We see an example of this in the case of the multimedia feature; even though the feature significantly predicts the outcome in two domains, more research could help to reconcile the fact that the observed e\u21b5ect is in opposite directions. Furthermore, related insights run the risk of \"talking past\" each other An open question remains regarding the right \"level\" of a hypothesis. In setting up the procedure, we iterated on the prompts before landing on a set where the LLM responded with a hypothesis in a format we felt resembled hypotheses found in past papers. 41 There were two dimensions we attempted to balance. The first dimension maps onto discussions about \"basic\", or theoretical, insights versus insights that are more \"applied,\" or substantive (e.g., Lynch et al. 2012; Blanchard et al. 2022 ). The hypotheses generated through this process are more substantive than theoretical -this was intentional. We aimed to generate hypotheses that were \"empirically plausible\" (Ludwig and Mullainathan 2024) or able to be observed in the data without needing additional background knowledge. While o\u21b5-theshelf LLMs could conceivably draw on existing knowledge to produce more theoretically rich hypotheses (Yiu, Kosoy, and Gopnik 2023) , leaning into this would increase the chance the LLMs \"hallucinated\" or drew insights from a world model di\u21b5erent from our own (Vafa 41 For some prompts, we even included these examples explicitly, for instance, \"merely measuring intent will increase subsequent purchase behavior\" (Morwitz, Johnson, and Schmittlein 1993) and \"how people monitor their progress toward goal completion influences their motivation\" (Koo and Fishbach 2012) . Another prompt included definitions provided in Banerjee and Urminsky (2023) for specific constructs, e.g., \"Location: About the location or position of something or about location in general.\"",
                "cite_spans": [
                    {
                        "start": 1356,
                        "end": 1374,
                        "text": "Lynch et al. 2012;",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 1375,
                        "end": 1396,
                        "text": "Blanchard et al. 2022",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 1802,
                        "end": 1831,
                        "text": "(Yiu, Kosoy, and Gopnik 2023)",
                        "ref_id": "BIBREF133"
                    },
                    {
                        "start": 2114,
                        "end": 2154,
                        "text": "(Morwitz, Johnson, and Schmittlein 1993)",
                        "ref_id": "BIBREF90"
                    },
                    {
                        "start": 2246,
                        "end": 2269,
                        "text": "(Koo and Fishbach 2012)",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "et al. 2024). The second dimension is one of complexity. As with any pair of messages, most pairs of headlines vary several things at once. It is conceivable that the hypotheses generated could reflect this complexity; in fact, some did specify interactions (e.g., \"using first-person narration and acknowledging personal change in beliefs leads to less engagement with a message,\" emphasis added). However, more complex psychological hypotheses are theoretically possible (Adolphs et al. 2016; Peterson et al. 2021 ). An early prompt we tried produced an example of what a more complex hypothesis might look like: \"begin with a positive emotional state and then transition to a negative one, depicting a journey of emotional upheaval.\" By choosing prompts for which the outputs were both empirically plausible and not overly complex, we may have shifted the distribution of hypotheses to be more substantive than theoretical. Nevertheless, as a result, the hypotheses are simpler to read, easier to test with available data, and written at a level that others can use.",
                "cite_spans": [
                    {
                        "start": 473,
                        "end": 494,
                        "text": "(Adolphs et al. 2016;",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 495,
                        "end": 515,
                        "text": "Peterson et al. 2021",
                        "ref_id": "BIBREF100"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "The framework in this paper is intended to be built on. Some of the technologies used in this paper are still in their infancy but are developing rapidly. The uncertainty of this development makes it di cult to predict what this will mean for the pipeline described above. However, we could comment on how our process might be extended with existing applications. Two extensions, in particular, seem ripe for future work. One extension is to our approach to generating hypotheses (Step 1). The current approach uses GPT-4-Turbo as it comes; however, one could fine-tune these models further to specific domains. Banker et al. ( 2023) does this, using abstracts in published and unpublished psychology papers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "Extending our pipeline using a similar approach could provide theoretically richer insights while helping to identify gaps in existing knowledge (see also Sourati and Evans 2023) . The second extension is to the ranking step (Step 2); the current approach uses a machinelearning prediction to score and rank-order hypotheses. Iterating through this process using",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 178,
                        "text": "Sourati and Evans 2023)",
                        "ref_id": "BIBREF120"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "an algorithm like the one used in Zhou et al. (2024) could facilitate the search process.",
                "cite_spans": [
                    {
                        "start": 34,
                        "end": 52,
                        "text": "Zhou et al. (2024)",
                        "ref_id": "BIBREF134"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "Other methods of selection could also help, such as using a panel of everyday consumers or domain experts to evaluate the set of hypotheses (e.g., Toubia and Flor\u00e8s 2007; Otis 2022; DellaVigna, Pope, and Vivalt 2019; Camerer et al. 2016; Landy et al. 2020) .",
                "cite_spans": [
                    {
                        "start": 147,
                        "end": 170,
                        "text": "Toubia and Flor\u00e8s 2007;",
                        "ref_id": "BIBREF124"
                    },
                    {
                        "start": 171,
                        "end": 181,
                        "text": "Otis 2022;",
                        "ref_id": "BIBREF98"
                    },
                    {
                        "start": 182,
                        "end": 216,
                        "text": "DellaVigna, Pope, and Vivalt 2019;",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 217,
                        "end": 237,
                        "text": "Camerer et al. 2016;",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 238,
                        "end": 256,
                        "text": "Landy et al. 2020)",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "In this paper, we have used relatively short bits of text, ranging from a single word to a couple of sentences. LLMs' current ability to handle much longer texts suggests one may be able to use resumes or application letters, for instance, to generate new insights into who gets hired or admitted to specific roles. However, whether the insights produced will be useful remains an open empirical question.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "Like the work of those who came before us, we hope this paper engenders more innovation in methods that produce interpretable hypotheses from unstructured data sources, especially text. There is still much to learn about how language shapes behavior. This paper provides a framework to help convert language in everyday text to interpretable marketing insights.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "While this framework can augment the current approach to generating hypotheses, it does not preclude the need for careful testing. It would be remiss to confuse one for the other.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "This paper o\u21b5ers structure to the former. Note: Here we have combined treatment arms within a trial that had the same headline. Introducing a specific incident and posing a direct question leads to more engagement with a message.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "GENERAL DISCUSSION",
                "sec_num": null
            },
            {
                "text": "Using conversational language and making a confident prediction about audience enjoyment leads to more engagement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Con v ersationalLanguage + Confident Prediction",
                "sec_num": "9"
            },
            {
                "text": "Describing physical reactions makes a message more engaging.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Physical Reactions",
                "sec_num": null
            },
            {
                "text": "Using direct addressing and provocative language influences engagement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "X Direct & Provocative Language",
                "sec_num": null
            },
            {
                "text": "Incorporating taboo topics and invoking curiosity leads to more engagement.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Taboo Topics + Curiosity",
                "sec_num": null
            },
            {
                "text": "Using a first-person narrative a\u21b5ects engagement with a message. General Accusation to Specific Anecdote Shifting the focus of a message from a general accusation to a specific anecdote a\u21b5ects engagement with a message.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "First-Person",
                "sec_num": null
            },
            {
                "text": "Incorporating visual elements and invoking curiosity leads to more engagement with a message. Mistake + Long-Term Consequence Using a narrative that includes a mistake and its long-term consequences makes people more likely to engage with a message.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Lang + Curiousity",
                "sec_num": null
            },
            {
                "text": "Note: Two additional hypotheses, predicted to have a negative e\u21b5ect, were also selected for testing. 1) Shortening and simplifying phrases a\u21b5ects engagement with a message. and 2) Focusing on positive aspects of human behavior a\u21b5ects engagement with a message. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Visual Lang + Curiousity",
                "sec_num": null
            },
            {
                "text": "We use large language models to generate hypotheses, produce morphs, and rate di\u21b5erent pieces of text on various hypotheses. For each of these tasks, we require a prompt, to guide the language model's output. In order to minimize the dependence of any results on a particular prompting approach, we also introduce some randomization in the prompting process. In this section, we include a full base prompt for each task, and outline the variations applied to the base prompt. The full materials will be made available through the OSF: https://osf.io/d5xvb/?view only=301ca63ed1004401adb697a625ff8d61.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 1: PROMPTING MATERIALS",
                "sec_num": null
            },
            {
                "text": "Our prompt for generating hypotheses takes a pair of headlines, H A and H B , from the same A/B test as input. It specifies that the language model should identify a feature that changed moving from H A to H B . In addition, it provides additional context by specifying a role for the language model and a structure for the hypothesis. We also impose some requirements on quality, to ensure that the resulting hypothesis satisfied our goals of clarity, generalizability, empirical plausibility, unidimensionality, and usability. The basic prompt format is shown Section 1.2.1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "Within this format, we then varied multiple elements. Firstly, we randomized the role, including an editor or communication scientist for example. Secondly, we varied the hypothesis structure by providing di\u21b5erent specific endings. Thirdly, we included more or less information for GPT by possibly giving examples of previous hypotheses, examples of \"known constructs\" which GPT was instructed to avoid, or removing the example headlines (to serve as a control). Below, we include some examples or an excerpt from each type of randomization.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "\u2022 Preamble: One of nine di\u21b5erent preambles was selected, to encourage analytical thought. Examples include:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "1. an editor of a top marketing journal such as the Journal of Consumer Research or the Journal of Marketing, 2. a communication scientist researching the e\u21b5ects of linguistic framing on reader perception, and 3. a consumer psychology expert specializing in persuasive messaging.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "\u2022 Hypothesis structure: One of eight di\u21b5erent hypothesis structures was selected, to force a format for the output hypothesis that was compatible with later analysis.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "The {direction} key was filled in with the \"more [less]\" or \"increases [decreases]\" depending on whether b m a,b was positive or negative. Examples include:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "1. Hypothesis: leads to {direction} engagement with a message.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "2. Hypothesis: makes people direction likely to engage with a message.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating hypotheses",
                "sec_num": null
            },
            {
                "text": "influences engagement with a message.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": "3."
            },
            {
                "text": "\u2022 Variations: We also created three additional variations to the base prompt.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": "3."
            },
            {
                "text": "1. Control: This variation did not refer to any Upworthy headlines and was included to later assess whether hypotheses generated by GPT with access to our dataset di\u21b5ered from those generated by GPT without any specific headlines.1 ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis:",
                "sec_num": "3."
            },
            {
                "text": "In this variation, we included some examples of ideal hypotheses. This included \"taking photos with the intention to share will induce self-presentational concern and generate disutility, thus actually decreasing enjoyment of the current experience\" and \"perception of moving at faster speed results in more abstract mental representation and choices consistent with desirability\", for example.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Examples:",
                "sec_num": "2."
            },
            {
                "text": "In this variation, we included some known constructs, sourced from the BU analysis. This included Reading Ease: Simpler and easier to read and understand and Common Words: Contains more simple or common words, for example.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Known constructs:",
                "sec_num": "3."
            },
            {
                "text": "The complete set of prompts was made by taking the base prompt format, sampling one of the 9 preambles, one of the 8 structures, and one of the 4 variations (the three listed, plus the possibility of no variation).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Known constructs:",
                "sec_num": "3."
            },
            {
                "text": "Assume you are { preamble }.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompt format",
                "sec_num": "1.1.1"
            },
            {
                "text": "Below are two headlines . Assume that both are alternative headlines for the same news story .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompt format",
                "sec_num": "1.1.1"
            },
            {
                "text": "Your task is to identify what has changed from Headline A in order to produce Headline B. Focus on the generalizable insight that can be applied in other contexts . Ignore things that are specific to this story . Do not make references this story they may not be for others .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompt format",
                "sec_num": "1.1.1"
            },
            {
                "text": "Come up with an insight the captures the sort of change observed moving from A to B.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "{ examples }",
                "sec_num": null
            },
            {
                "text": "Produce this insight as a single sentence that begins and ends in this exact format :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "{ examples }",
                "sec_num": null
            },
            {
                "text": "{ hypothesis structure } Please make sure that the hypothesis is :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "{ examples }",
                "sec_num": null
            },
            {
                "text": "i. clear (i.e., precise , not too wordy , and easy to understand ); ii . generalizable to novel situations (i.e., they would make sense if applied to other headline experiments or other messaging contexts ); iii . empirically plausible (i.e., this is a dimension on which messages can vary on ); iv . unidimensional (i.e., avoid hypotheses that list multiple constructs so if there are many things changing , pick one ); v. usable (i.e., a human equipped with this insight could use it to improve another headline in a similar way )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "{ examples }",
                "sec_num": null
            },
            {
                "text": "{ known contrasts } Headlines to Assess : Headline A: { H_A } Headline B: { H_B }",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "{ examples }",
                "sec_num": null
            },
            {
                "text": "Our prompt for generating morphs takes three examples of headlines from Upworthy, a single headline, H, and a hypothesis, D. When sampling examples and headlines, we ensure that all four headlines come from di\u21b5erent trials. The prompt then includes instructions to rewrite headline H according to the given instructions D, while keeping the content of the story as similar as possible.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating morphs",
                "sec_num": null
            },
            {
                "text": "In addition to the base prompt for morphing, we introduced two variations. The first instructed GPT to produce two variations as output: one that increased the feature of interest by 75%, and another that decreased the feature of interest by 75%. The second variation specified that the morph should be as similar to the original headline in nearly every way except for the feature being changed. The aim is to rewrite the headline such that it maximizes engagements . Therefore , Headline B should either emphasize or minimize the feature mentioned according to the hypothesized direction . Specifically , when the feature is thought to increase engagement , dial that feature up in Headline B. When the feature is thought to decrease engagement , dial that feature down in Headline B. If there is no clear direction hypothesized , emphasize the feature .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating morphs",
                "sec_num": null
            },
            {
                "text": "Headline A: { H_A } Instruction : {D} Headline B:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Generating morphs",
                "sec_num": null
            },
            {
                "text": "Our prompt for labeling headlines takes a single headline, H, and a hypothesis, D, as input. It specifies that the language model should evaluate the given headline on the given hypothesis on a scale of 0 to 7.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Labeling",
                "sec_num": null
            },
            {
                "text": "Assume you are a communication scientist researching the effects of linguistic framing on reader perception . Your task here is to evaluate a given headline on a specific dimension . Use a scale from 0 to 7, where lower values means the feature is weakly present and higher values mean it is strongly present . 0 means the dimension is not present .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompt format",
                "sec_num": "1.3.1"
            },
            {
                "text": "Your response should therefore be numeric , between 0 and 7.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prompt format",
                "sec_num": "1.3.1"
            },
            {
                "text": "Rate the headline on the following dimension : {D} Rating :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Headline : {H}",
                "sec_num": null
            },
            {
                "text": "Step 1: Generating Hypotheses Assume that we have some dataset D composed of observations and outcomes, (x i , y i ). In the Upworthy dataset for example, each observation x is a pair of headlines (H a , H b ), and y is CTR a,b . The goal of this step is to come up with a set of hypotheses about the dataset and the outcome of interest. Here, by hypothesis we mean a statement that links a feature about D to a measurable impact on y. For example, one hypothesis may be: \"using ambiguous or obscure cultural references makes people less likely to engage with a message.\" The measurable feature is the level of ambiguous or obscure cultural references within the headline. The measurable impact is a decrease in engagement, which in our case is measured by CTR. Our procedure for generating hypotheses is to take a single data point x and to ask a large language model to come up with a plausible reason for why x has a large or small value of y. In addition, we specify a strict output format for the language model, in order to force the output to be a valid hypothesis. (Details of this step, along with quality checks, are given below.) The output of this step will be a large set of hypotheses, which we call H.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Step 2: Ranking Hypotheses Morphing. For this step, we assume that we have the same dataset D, along with a set of hypotheses, H. The goal of this step is to come up with a set of morphs, which are generated data points that resemble original data points from D, while varying features outlined in hypotheses from H. That is, for a given data point x and a given hypothesis h 2 H, we define the morph of x given h to be a new data point x 0 which satisfies two requirements. Firstly, x 0 should appear as similar as possible to x. Secondly, x 0 should exhibit the feature from h more strongly than x exhibits the feature. Note that this definition implies x 0 should vary features not mentioned in h as little as possible, since it should appear as similar possible to x. Our procedure for generating morphs is to take a single data point x and a single hypothesis h, and ask a large language model to write a headline that is both as similar as possible to x while making adjustments to increase the feature from h. In addition, we provide some examples of other headlines, to help the language model produce a morph that is consistent in style with D. Examples of morphs from applying this procedure to the Upworthy dataset are shown in the main text. The output of this step will be a large set of morphs, which we will call M:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "M = {(x, h, x 0 ) | x 2 D, h 2 H} .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "We also suggest that the data points used in the creation of M should be independent of the data points used to train the machine learning model, in order to avoid biased estimates in later steps.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Scoring. For this step, we assume that we have the set of morphs M generated in the previous step, along with a machine learning model m which estimates E[y | x]. For each hypothesis h 2 H, we can then score pairs of original and morphed headlines, by averaging over morphs in M that used h. We call this the predicted treatment e\u21b5ect (PTE), which can be expressed as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P T E(h) = E [m(x, x 0 ) | (x, h, x 0 ) 2 M] ,",
                        "eq_num": "(1)"
                    }
                ],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "where the expectation is a sample average calculated over actual morphs. In the Upworthy data, we use the model b m, and note that this is why we make sure that the training and morphing partitions of the data are kept independent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Step 3: Filtering Hypotheses Clustered Selection. For this step, we assume that we have the set of hypothesis H and the predicted treatment e\u21b5ect function P T E : H ! R. We also require some similarity measure d between pairs of hypotheses, where d(h, h 0 ) is small when two hypotheses h and h 0 are very similar, and is large when they are very di\u21b5erent. Our goal is to collect a subset of hypotheses from H that are both highly diverse, and have a high PTE. We define this set as follows: fix the value of some \" > 0, then define",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "H 0 = {h 2 H | d(h, h 0 ) > \" for all h 0 such that P T E(h 0 ) > P T E(h)} .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "For a given \" > 0, the set H 0 is uniquely defined, and can be constructed using a sequential selection strategy outlined below.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "WEB APPENDIX 3: QUALITY CHECKS FOR GPT TASKS Quality of hypotheses 3.1.1 Hypothesis Quality Rating Task Participants. 79 Prolific users (Age: M = 37.91, SD = 12.63; Gender Identity: 39 Female, 38 Male, 2 Self-Identified; Race and Ethnic Identity: 60.8% white, 13.9% Black, 7.6% Latin American, 10.1% Multi-Racial, 7.6% All others) completed the labeling survey conducted in June 2024. The median participant completed the survey in 17.0 minutes. Each participant consented to participating in the study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Procedure. After consenting, participants were told that they would be reading eight hypotheses and were asked to \"imagine these hypotheses being applied to messages you might see in the world, such as online newspaper headlines or political campaign text messages or email subject lines from your favorite charity\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "As part of the instructions, participants learned of the two parts to the task and then proceeded to complete them.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "The first part asked participants to rate a hypothesis based on the following traits:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Clarity (i.e., whether the hypothesis is easy to understand)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Face-Value (i.e., whether the hypothesis seems logical or if it's something that could be observed without complex analysis)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Generalizability (i.e., whether the hypothesis could extend to multiple contexts where messages are sent)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Usability (i.e., whether the hypothesis could be used by a human to change a given message)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Overall Impression (i.e., is this a good hypothesis?)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Responses ranged from \"1 (Low)\" to \"7 (High)\". Participants were also asked to select which contexts they could imagine the hypothesis being applied to. The set of contexts included:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Online newspaper headlines \u2022 Product descriptions \u2022 Emails from a doctor's o ce \u2022 Political campaign text messages \u2022 Emails from charities \u2022 Billboard advertisements \u2022 Social media posts \u2022 None of the above",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "The second part asked participants to make a prediction into how the \"insight\" might a\u21b5ect other outcomes. For example, for the hypothesis \"using humor increases engagement with a message\", the insight is \"using humor\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "The list of outcomes included:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Making a donation (of any amount, i.e., assuming the message was asking for donations, would applying this insight a\u21b5ect how many people chose to donate)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Amount donated (i.e., for those who might've made a donation anyway, would this change how much they donated or the total amount fundraised)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Registering to vote (i.e., might applying this insight to a message intended to get people registered lead more / less people to actually register)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Voting in an election (i.e., might applying this insight to a 'Get Out the Vote' message change how many people went and voted)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Opening an email or message (i.e., clicking on the message to view its contents)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Responding to an email or message (e.g., this could mean writing a reply or comment or simply clicking on the link to take some action suggested like RSVPing to invitation, making an appointment, signing up for something)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Unsubscribing from future messages (e.g., after receiving an email, the person chooses to unsubscribe)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Blocking the messenger (e.g., blocking them on social media; blocking the number texting you; blocking the emails)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Sharing the message (e.g., reposting on social media; forwarding email)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Clicking on the content (e.g., clicking on a story in a news website, clicking on a social media post)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "\u2022 Scanning a QR code (e.g., in a magazine, on a billboard, flyer, etc)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Participants were asked \"If you had to guess, what e\u21b5ect do you think it would have on the following outcomes?\" Response options included \"Large Decrease\", \"Small Decrease\", \"No Meaningful E\u21b5ect\", \"Small Increase\", \"Large Increase\", and \"Not Applicable\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Each participant saw eight hypotheses, drawn randomly from a set of 106. 2 Hypotheses were shown on separate pages.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Participants would see a hypothesis on one page, along with the rating questions and the context question. On the next page, they would see the \"insight\" alongside the outcomes questions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "2 These 106 hypotheses consisted of 100 hypotheses randomly drawn from full set of hypotheses generated using GPT. Specifically, we randomly selected 10 hypotheses per decile of simulated treatment e\u21b5ects (see main paper). The six additional hypotheses were the six selected in the paper to be tested out of sample.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "At the end of the survey, participants were asked their age, gender, education, and ethnic identity. We also asked them if they used GPT or another LLM to assist with this task and whether they were familiar with Upworthy.com.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Results. Overall, participants perceived hypotheses to be of high quality. Each hypothesis was rated by a median of 6 participants (Min: 4), which we then averaged across. On each trait, ratings were above the scale's midpoint of 4 (see Figure 1 ). Looking next at the number of contexts participants, on average, expected all hypotheses to apply to at least one other context (Prop Selecting None of the Above: 0). Most hypotheses could be applied to social media (Prop: .892), political SMS messages (.642), emails from charities (.547) and headlines (.575). Few were seen as applicable to emails from a doctor's o ce (Prop: .094). See Table 1 .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 244,
                        "end": 245,
                        "text": "1",
                        "ref_id": "FIGREF6"
                    },
                    {
                        "start": 644,
                        "end": 645,
                        "text": "1",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "For forecasted e\u21b5ects on the di\u21b5erent outcomes, the median hypothesis was expected to have a positive e\u21b5ect on nearly every measure, other than \"Unsubscribe\" and \"Block\" which both received a median rating of 0 (\"No Meaningful E\u21b5ect\"). The largest anticipated outcome was on \"Clicks\" for which the median rating was 1 (on a scale that ranged from 2 to +2. For contexts, we report the proportion of hypotheses for which more than half the raters selected that context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 2: FORMALIZING THE STEPS OF HYPOTHESES GENERATION FRAMEWORK",
                "sec_num": null
            },
            {
                "text": "Our first checks are aimed at confirming the quality of morphs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quality of morphing procedure",
                "sec_num": null
            },
            {
                "text": "We first provide two checks to provide us confidence that morphed headlines are, in fact, \"within-distribution\". This is important, since for the ML model to make valid predictions, the morphs should come from the same data generating distribution, and for the morphs to be good quality, they should be good quality. These checks are designed to confirm that morphs are generally similar to the headlines they are based on, are high-quality, and do in fact manipulate the feature of interest.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Are morphs in distribution?",
                "sec_num": "3.2.1"
            },
            {
                "text": "For our first check, we use the sentence embedding model (described in the main text) to measure similarity between pairs of headlines. For reference, headlines from di\u21b5erent trials have a median pairwise distance of 1.78 (using Euclidean distance between embedding vectors). By comparison, headlines from the same trial have a median pairwise distance of 1.04, suggesting that headlines from the same trial are more alike than those from di\u21b5erent trials. Finally, the median pairwise distance between headlines and their associated morphs is just .469, suggesting that morphs are indeed very similar to the original headline they are based on. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Are morphs in distribution?",
                "sec_num": "3.2.1"
            },
            {
                "text": "This study aimed to test whether headlines generated using large language models (GPT-4) are perceived di\u21b5erently to those written by humans. In particular, we sought to test whether GPT headlines are perceived as worse. This study was pre-registered on AsPredicted.org (#177783).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Participants. 120 Prolific users (Age: M = 40.33, SD = 14.64; Gender Identity: 59 Female, 59 Male, 2 Self-Identified; Race and Ethnic Identity: 60.8% white, 12.5% Black, 8.3% Latin American, 8.3% Multi-Racial, 10.0% All others) completed the labeling survey conducted in June 2024. The median participant completed the survey in 11.12 minutes. Each participant consented to participating in the study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Procedure. After consenting, participants were told that they would be reading twenty headlines and would be asked to rate their level of interest.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Each participant then saw twenty headlines randomly drawn from a set of 300. This set contained 150 original Upworthy headlines and 150 morphs. This study aimed to test whether human raters could accurately detect headlines generated using large language models (GPT-4). In particular, we sought to test whether GPT headlines were perceived as \"AI generated\". We also tested whether GPT headlines were perceived as relatively more AI generated (less \"human generated\") than Upworthy headlines written by humans. This study was pre-registered on AsPredicted.org (#177785).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Participants. 101 Prolific users (Age: M = 38.92, SD = 12.64; Gender Identity: 47 Female, 51 Male, 3 Self-Identified; Race and Ethnic Identity: 58.4% white, 13.9% Black, 9.9% Latin American, 6.9% Multi-Racial, 10.9% All others) completed the survey conducted in June 2024. The median participant completed the survey in 6.15 minutes. Each participant consented to participating in the study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Procedure. After consenting, participants were told that they would be reading twenty headlines and would be asked decide whether each headline was written by a human or AI.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Each participant saw 20 headlines randomly drawn from a set of 300 (150 morphs; 150 original Upworhty headlines). This set was identical to the one used in the attitudes experiment above. Each headline was presented on a separate page, along with the question \"Was this headline written by a human or AI?\". The 7-point scale ranged from \"+3: Definitely Human\" to \"+3: Definitely AI\" where the midpoint was \"0: Unsure\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Participants were incentivized to answer according to their true beliefs. Specifically, they earned and lost points depending on whether they were categorically (in)correct and how confident they were. For example, if they rated a headline as \"\"+3: Definitely Human\" and the headline was an Upworthy headline, they earned 3 points. If, in fact, that headline was GPT-generated, they lost 3 points. In the end, the points were summed up and each participant was paid $0.05 for every positive point. The maximum bonus one could earn was therefore $3.00.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "After completing the main rating task, participants answered a reading check to confirm they understood the instructions they were meant to be following and then ended the survey with a set of demographic questions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Results. Our primary analysis was a one-sided t-test comparing the average rating of morphed headlines to 0.5. As pre-registered, if the mean is significantly less than 0.5, we would reject the null that GPT-generated hypotheses were perceived as AI generated. On average, morphed headlines were rated as .11, significantly less than 0.5, one-sided t(1028) = 5.91, p < .001.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Comparing the mean to the midpoint of the scale, zero, we see a marginal di\u21b5erence, one-sided t(1028) = 1.59, p = .056.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "As a secondary analysis, we compared ratings of the morphed headlines to the ratings of Upworthy headlines. Morphed headlines (M = .11, SD = 2.14) were seen as relatively more AI generated (less human generated) then Upworthy headlines (M = .09, SD = 2.20), t(2018) = 2.01, Cohen's d = .09, 95% [.00, .18], p = .045. When we account for participantlevel and headline-level fixed e\u21b5ects, the e\u21b5ects are similar, p = .057. Nevertheless, the equivalence test was significant (p < .001) given equivalence bounds of half a unit on the scale ( .5, .5; Lakens 2017), suggesting these di\u21b5erences may not be meaningfully di\u21b5erent.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "3.2.4 Experiment 3: Can humans accurately detect which headlines were produced by Upworthy?",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "This study aimed to test whether actual Upworthy headlines are perceived as Upworthy headlines, more than headlines generated by GPT. This study was pre-registered on AsPredicted.org (#177786).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Participants. 10 Prolific users (Age: M = 40.43, SD = 13.14; Gender Identity: 47 Female, 50 Male, 2 Self-Identified, 1 NA; Race and Ethnic Identity: 62.6% white, 13.1% Black, 8.1% Latin American, 6.1% Multi-Racial, 4.0% East Asian, 6.1% All others) completed the survey conducted in June 2024. The median participant completed the survey in 7.58 minutes. Each participant consented to participating in the study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Procedure. After consenting, participants were told that they would be reading twenty headlines and would be asked decide whether each headline was produced by Upworthy.com or not. Participants were told that Upworthy.com was a well-known news platform alongside a link to the website in case they wanted to view it. They were also provided 10 examples headlines written by Upworthy between 2014 and 2016.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Each participant then saw 20 headlines randomly drawn from a set of 300 (150 morphs; 150 original Upworhty headlines). This set was identical to the one used in the attitudes experiment and morph detection experiment above. Each headline was presented on a separate page, along with the question \"Was this headline written writers at Upworthy.com?\". The 7-point scale ranged from \" 3: Definitely Not Upworthy\" to \"+3: Definitely Upworthy\" where the midpoint was \"0: Unsure\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Like the study above, participants were incentivized to answer according to their true beliefs. Specifically, they earned and lost points depending on whether they were categorically (in)correct and how confident they were. For example, if they rated a headline as \"\"+2: Very Likely Upworthy\" and the headline was not an actual Upworthy headline, they lost 2 points. In the end, the points were summed up and each participant was paid $0.05 for every positive point. The maximum bonus one could earn was $3.00.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "After completing the main rating task, participants answered a reading check to confirm they understood the instructions they were meant to be following and then ended the survey with a set of demographic questions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Results. Our primary analysis involved a two-sided t-test comparing the average rating of original headlines to morphed headlines. Headlines written by Upworthy writers (M = .50, 1.92) were thought more likely to have been written by Upworthy writers than morphed headlines (M = .21, SD = 1.95), t(1978) = 3.31, Cohen's d = .15, 95% CI [.06, .24] . This e\u21b5ect appears more pronounced when adjusting for participant-level and headline-level fixed e\u21b5ects, p = .003. However, the equivalence test was also significant (p = .007), given equivalence bounds of half a unit on the scale ( .5, .5; Lakens 2017), suggesting these ratings may not be meaningfully di\u21b5erent.",
                "cite_spans": [
                    {
                        "start": 336,
                        "end": 346,
                        "text": "[.06, .24]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "Furthermore, morphed headlines were also perceived to be written by Upworthy writers, with an average rating significantly greater than the midpoint of zero, one-sided t(994) = 3.37, p < .001.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Experiment 1: How do humans' attitudes towards morphed headlines compare to original?",
                "sec_num": "3.2.2"
            },
            {
                "text": "We now consider a separate check, to confirm that morphs are manipulating our feature of interest. Our strategy here will be to collect labels for a variety of headline-morphhypothesis triplets, and measuring the change in label values for hypotheses from which the morph was generated (for which the change should be large) and the change in label values for hypotheses unrelated to the morph (for which the change should be small).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do morphs manipulate the feature of interest?",
                "sec_num": "3.2.5"
            },
            {
                "text": "Because of the large number of labels required for this exercise, we use GPT to emulate the labeling task outlined in the main text. We first select a subset of 87 hypotheses by taking the six hand-selected hypotheses (see main text) and uniformly sampling from the remaining hypotheses. We then combine the 120 original headlines and 10,351 unique morphed headlines into a single set of headlines. For each headline, we then label it on each of the 87 hypotheses, using a GPT task. The GPT task uses a prompt instructing the model to rate a headline on a given dimension, on a scale of 0 to 7, where 0 indicates the feature is not present. For the full prompt format, see above.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do morphs manipulate the feature of interest?",
                "sec_num": "3.2.5"
            },
            {
                "text": "We find that 53% of morphs have a higher score for the feature of interest than their original headline when the feature of interest is the one on which the morph is generated. For that feature, 40% of morphs have the same value as the original headline, and only 7% decrease the label value for the morphed feature. By comparison, only 24% of morphs have a higher score for the feature of interest when that feature is not one which was being morphed, with 57% remaining unchanged, and 19% decreasing. The mean change in label value is 0.89 (on the eight-point scale) for morphed features, and 0.09 for unrelated features. Hence, we see that morphing does a reasonable job of increasing the value of the morphed feature, while holding other features constant.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Do morphs manipulate the feature of interest?",
                "sec_num": "3.2.5"
            },
            {
                "text": "Since we use GPT to label features for a variety of sub-tasks, we are interested in checking the quality of our GPT ratings (see also Rathje et al. 2023) . For this exercise, we compare the mean of human ratings and to the GPT ratings, for which we have ratings for (i.e., the six hypotheses of interest on headlines used in the regression set). We compare the strength of agreement between the human and GPT ratings.",
                "cite_spans": [
                    {
                        "start": 134,
                        "end": 153,
                        "text": "Rathje et al. 2023)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Quality of labeling exercises: GPT versus human ratings",
                "sec_num": null
            },
            {
                "text": "For our first check, we calculate Krippendor\u21b5's alpha coe cient for each headline label based on the suggestion by Humphreys and Wang (2018) . Because we compare ratings at the headline level, we have ratings for 3,400 headlines for each label. For the human rating, we use the mean human label value, without any other adjustment. For the GPT rating, we use the result of the GPT labeling task, without any other adjustment. To calculate the 'raw' Krippendor\u21b5's alpha, we use the ordinal metric when calculating alpha values. The results of this test are included in Table 3 . We find that the agreement is very poor, and in some cases very negative. However, Krippendor\u21b5's alpha is sensitive to changes in scale and location of the rating of interest. Since we are typically interested in the relative size of labels, we standardize each of the GPT and human labels, and repeat the above exercise. The results of this are shown as a 'Z-score' alpha in Table 3 , where we see that this substantially improves the alpha coe cient, although the values still range from model (0.600 for positive human behaviour) to poor (0.111 for short and simple).",
                "cite_spans": [
                    {
                        "start": 115,
                        "end": 140,
                        "text": "Humphreys and Wang (2018)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 574,
                        "end": 575,
                        "text": "3",
                        "ref_id": "TABREF5"
                    },
                    {
                        "start": 960,
                        "end": 961,
                        "text": "3",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Quality of labeling exercises: GPT versus human ratings",
                "sec_num": null
            },
            {
                "text": "As a further check of GPT label quality, we consider how well a \"marginal\" human coder's rating agrees with both GPT and the mean human label. For this check, we randomly sample a single rating for each headline and hypothesis, and reserve this as the marginal rater. We then aggregate the remaining human ratings to form a jackknifed mean human rating. We then look at the correlation between the GPT rating against the marginal human rating, versus the jackknifed mean human rating against the marginal human rating. We report the Pearson correlation coe cient values, but find similar results using the Spearman rank correlation coe cient. The results of this exercise are in Table 4 . Firstly, we see that the marginal human rating has a similar correlation to both the jackknifed mean human rating and the GPT rating for each label, suggesting that the jackknifed mean human rating and the GPT rating are both appropriate as label sources. Secondly, we see that the strength of association between the GPT label and the mean human ratings (either the jackknife mean or the full mean) is much stronger than the marginal human rating. This gives further evidence that the GPT rating is a good approximation for the mean human rating (up to a linear transformation). While GPT's rating are not perfectly correlated with the average human rating, they are no worse than a single human is to the average. The left two columns show correlations between the marginal human rating to the jackknifed mean human rating and the GPT rating, in order to compare how closely each source resembles an additional human rater. The right two columns show correlations between the GPT rating and the jackknifed mean human rating and the full human rating, in order to assess how closely the two sources agree.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 685,
                        "end": 686,
                        "text": "4",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "Quality of labeling exercises: GPT versus human ratings",
                "sec_num": null
            },
            {
                "text": "In this section, we consider the motivation for having GPT produce hypotheses on a pairby-pair basis. We will collect hypotheses from both humans and GPT, and from a strategy that has a participant (or language model) generate a hypothesis from a single pair, or based on multiple pairs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "To collect human hypotheses, we conducted two studies (data and materials available on OSF). The first gathered human hypotheses from pairs of headlines. 104 Prolific users completed a study that asked them to read a pair of headlines and fill in a hypothesis in the format \"Hypothesis:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "increases [decreases] engagement with a message.\" The specific format was randomly drawn from the same set used in the LLM prompts. Participants in the pairwise study saw two pairs of headlines each and wrote two hypotheses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "The second study gathered human hypotheses after seeing many pairs of headlines. This was the same study used to collect human guesses. 303 participants first completed the guessing task described in the body of the paper, which consisted of 40 trials. Each trial displayed a pair of headlines written for the same story and participants were incentivized to select the headline that performed better in an AB test. After completing the main set of tasks, participants were asked to fill in a hypothesis in the format \"Hypothesis: increases [decreases] engagement with a message.\" The specific format was randomly drawn from the same set used in the LLM prompts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "For the population of GPT hypotheses based on a single pair of headlines, we use the same population of 2,100 hypotheses as those collected in the main text. For the population of GPT hypotheses based on multiple pairs of headlines, we run a separate hypothesis collection exercise, that is otherwise as similar as possible to the process reported in the main text. We use a prompt that is a slightly modified one from the prompt shown above, which replaces the single pair of headlines with 20 consecutive pairs of headlines, and makes minor adjustments to the instructions accordingly. To maximise the diversity of generated hypotheses, we draw a single headline pair from each component and assign that pair to exactly one prompt. In line with the preceding hypothesis generation process, we use only the training partition of the Upworthy dataset. This ensures that each prompt draws 20 pairs of headlines from trials not used in any other prompt. Because this process limits us to only 133 unique prompts, we also sample 10 hypotheses from each prompt. We apply the same cleaning procedure as used for pairwise hypotheses. Despite an instruction to produce only one single insight, 574 of these draws produced multiple hypotheses, which we remove from the sample.3 This leaves a final sample of 756 hypotheses generated from the aggregate GPT exercise.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "We now have a dataset containing hypotheses from two sources, humans and GPT, generated using two strategies, either from single headline pairs or from multiple headline pairs. We measure semantic diversity within each of these four groups by finding the pairwise distance between embedding vectors for hypotheses. The results of this are shown in Table 5. The results show that when presenting either humans or GPT with multiple pairs of headlines, the resulting hypotheses tend to have reduced semantic diversity. The results are stronger for headlines created from GPT. We have seen compelling evidence that using average PTE to prioritize hypotheses worthy of testing helps to identify features that do indeed predict our outcome of interest. In this section, we test for whether the predicted treatment e\u21b5ect produced in the generating stage correlates to the estimated treatment e\u21b5ect in the hold-out set. For this, we first use GPT to emulate the human labeling task from the main text. Then we use these labels in a series of regressions with the same specification as the Hypothesis Testing section from the main text. Finally, we compare how the coe cient value of the hypothesized features to the respective average PTE values gathered in the ranking step.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "We select a random sample of hypotheses for testing by first dividing the range of observed average PTE values into 10 intervals of equal width (winsorizing the top and bottom 1% of values to account for sparsity at extreme ranges of the distribution). From each interval, we then sample 40 hypotheses uniformly at random. We then repeat the label collection strategy outlined above in Section 3.2.5, except that here the headlines we collect labels for are from the regression partition of our dataset. We follow a similar process to the human labeling task used for Hypothesis Testing in the main text, only this time we needed many more ratings so we used GPT (see also Section 3.3 above). Unlike the human rating task, the scale in the prompt for GPT did not include \"0\" as an option. The result is a dataset containing 1, 360, 800 labels, exhausting all combinations of the 400 sampled hypotheses and 3, 402 headlines (from the regression set). For each label, we then run a regression using the same specification as used in hypothesis testing in the main text, predicting CTR without any other covariates, and extract both the coe cient estimate b r and the p-value for b r .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "Figure Figure 3 displays a summary of the results where we average the b r coe cient values from the regressions per stratum. This suggests that hypotheses with higher average PTEs are identifying features that produce larger e\u21b5ects. Figure 4 aggregates the p-values for these coe cients per stratum. Here we see that hypotheses with higher average PTEs produce more significant results out-of-sample.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 14,
                        "end": 15,
                        "text": "3",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 241,
                        "end": 242,
                        "text": "4",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "It is important to keep in mind in interpreting these results is that although these are a diverse set of hypotheses, there is still a lot of overlap, both on the surface, in the description of the features, but also likely in the underlying psychology. While we adjust for the False Discovery Rate in this analysis, it is possible that two hypotheses, randomly chosen for this exercise, are picking up a similar feature. Another thing to note in interpreting these values is that there may be an asymmetry in the quality of hypotheses across the various stratum. Higher average PTEs indicate that the hypotheses, when applied to several random headlines, resulted in morphed headlines that were predicted to perform better than their original. This is a function of both the ML algorithm, the hypothesis quality, and the morph quality. It is di cult to imagine these being high by chance, but the reverse is possible. That is, lower average PTEs could be due to many low-quality morphs or morphs that happened to outof-distribution or it could be due to the hypothesis, either being too specific or too complex or non-sensical (therefore producing odd morphs). The latter would also be harder to rate, resulting in inconsistent e\u21b5ects when estimating the regression. We see some suggestive evidence of this by the fact that the proportion of hypotheses in the lowest decile (most negative average PTEs) is not as high as those in the top decile -we suspect it is because features likely to produce negative e\u21b5ects are mixed in with hypotheses of lower quality.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "Nevertheless, the results extend the findings in the main text. When testing with human raters, we found evidence for four out of six hypotheses, a higher proportion than we might expect at \u21b5 = .05. Note that these were selected to be independent hypotheses and therefore needed no further adjustments. Here we find a similar rate of significant e\u21b5ects (out of sample) for hypotheses with the highest average PTEs (calculated in the ranking step), further validating our framework for discovery. We partnered with an organization that produces articles on popular culture, lifestyle and sport. This organization shared a dataset of 553,328 di\u21b5erent posts for various articles on a large social media platform between July 2022 and February 2023. The dataset contains the message of the post, along with the URL of the page being linked on the organization's website, a categorization of the page being linked into one of 66 categories. A total of 1442 rows are dropped, 1245 for missing the message information and 204 for having zero total From these measures we calculate the ratio of clicks to total reach (the click-through-rate), along with the ratio of comments, likes, and shares to total reach (the comment rate, like rate, and share rate, respectively). There are some additional outcomes, such as the number of di\u21b5erent reaction types, but these are generally very small and excluded from our analysis (the most frequent reaction occurs at a rate less than 0.2%). Because of its similarity to the outcome used throughout the Upworthy dataset, we consider the click-through-rate to be our primary outcome of interest. This dataset is not organized into any kind of valid experimental unit. There are generally a much higher number of posts linking to the same URL, and many posts that share a common outcome. However, since these are not conducted as a valid A/B trial, we neither combine posts with common messages, nor do we create a pairwise dataset comparing two di\u21b5erent posts. Instead, for this dataset we conduct analysis at the post level, and treat click-through-rate as our main outcome of interest. We also repeat the dataset partitioning strategy applied to the Upworthy dataset: we form 'components' that group trials which share any common message. The resulting post-level dataset contains the following splits:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "\u2022 A training dataset with 8,708 unique messages across 133,470 posts, for 5,371 di\u21b5erent URLs",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "\u2022 A regression set with 8,970 unique messages across 133,739 posts from 5,475 unique URLs",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "\u2022 A morphing set with 3,492 unique messages across 56,851 posts from 2,123 unique URLs",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "\u2022 A lock-box set with 14,523 unique messages across 227,826 posts from 8,953 unique URLs Some summary statistics for these splits are included in Table 6 . Note: Here we do not combine posts with common messages, since posts are not valid A/B trials.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 152,
                        "end": 153,
                        "text": "6",
                        "ref_id": "TABREF8"
                    }
                ],
                "eq_spans": [],
                "section": "WEB APPENDIX 4: DIVERSITY OF HYPOTHESES",
                "sec_num": null
            },
            {
                "text": "We follow a similar procedure as described in the main text. Again, we pre-registered our procedure on AsPredicted.org (#181144). We used the same six hypotheses uncovered and tested above. We kept the direction consistent for simplicity, but note here that behavioral interventions often have di\u21b5erent e\u21b5ects across di\u21b5erent people and di\u21b5erent contexts (Goswami and Urminsky 2022). Our primary outcome was the click-through-rate (CTR). However, we were also interested in examining whether the hypothesized features might a\u21b5ect other outcomes too; in particular, the like rate, share rate, and comment rate. Together, the aim was to understand whether hypotheses generated in one dataset could predict various outcomes in another time and place.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": "6.1.2"
            },
            {
                "text": "We planned to recruit 900 participants. Each participant saw 30 subject lines, each on a separate page, randomly drawn from a set of 5,077. For each subject line, participants were asked to \"select the level which each trait is featured in this subject line, from '1 (Low)' to '7 (High)'.\" There was also an option to select \"0\" to indicate the trait was not present. The traits were listed by their shorthand: (i) includes element of surprise followed by cli\u21b5hanger, (ii) incorporates parody, (iii) refers to multimedia evidence, (iv) describes physical reaction, (v) short and simple phrases, (vi) focus on positive aspects of human behavior.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": "6.1.2"
            },
            {
                "text": "In the end we recruited 900 participants (M age = 37.74, SD = 13.04; 448 Male, 435 Female, 17 Self-Identified; 60.6% white, 14.4% Black, 11.7% Latin American, 5.0% Multiracial, 3.7% East Asian, 4.7% all others) through Prolific. Altogether, participants provided 162,000 labels.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.1.3"
            },
            {
                "text": "To test each of the six hypotheses, we estimated OLS regressions following a similar specification to the one used for testing hypotheses in the Upworthy data. Notably, we regressed CTR on Rating rather than taking the di\u21b5erence of each since these posts were not part of a randomized experiment. We also dropped an additional two rows from the data, for having a total reach of zero viewers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.1.3"
            },
            {
                "text": "The estimated coe cients for each of our main regressions (outcome: CTR) are displayed in Table 7 . We find that four of the hypothesized features have significant association with CTR (ps < 0.01). We find that physical reactions has a positive association, which accords with its originally hypothesized association, while both short and simple phrases and positive human behavior have a significant negative relationship, in accordance with their original setting. On the other hand, multimedia has a strong negative association. To make coe cients interpretable, we have scaled the outcome variable, CTR, by dividing by the standard deviation of CTR (.0202), and scaled each of the hypothesized features to have unit variance. Hence, a one standard deviation increase in any of the hypothesized features produces a change in CTR equal to b times the standard deviation in CTR.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 96,
                        "end": 97,
                        "text": "7",
                        "ref_id": "TABREF9"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.1.3"
            },
            {
                "text": "We also analyzed the relationship on the remaining outcomes. Figure 5 shows the coecient estimates are significantly di\u21b5erent from zero (p < .05) for several features and outcome combinations. In addition to the four features that show strong associations to the CTR, five show strong associations to the like rate, five to the share rate, and one to the comment rate, p < .05. Worth noting is the fact that the e\u21b5ect of many of the features go in opposite directions depending on the outcome. As it happens, in this dataset, the rate of comments, shares and likes are negatively correlated with the the CTR. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 68,
                        "end": 69,
                        "text": "5",
                        "ref_id": "FIGREF10"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.1.3"
            },
            {
                "text": "As a further check of the generalizability of our hypotheses, we partnered with a progressive outreach organization to test whether the hypotheses could generalize further to this context.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Progressive Outreach Data",
                "sec_num": null
            },
            {
                "text": "We partnered with an organization that seeks to advance the interests of domestic workers across America. This organization shared a dataset of 1, 653 email campaigns sent to its members and supporters between September 2020 and May 2024. The dataset contains the subject line of the email (which ranges from a single word to roughly 130 characters, but is between 16 and 86 characters for 95% of subject lines), the date sent, and several outcomes of interest: the number of recipients, the proportion of recipients who opened the email (open rate), the proportion of recipients who clicked on a link in the email (click-through-rate), the proportion of recipients who \"took an action\" (conversion rate; such as completing a survey, signing a pledge, or making a donation), the proportion who made a donation through the link (contribution rate), and the amount donated (average contribution amount), the proportion of recipients who unsubscribed after receiving the email (unsubscribe rate), and the number of emails that were 'bounced' (rejected by the email server) (bounce rate).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "6.2.1"
            },
            {
                "text": "This dataset is also organized into trials, although notably, most trials contain only a single headline: of the 1, 211 trials, 1, 064 of them contain only one subject line, leaving 147 trials with more than one subject line and 364 unique subject lines between them. We apply the same data partitioning strategy as for Upworthy, to ensure that no subject lines are repeated across splits, and no trials are distributed across di\u21b5erent splits. However, we use use di\u21b5erent split sizes, in order to produce a small partition for exploratory analysis, and a larger partition for running regressions. The resulting pairwise dataset contains the following splits:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "6.2.1"
            },
            {
                "text": "\u2022 A exploratory dataset with 138 pairs from 30 trials",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "6.2.1"
            },
            {
                "text": "\u2022 A regression set with 506 unique headline pairs from 117 unique trials Some summary statistics for these splits are included in Table 8 . Note: Here we do not combine posts with common messages, since posts are not valid A/B trials.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 136,
                        "end": 137,
                        "text": "8",
                        "ref_id": "TABREF10"
                    }
                ],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "6.2.1"
            },
            {
                "text": "Because this data has also been organized into pairs, we once again consider the di\u21b5erence in rates (for each of the outcomes outlined above) as our key dependent measure. Moreover, while we planned to analyze e\u21b5ects across all outcomes, we chose the CTR as our primary outcome since it most resembled the outcome in the Upworthy data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data",
                "sec_num": "6.2.1"
            },
            {
                "text": "We follow a similar procedure as above (also reported in main text). Again, we preregistered our procedure on AsPredicted.org (#178928). We used the same six hypotheses uncovered and tested above. We kept the direction consistent for simplicity, but note here that behavioral interventions often have di\u21b5erent e\u21b5ects across di\u21b5erent people and di\u21b5erent contexts (Goswami and Urminsky 2022). Our primary outcome was the click-through-rate (CTR). However, we were also interested in examining whether the hypothesized features might a\u21b5ect other outcomes too; in particular, the conversion rate, contribution rate; average contribution amount, and unsubscribe rate.4 Together, the aim was to understand whether hypotheses generated in one dataset could predict various outcomes in another time and place.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": "6.2.2"
            },
            {
                "text": "We planned to recruit 100 participants. Each participant saw 30 subject lines, each on a separate page, randomly drawn from a set of 300. For each subject line, participants were asked to \"select the level which each trait is featured in this subject line, from '1 (Low)' to '7 (High)'.\" There was also an option to select \"0\" to indicate the trait was not present. The traits were listed by their shorthand: (i) includes element of surprise followed by cli\u21b5hanger, (ii) incorporates parody, (iii) refers to multimedia evidence, (iv) describes physical reaction, (v) short and simple phrases, (vi) focus on positive aspects of human behavior.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Procedure",
                "sec_num": "6.2.2"
            },
            {
                "text": "In the end we recruited 101 participants (M age = 37.5, SD = 11.2; 48 Male, 50 Female, 3 Self-Identified; 66.3% white, 11.9% Black, 6.9% Latin American, 5.9% Multi-racial, 8.9% all others) through Prolific. Altogether, participants provided 18,180 labels.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.2.3"
            },
            {
                "text": "To test each of the six hypotheses, we estimated OLS regressions following the specification in the main text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.2.3"
            },
            {
                "text": "The estimated coe cients for each of our main regressions (outcome: CTR) are displayed in Table 9 . In a regression with two-way clustered standard errors (clustering on the subject line ID within each pair), physical reactions has a significant and positive association with CTR (p < .05) and short and simple has a significant and negative association with CTR (p < 0.05). Standard errors shown in parentheses are clustered on the ID of both the left and right ID of the pair. To make coe cients interpretable, we have scaled the outcome variable, CTR, by dividing by the standard deviation of CTR for experiments with at least two trials (.0101), and scaled each of the hypothesized features to have unit variance. Hence, a one standard deviation increase in any of the hypothesized features produces a change in CTR equal to b times the standard deviation in CTR for multi-arm trials.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 96,
                        "end": 97,
                        "text": "9",
                        "ref_id": "TABREF11"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.2.3"
            },
            {
                "text": "We also analyzed the relationship on the remaining outcomes. Figure 6 shows the coecient estimates are significantly di\u21b5erent from zero (p < .05) for several features and outcome combinations. In addition to the two features that show strong associations on the CTR, one appears to predict the total open rate, two the conversion rate, two the contribution rate, and three the unsubscribe rate. Shown is the coe cient estimates with 95% confidence intervals for each outcome and feature combination.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 68,
                        "end": 69,
                        "text": "6",
                        "ref_id": "FIGREF11"
                    }
                ],
                "eq_spans": [],
                "section": "Results",
                "sec_num": "6.2.3"
            },
            {
                "text": "On 11 July 2024, Matias et al. (2021) published a correction to the data where they acknowledge \"problems with the randomization of the tests between June 25, 2013 and January 10, 2014. A total of 7,004 A/B tests or 22% of experiments may have been a\u21b5ected\" (see also Eckles 2024) . They go on to encourage \"researchers to treat these tests as not randomized... researchers conducting causal analysis [are encouraged] to omit all experiments from June 25, 2013 through the end of January 10, 2014.\"",
                "cite_spans": [
                    {
                        "start": 17,
                        "end": 37,
                        "text": "Matias et al. (2021)",
                        "ref_id": null
                    },
                    {
                        "start": 268,
                        "end": 280,
                        "text": "Eckles 2024)",
                        "ref_id": "BIBREF136"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 7: EXCLUDING NON-RANDOM TRIALS",
                "sec_num": null
            },
            {
                "text": "Of primary concern for us is in testing the hypotheses generated. Table 10 estimates the primary regressions from the main text, omitting trials from June 25, 2013 to January 10, 2014.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 72,
                        "end": 74,
                        "text": "10",
                        "ref_id": "TABREF12"
                    }
                ],
                "eq_spans": [],
                "section": "WEB APPENDIX 7: EXCLUDING NON-RANDOM TRIALS",
                "sec_num": null
            },
            {
                "text": "The results are largely consistent with the results reported in the paper. While the evidence against the null is weaker for some features (e.g., physical reactions), it appears stronger for others (e.g., parody; positive human behavior).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 7: EXCLUDING NON-RANDOM TRIALS",
                "sec_num": null
            },
            {
                "text": "As an additional check, we also train a new ML model to predict CTR, excluding rows of the data covered by the correction but holding all other decisions constant. We then compare the predictions of the refit model to those of the original model on the regression dataset used throughout the paper, also excluding those rows impacted by the non-randomness problem; this results in a dataset of 1337 valid headline pairs out of the 1693 pairs from the previous dataset. Firstly, we find that the two models produce similar predictions: they have a correlation of 91.7% on this dataset. Secondly, the performance is comparable: the original model has Adjusted R 2 = .122 on this dataset (slightly worse than the value reported in Table ?? for the larger dataset), and the refit model has Adjusted R 2 = .126 on the dataset excluding rows with randomization problems. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "WEB APPENDIX 7: EXCLUDING NON-RANDOM TRIALS",
                "sec_num": null
            },
            {
                "text": "Testing these hypotheses involved humans reading through headlines and labeling them based on the hypothesized features. We hand-picked six to facilitate this process; even then, we gathered over 140,000 human labels. We could have picked a di\u21b5erent set and in the Appendix we report the results for similar out-of-sample tests on 400 hypotheses randomly selected (only we use GPT-labels rather than human labels).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The specific prompts we used are available for download on https://bit.ly/headlines-osf",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "While engagement is often a necessary pre-condition for influencing behavior, we recognize it is often not su cient (e.g.,John et al. 2017). Engagement alone cannot, for example, overcome structural barriers(Linos et al. 2022;Thaler and Sunstein 2009)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "On July 11, 2024, the authors published a correction to the data, noting problems with the randomization of trials between June 25, 2013 and January 10, 2014. They advise that these trials be omitted when conducting causal analysis. We report results without exclusions in the paper and replicate the main set of tests in the Appendix. See Appendix.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "It appears that sometimes headlines were reused; for example, imagine Trial 1 tested Headline A against Headline B, Trial 2 tested B against C, and Trial 3 tested C against D. In this case, even though Headline A and D never appeared in the same trial, we assume something about them are the same since they are \"linked\" by Trial 2. To minimize leakage(Kapoor and Narayanan 2022), Trials 1-3 would all be assigned the same component.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We used a version of this model that was additionally fine-tuned as part of a HuggingFace event, see https: //huggingface.co/sentence-transformers/all-mpnet-base-v2",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Participants were shown headlines from the training set for the training rounds and from the validation-regression set for the testing rounds to avoid leakage here too.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We decided to use the same set of original headlines for all hypotheses to facilitate a more direct comparison of simulated treatment e\u21b5ects across hypotheses.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "This is analogous to \"convergent\" or \"fanning in\" approaches commonly discussed in the creativity literature, where the aim is to reduce the set of ideas(Banathy 1996;Cropley 2006;Malaie, Spivey, and Marghetis 2024;Toubia and Flor\u00e8s 2007).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Note that these were rated using a separate evaluation design, where options are presented individually (rather than paired) and evaluated separately(Hsee et al. 1999). We felt this was closer to what one might experience when seeing a headline online.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "See OSF for the complete set of hypotheses. See Appendix Section 5 for results of out-of-sample tests using GPT ratings.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "Since we planned to exclude these from the rest of the pipeline, prompts that had Control instructions were undersampled before being matched to a pair.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "repeating the following analysis on these excluded hypotheses, we find the same conclusion: that GPT-generated hypotheses from groups of headline pairs have less diversity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "We also planned to look at the open rate but this measure is particularly noisy since Apple introduced its Mail Privacy Protection policies (e.g.,Kaczanowski 2021;Mask 2021).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Each headline was displayed on a single page along with the set of five questions:\u2022 Interest: How interested would you be in reading this article? Response options ranged from \"0: Not interested\" to \"6: Extremely interested\".\u2022 Likelihood to Click: Imagine you came across this headline online, how likely would you be to click on it? Response options ranged from \"0: Not likely at all\" to \"6: Extremely likely\"\u2022 Own Impression: Based only on this headline, what is your overall impression of the article? Response options ranged from \" 3: Extremely unfavorable\" to \"3: Extremely favorable\"\u2022 Other Impression: Based only on this headline, what do you think the overall impression is of other Prolific users? Response options ranged from \" 3: Extremely unfavorable\" to \"3: Extremely favorable\"\u2022 Quality: What is the quality of this headline? Response options ranged from \" 3: Extremely bad\" to \"3: Extremely good\"After completing the main rating task, participants answered a reading check to confirm they understood the instructions they were meant to be following and then ended the survey with a set of demographic questions.Results. Our primary analyses included a series of two-sided t-tests comparing the average rating of morphed headlines to original headlines.We find no detectable di\u21b5erence between attitudes towards morphs versus actual Upworthy headlines (see Table 2 ). The results are qualitatively similar when we account for participant fixed e\u21b5ects and headline-level fixed e\u21b5ects. And when we control for outliers. Furthermore, the equivalence test was significant (p < .001) for all measures, given equivalence bounds of half a unit on the scale ( .5, .5; see also Lakens 2017 Survey follows an identical format to #2 but replaces \"headlines\" with \"subject lines\" throughout. Data is available on OSF; materials are not yet available since they contain proprietary information.",
                "cite_spans": [
                    {
                        "start": 1682,
                        "end": 1693,
                        "text": "Lakens 2017",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1376,
                        "end": 1377,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "annex",
                "sec_num": null
            },
            {
                "text": "Task (Pairwise)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Hypothesis Generation",
                "sec_num": null
            },
            {
                "text": "Participants provide a hypothesis in the format \"Hypothesis: increases [decreases] engagement with a message.\" where they write in a response to fill in the blank after seeing a single pair of headlines. Each participant sees two pairs and writes two hypotheses.Contains 204 hypotheses written by humans.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "5",
                "sec_num": null
            },
            {
                "text": "-Participants were randomly assigned to an \"increase\" (vs. \"decrease\") set in which Headline B always performed better (worse) than Headline A. Hypotheses formats reflected this di\u21b5erence. Formats were randomly drawn from same set used in LLM prompts. Half of participants were also randomly assigned to see four example hypotheses (vs. no examples). Data and materials available on OSF. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "104",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "People systematically overlook subtractive changes",
                "authors": [
                    {
                        "first": "Gabrielle",
                        "middle": [
                            "S"
                        ],
                        "last": "Adams",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Benjamin",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [
                            "H"
                        ],
                        "last": "Converse",
                        "suffix": ""
                    },
                    {
                        "first": "Leidy",
                        "middle": [
                            "E"
                        ],
                        "last": "Hales",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Klotz",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Nature",
                "volume": "592",
                "issue": "7853",
                "pages": "258--261",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adams, Gabrielle S., Benjamin A. Converse, Andrew H. Hales, and Leidy E. Klotz (2021), \"Peo- ple systematically overlook subtractive changes,\" Nature, 592 (7853), 258-261 https: //www.nature.com/articles/s41586-021-03380-y.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Datadriven approaches in the investigation of social perception",
                "authors": [
                    {
                        "first": "Ralph",
                        "middle": [],
                        "last": "Adolphs",
                        "suffix": ""
                    },
                    {
                        "first": "Lauri",
                        "middle": [],
                        "last": "Nummenmaa",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Todorov",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "V"
                        ],
                        "last": "Haxby",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences",
                "volume": "371",
                "issue": "1693",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1098/rstb.2015.0367"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Adolphs, Ralph, Lauri Nummenmaa, Alexander Todorov, and James V. Haxby (2016), \"Data- driven approaches in the investigation of social perception,\" Philosophical Transac- tions of the Royal Society B: Biological Sciences, 371 (1693), 20150367 https:// royalsocietypublishing.org/doi/full/10.1098/rstb.2015.0367.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Semantic determinants of memorability",
                "authors": [
                    {
                        "first": "Ada",
                        "middle": [],
                        "last": "Aka",
                        "suffix": ""
                    },
                    {
                        "first": "Sudeep",
                        "middle": [],
                        "last": "Bhatia",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Mccoy",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Cognition",
                "volume": "239",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aka, Ada, Sudeep Bhatia, and John McCoy (2023), \"Semantic determinants of memorabil- ity,\" Cognition, 239, 105497 https://www.sciencedirect.com/science/article/pii/ S0010027723001312.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "In Defense of Bumbling",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [
                            "W"
                        ],
                        "last": "Alba",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Journal of Consumer Research",
                "volume": "38",
                "issue": "6",
                "pages": "981--987",
                "other_ids": {
                    "DOI": [
                        "10.1086/661230"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Alba, Joseph W. (2012), \"In Defense of Bumbling,\" Journal of Consumer Research, 38 (6), 981-987 https://doi.org/10.1086/661230.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Value Aligned Large Language Models",
                "authors": [
                    {
                        "first": "Panagiotis",
                        "middle": [],
                        "last": "Angelopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjog",
                        "middle": [],
                        "last": "Misra",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Angelopoulos, Panagiotis, Kevin Lee, and Sanjog Misra \"Value Aligned Large Language Models,\" (2024) https://papers.ssrn.com/abstract=4781850.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "A/B Testing with Fat Tails",
                "authors": [
                    {
                        "first": "Eduardo",
                        "middle": [
                            "M"
                        ],
                        "last": "Azevedo",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Jos\u00e9",
                        "middle": [],
                        "last": "Luis Montiel",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [],
                        "last": "Olea",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "Glen"
                        ],
                        "last": "Rao",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Weyl",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Political Economy",
                "volume": "128",
                "issue": "12",
                "pages": "4614--4614",
                "other_ids": {
                    "DOI": [
                        "10.1086/710607"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Azevedo, Eduardo M., Alex Deng, Jos\u00e9 Luis Montiel Olea, Justin Rao, and E. Glen Weyl (2020), \"A/B Testing with Fat Tails,\" Journal of Political Economy, 128 (12), 4614-000 https: //doi.org/10.1086/710607.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Designing Social Systems in a Changing World Contemporary Systems Thinking",
                "authors": [
                    {
                        "first": "Bela",
                        "middle": [
                            "H"
                        ],
                        "last": "Banathy",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-1-4757-9981-1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Banathy, Bela H. (1996), Designing Social Systems in a Changing World Contemporary Systems Thinking, Boston, MA: Springer US, http://link.springer.com/10.1007/978-1- 4757-9981-1.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "The Language That Drives Engagement: A Systematic Large-scale Analysis of Headline Experiments",
                "authors": [
                    {
                        "first": "Akshina",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Oleg",
                        "middle": [],
                        "last": "Urminsky",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.2139/ssrn.3770366"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Banerjee, Akshina and Oleg Urminsky \"The Language That Drives Engagement: A System- atic Large-scale Analysis of Headline Experiments.,\" (2023) https://dx.doi.org/10.2139/ ssrn.3770366.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Machine-Assisted Social Psychology Hypothesis Generation",
                "authors": [
                    {
                        "first": "Sachin",
                        "middle": [],
                        "last": "Banker",
                        "suffix": ""
                    },
                    {
                        "first": "Promothesh",
                        "middle": [],
                        "last": "Chatterjee",
                        "suffix": ""
                    },
                    {
                        "first": "Himanshu",
                        "middle": [],
                        "last": "Mishra",
                        "suffix": ""
                    },
                    {
                        "first": "Arul",
                        "middle": [],
                        "last": "Mishra",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.31234/osf.io/kv6f7"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Banker, Sachin, Promothesh Chatterjee, Himanshu Mishra, and Arul Mishra \"Machine-Assisted So- cial Psychology Hypothesis Generation,\" (2023) https://doi.org/10.31234/osf.io/kv6f7.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Misarticulation: Why We Sometimes Feel Our Words Don't Match Our Thoughts",
                "authors": [
                    {
                        "first": "Rafael",
                        "middle": [
                            "M"
                        ],
                        "last": "Batista",
                        "suffix": ""
                    },
                    {
                        "first": "Juliana",
                        "middle": [],
                        "last": "Schroeder",
                        "suffix": ""
                    },
                    {
                        "first": "Aastha",
                        "middle": [],
                        "last": "Mittal",
                        "suffix": ""
                    },
                    {
                        "first": "Sendhil",
                        "middle": [],
                        "last": "Mullainathan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.2139/ssrn.4687986"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Batista, Rafael M., Juliana Schroeder, Aastha Mittal, and Sendhil Mullainathan \"Misarticula- tion: Why We Sometimes Feel Our Words Don't Match Our Thoughts,\" (2024) https: //dx.doi.org/10.2139/ssrn.4687986.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing",
                "authors": [
                    {
                        "first": "Yoav",
                        "middle": [],
                        "last": "Benjamini",
                        "suffix": ""
                    },
                    {
                        "first": "Yosef",
                        "middle": [],
                        "last": "Hochberg",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Journal of the Royal Statistical Society: Series B (Methodological)",
                "volume": "57",
                "issue": "1",
                "pages": "289--300",
                "other_ids": {
                    "DOI": [
                        "10.1111/j.2517-6161.1995.tb02031.x"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Benjamini, Yoav and Yosef Hochberg (1995), \"Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing,\" Journal of the Royal Statistical Society: Series B (Methodological), 57 (1), 289-300 https://doi.org/10.1111/j.2517-6161.1995.tb02031.x.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Uniting the Tribes: Using Text for Marketing Insight",
                "authors": [
                    {
                        "first": "Jonah",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Ashlee",
                        "middle": [],
                        "last": "Humphreys",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Ludwig",
                        "suffix": ""
                    },
                    {
                        "first": "Wendy",
                        "middle": [
                            "W"
                        ],
                        "last": "Moe",
                        "suffix": ""
                    },
                    {
                        "first": "Oded",
                        "middle": [],
                        "last": "Netzer",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Schweidel",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Marketing",
                "volume": "84",
                "issue": "1",
                "pages": "1--25",
                "other_ids": {
                    "DOI": [
                        "10.1177/0022242919873106"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Berger, Jonah, Ashlee Humphreys, Stephan Ludwig, Wendy W. Moe, Oded Netzer, and David A. Schweidel (2020), \"Uniting the Tribes: Using Text for Marketing Insight,\" Journal of Mar- keting, 84 (1), 1-25 https://doi.org/10.1177/0022242919873106.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "What Makes Content Engaging? How Emotional Dynamics Shape Success",
                "authors": [
                    {
                        "first": "Jonah",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Yoon",
                        "middle": [],
                        "last": "Duk Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "Meyer",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Journal of Consumer Research",
                "volume": "48",
                "issue": "2",
                "pages": "235--250",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucab010"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Berger, Jonah, Yoon Duk Kim, and Robert Meyer (2021), \"What Makes Content Engaging? How Emotional Dynamics Shape Success,\" Journal of Consumer Research, 48 (2), 235-250 https://doi.org/10.1093/jcr/ucab010.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "What Holds Attention? Linguistic Drivers of Engagement",
                "authors": [
                    {
                        "first": "Jonah",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Wendy",
                        "middle": [
                            "W"
                        ],
                        "last": "Moe",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Schweidel",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Journal of Marketing",
                "volume": "87",
                "issue": "5",
                "pages": "793--809",
                "other_ids": {
                    "DOI": [
                        "10.1177/00222429231152880"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Berger, Jonah, Wendy W. Moe, and David A. Schweidel (2023), \"What Holds Attention? Linguistic Drivers of Engagement,\" Journal of Marketing, 87 (5), 793-809 https://doi.org/10.1177/ 00222429231152880.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Wisdom from words: The psychology of consumer language",
                "authors": [
                    {
                        "first": "Jonah",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Grant",
                        "middle": [],
                        "last": "Packard",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Consumer Psychology Review",
                "volume": "6",
                "issue": "1",
                "pages": "3--16",
                "other_ids": {
                    "DOI": [
                        "10.1002/arcp.1085"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Berger, Jonah and Grant Packard (2023), \"Wisdom from words: The psychology of consumer language,\" Consumer Psychology Review, 6 (1), 3-16 https://onlinelibrary.wiley.com/ doi/abs/10.1002/arcp.1085.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Confirmatory Search and Asymmetric Dominance",
                "authors": [
                    {
                        "first": "Sudeep",
                        "middle": [],
                        "last": "Bhatia",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Journal of Behavioral Decision Making",
                "volume": "27",
                "issue": "5",
                "pages": "468--476",
                "other_ids": {
                    "DOI": [
                        "10.1002/bdm.1824"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bhatia, Sudeep (2014), \"Confirmatory Search and Asymmetric Dominance,\" Journal of Behavioral Decision Making, 27 (5), 468-476 https://doi.org/10.1002/bdm.1824.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Promoting Data Richness in Consumer Research: How to Develop and Evaluate Articles with Multiple Data Sources",
                "authors": [
                    {
                        "first": "Simon",
                        "middle": [
                            "J"
                        ],
                        "last": "Blanchard",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Goldenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Koen",
                        "middle": [],
                        "last": "Pauwels",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "A"
                        ],
                        "last": "Schweidel",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Journal of Consumer Research",
                "volume": "49",
                "issue": "2",
                "pages": "359--372",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucac018"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Blanchard, Simon J, Jacob Goldenberg, Koen Pauwels, and David A Schweidel (2022), \"Promoting Data Richness in Consumer Research: How to Develop and Evaluate Articles with Multiple Data Sources,\" Journal of Consumer Research, 49 (2), 359-372 https://doi.org/10.1093/ jcr/ucac018.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Using GPT for Market Research",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Brand",
                        "suffix": ""
                    },
                    {
                        "first": "Ayelet",
                        "middle": [],
                        "last": "Israeli",
                        "suffix": ""
                    },
                    {
                        "first": "Donald",
                        "middle": [],
                        "last": "Ngwe",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.2139/ssrn.4395751"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Brand, James, Ayelet Israeli, and Donald Ngwe \"Using GPT for Market Research,\" (2023) https: //dx.doi.org/10.2139/ssrn.4395751.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Customer Engagement: Conceptual Domain, Fundamental Propositions, and Implications for Research",
                "authors": [
                    {
                        "first": "Roderick",
                        "middle": [
                            "J"
                        ],
                        "last": "Brodie",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Linda",
                        "suffix": ""
                    },
                    {
                        "first": "Biljana",
                        "middle": [],
                        "last": "Hollebeek",
                        "suffix": ""
                    },
                    {
                        "first": "Ana",
                        "middle": [],
                        "last": "Juri\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ili\u0107",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Service Research",
                "volume": "14",
                "issue": "3",
                "pages": "252--271",
                "other_ids": {
                    "DOI": [
                        "10.1177/1094670511411703"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Brodie, Roderick J., Linda D. Hollebeek, Biljana Juri\u0107, and Ana Ili\u0107 (2011), \"Customer En- gagement: Conceptual Domain, Fundamental Propositions, and Implications for Research,\" Journal of Service Research, 14 (3), 252-271 https://doi.org/10.1177/1094670511411703.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Signature Verification using a \"Siamese\" Time Delay Neural Network",
                "authors": [
                    {
                        "first": "Jane",
                        "middle": [],
                        "last": "Bromley",
                        "suffix": ""
                    },
                    {
                        "first": "Isabelle",
                        "middle": [],
                        "last": "Guyon",
                        "suffix": ""
                    },
                    {
                        "first": "Yann",
                        "middle": [],
                        "last": "Lecun",
                        "suffix": ""
                    },
                    {
                        "first": "Eduard",
                        "middle": [],
                        "last": "S\u00e4ckinger",
                        "suffix": ""
                    },
                    {
                        "first": "Roopak",
                        "middle": [],
                        "last": "Shah",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "6",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bromley, Jane, Isabelle Guyon, Yann LeCun, Eduard S\u00e4ckinger, and Roopak Shah \"Signature Verification using a \"Siamese\" Time Delay Neural Network,\" \"Advances in Neural Information Processing Systems,\" Vol. 6., Morgan-Kaufmann (1993) https://proceedings.neurips.cc/ paper files/paper/1993/hash/288cc0ff022877bd3df94bc9360b9c5d-Abstract.html.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "What works in e-commerce -a meta-analysis of 6700 online experiments",
                "authors": [
                    {
                        "first": "Will",
                        "middle": [],
                        "last": "Browne",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [
                            "Swarbrick"
                        ],
                        "last": "Jones",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Browne, Will and Mike Swarbrick Jones \"What works in e-commerce -a meta-analysis of 6700 online experiments,\" Technical report, Qubit Digital Ltd (2017).",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "A Dynamic Model for Digital Advertising: The E\u21b5ects of Creative Format, Message Content, and Targeting on Engagement",
                "authors": [
                    {
                        "first": "Norris",
                        "middle": [
                            "I"
                        ],
                        "last": "Bruce",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "P S"
                        ],
                        "last": "Murthi",
                        "suffix": ""
                    },
                    {
                        "first": "Ram",
                        "middle": [
                            "C"
                        ],
                        "last": "Rao",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Journal of Marketing Research",
                "volume": "54",
                "issue": "2",
                "pages": "202--218",
                "other_ids": {
                    "DOI": [
                        "10.1509/jmr.14.0117"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bruce, Norris I., B.P.S. Murthi, and Ram C. Rao (2017), \"A Dynamic Model for Digital Advertising: The E\u21b5ects of Creative Format, Message Content, and Targeting on Engagement,\" Journal of Marketing Research, 54 (2), 202-218 https://doi.org/10.1509/jmr.14.0117.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Evaluating replicability of laboratory experiments in economics",
                "authors": [
                    {
                        "first": "Colin",
                        "middle": [
                            "F"
                        ],
                        "last": "Camerer",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Dreber",
                        "suffix": ""
                    },
                    {
                        "first": "Eskil",
                        "middle": [],
                        "last": "Forsell",
                        "suffix": ""
                    },
                    {
                        "first": "Teck-Hua",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "J\u00fcrgen",
                        "middle": [],
                        "last": "Huber",
                        "suffix": ""
                    },
                    {
                        "first": "Magnus",
                        "middle": [],
                        "last": "Johannesson",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Kirchler",
                        "suffix": ""
                    },
                    {
                        "first": "Johan",
                        "middle": [],
                        "last": "Almenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Altmejd",
                        "suffix": ""
                    },
                    {
                        "first": "Taizan",
                        "middle": [],
                        "last": "Chan",
                        "suffix": ""
                    },
                    {
                        "first": "Emma",
                        "middle": [],
                        "last": "Heikensten",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Holzmeister",
                        "suffix": ""
                    },
                    {
                        "first": "Taisuke",
                        "middle": [],
                        "last": "Imai",
                        "suffix": ""
                    },
                    {
                        "first": "Siri",
                        "middle": [],
                        "last": "Isaksson",
                        "suffix": ""
                    },
                    {
                        "first": "Gideon",
                        "middle": [],
                        "last": "Nave",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Pfei\u21b5er",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Razen",
                        "suffix": ""
                    },
                    {
                        "first": "Hang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Science",
                "volume": "351",
                "issue": "6280",
                "pages": "1433--1436",
                "other_ids": {
                    "DOI": [
                        "10.1126/science.aaf0918"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Camerer, Colin F., Anna Dreber, Eskil Forsell, Teck-Hua Ho, J\u00fcrgen Huber, Magnus Johannesson, Michael Kirchler, Johan Almenberg, Adam Altmejd, Taizan Chan, Emma Heikensten, Felix Holzmeister, Taisuke Imai, Siri Isaksson, Gideon Nave, Thomas Pfei\u21b5er, Michael Razen, and Hang Wu (2016), \"Evaluating replicability of laboratory experiments in economics,\" Science, 351 (6280), 1433-1436 https://doi.org/10.1126/science.aaf0918.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "How Sensory Language Shapes Influencer's Impact",
                "authors": [
                    {
                        "first": "Cascio",
                        "middle": [],
                        "last": "Rizzo",
                        "suffix": ""
                    },
                    {
                        "first": "Giovanni",
                        "middle": [],
                        "last": "Luca",
                        "suffix": ""
                    },
                    {
                        "first": "Jonah",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    },
                    {
                        "first": "Matteo",
                        "middle": [],
                        "last": "De Angelis",
                        "suffix": ""
                    },
                    {
                        "first": "Rumen",
                        "middle": [],
                        "last": "Pozharliev",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Journal of Consumer Research",
                "volume": "50",
                "issue": "4",
                "pages": "810--825",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucad017"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Cascio Rizzo, Giovanni Luca, Jonah Berger, Matteo De Angelis, and Rumen Pozharliev (2023), \"How Sensory Language Shapes Influencer's Impact,\" Journal of Consumer Research, 50 (4), 810-825 https://doi.org/10.1093/jcr/ucad017.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "The past, present and future of Registered Reports",
                "authors": [
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Chambers",
                        "suffix": ""
                    },
                    {
                        "first": "Loukia",
                        "middle": [],
                        "last": "Tzavella",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Nature Human Behaviour",
                "volume": "6",
                "issue": "1",
                "pages": "29--42",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chambers, Christopher D. and Loukia Tzavella (2021), \"The past, present and future of Regis- tered Reports,\" Nature Human Behaviour, 6 (1), 29-42 https://www.nature.com/articles/ s41562-021-01193-7.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "ConvoKit: A Toolkit for the Analysis of Conversations",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [
                            "P"
                        ],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "Caleb",
                        "middle": [],
                        "last": "Chiam",
                        "suffix": ""
                    },
                    {
                        "first": "Liye",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Justine",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Cristian",
                        "middle": [],
                        "last": "Danescu-Niculescu-Mizil",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
                "volume": "",
                "issue": "",
                "pages": "57--60",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chang, Jonathan P., Caleb Chiam, Liye Fu, Andrew Wang, Justine Zhang, and Cristian Danescu- Niculescu-Mizil \"ConvoKit: A Toolkit for the Analysis of Conversations,\" Olivier Pietquin, Smaranda Muresan, Vivian Chen, Casey Kennington, David Vandyke, Nina Dethlefs, Koji In- oue, Erik Ekstedt, and Stefan Ultes, editors, \"Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,\" pages 57-60, 1st virtual meeting: Associ- ation for Computational Linguistics (2020) https://aclanthology.org/2020.sigdial-1.8.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Slowed canonical progress in large fields of science",
                "authors": [
                    {
                        "first": "Johan",
                        "middle": [
                            "S G"
                        ],
                        "last": "Chu",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "A"
                        ],
                        "last": "Evans",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the National Academy of Sciences",
                "volume": "118",
                "issue": "41",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1073/pnas.2021636118"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chu, Johan S. G. and James A. Evans (2021), \"Slowed canonical progress in large fields of sci- ence,\" Proceedings of the National Academy of Sciences, 118 (41), e2021636118 https: //www.pnas.org/doi/abs/10.1073/pnas.2021636118, publisher: Proceedings of the National Academy of Sciences.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "I Really Know You: How Influencers Can Increase Audience Engagement by Referencing Their Close Social Ties",
                "authors": [
                    {
                        "first": "Jaeyeon",
                        "middle": [
                            "("
                        ],
                        "last": "Chung",
                        "suffix": ""
                    },
                    {
                        "first": ")",
                        "middle": [],
                        "last": "Jae",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Ajay",
                        "middle": [],
                        "last": "Kalra",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Journal of Consumer Research",
                "volume": "50",
                "issue": "4",
                "pages": "683--703",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucad019"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chung, Jaeyeon (Jae), Yu Ding, and Ajay Kalra (2023), \"I Really Know You: How Influencers Can Increase Audience Engagement by Referencing Their Close Social Ties,\" Journal of Consumer Research, 50 (4), 683-703 https://doi.org/10.1093/jcr/ucad019.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "The language-as-fixed-e\u21b5ect fallacy: A critique of language statistics in psychological research",
                "authors": [
                    {
                        "first": "Herbert",
                        "middle": [
                            "H"
                        ],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 1973,
                "venue": "Journal of Verbal Learning and Verbal Behavior",
                "volume": "12",
                "issue": "4",
                "pages": "335--359",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Clark, Herbert H. (1973), \"The language-as-fixed-e\u21b5ect fallacy: A critique of language statistics in psychological research,\" Journal of Verbal Learning and Verbal Behavior, 12 (4), 335-359 https://www.sciencedirect.com/science/article/pii/S0022537173800143.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Praise of Convergent Thinking",
                "authors": [
                    {
                        "first": "Arthur",
                        "middle": [],
                        "last": "Cropley",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "18",
                "issue": "",
                "pages": "391--404",
                "other_ids": {
                    "DOI": [
                        "10.1207/s15326934crj1803_13"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Cropley, Arthur (2006), \"In Praise of Convergent Thinking,\" Creativity Research Journal, 18 (3), 391-404 https://doi.org/10.1207/s15326934crj1803 13.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Closing the Marketing Capabilities Gap",
                "authors": [
                    {
                        "first": "George",
                        "middle": [
                            "S"
                        ],
                        "last": "Day",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Marketing",
                "volume": "75",
                "issue": "4",
                "pages": "183--195",
                "other_ids": {
                    "DOI": [
                        "10.1509/jmkg.75.4.183"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Day, George S. (2011), \"Closing the Marketing Capabilities Gap,\" Journal of Marketing, 75 (4), 183-195 https://doi.org/10.1509/jmkg.75.4.183.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Psychological ownership interventions increase interest in claiming government benefits",
                "authors": [
                    {
                        "first": "La",
                        "middle": [],
                        "last": "De",
                        "suffix": ""
                    },
                    {
                        "first": "Wendy",
                        "middle": [],
                        "last": "Rosa",
                        "suffix": ""
                    },
                    {
                        "first": "Eesha",
                        "middle": [],
                        "last": "Sharma",
                        "suffix": ""
                    },
                    {
                        "first": "Stephanie",
                        "middle": [
                            "M"
                        ],
                        "last": "Tully",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Giannella",
                        "suffix": ""
                    },
                    {
                        "first": "Gwen",
                        "middle": [],
                        "last": "Rino",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the National Academy of Sciences",
                "volume": "118",
                "issue": "35",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1073/pnas.2106357118"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "De La Rosa, Wendy, Eesha Sharma, Stephanie M. Tully, Eric Giannella, and Gwen Rino (2021), \"Psychological ownership interventions increase interest in claiming government benefits,\" Proceedings of the National Academy of Sciences, 118 (35), e2106357118 https://doi.org/ 10.1073/pnas.2106357118.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Predict science to improve science",
                "authors": [
                    {
                        "first": "Stefano",
                        "middle": [],
                        "last": "Dellavigna",
                        "suffix": ""
                    },
                    {
                        "first": "Devin",
                        "middle": [],
                        "last": "Pope",
                        "suffix": ""
                    },
                    {
                        "first": "Eva",
                        "middle": [],
                        "last": "Vivalt",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Science",
                "volume": "366",
                "issue": "6464",
                "pages": "428--429",
                "other_ids": {
                    "DOI": [
                        "10.1126/science.aaz1704"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "DellaVigna, Stefano, Devin Pope, and Eva Vivalt (2019), \"Predict science to improve science,\" Science, 366 (6464), 428-429 https://doi.org/10.1126/science.aaz1704.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Using large language models in psychology",
                "authors": [
                    {
                        "first": "Dorottya",
                        "middle": [],
                        "last": "Demszky",
                        "suffix": ""
                    },
                    {
                        "first": "Diyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "S"
                        ],
                        "last": "Yeager",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "J"
                        ],
                        "last": "Bryan",
                        "suffix": ""
                    },
                    {
                        "first": "Margarett",
                        "middle": [],
                        "last": "Clapper",
                        "suffix": ""
                    },
                    {
                        "first": "Susannah",
                        "middle": [],
                        "last": "Chandhok",
                        "suffix": ""
                    },
                    {
                        "first": "Johannes",
                        "middle": [
                            "C"
                        ],
                        "last": "Eichstaedt",
                        "suffix": ""
                    },
                    {
                        "first": "Cameron",
                        "middle": [],
                        "last": "Hecht",
                        "suffix": ""
                    },
                    {
                        "first": "Jeremy",
                        "middle": [],
                        "last": "Jamieson",
                        "suffix": ""
                    },
                    {
                        "first": "Meghann",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Michaela",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Danielle",
                        "middle": [],
                        "last": "Krettek-Cobb",
                        "suffix": ""
                    },
                    {
                        "first": "Leslie",
                        "middle": [],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Nirel",
                        "middle": [],
                        "last": "Jonesmitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Desmond",
                        "middle": [
                            "C"
                        ],
                        "last": "Ong",
                        "suffix": ""
                    },
                    {
                        "first": "Carol",
                        "middle": [
                            "S"
                        ],
                        "last": "Dweck",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "J"
                        ],
                        "last": "Gross",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "W"
                        ],
                        "last": "Pennebaker",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Nature Reviews Psychology",
                "volume": "2",
                "issue": "11",
                "pages": "688--701",
                "other_ids": {
                    "DOI": [
                        "10.1038/s44159-023-00241-5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Demszky, Dorottya, Diyi Yang, David S. Yeager, Christopher J. Bryan, Margarett Clapper, Susan- nah Chandhok, Johannes C. Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann John- son, Michaela Jones, Danielle Krettek-Cobb, Leslie Lai, Nirel JonesMitchell, Desmond C. Ong, Carol S. Dweck, James J. Gross, and James W. Pennebaker (2023), \"Using large language models in psychology,\" Nature Reviews Psychology, 2 (11), 688-701 https: //doi.org/10.1038/s44159-023-00241-5.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "User Engagement with Online Discussion Content: Does it A\u21b5ect Attrition?",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Deolankar",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Varad",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Goli",
                        "suffix": ""
                    },
                    {
                        "first": "Pradeep",
                        "middle": [
                            "K"
                        ],
                        "last": "Sriram",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chintagunta",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.2139/ssrn.4755183"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Deolankar, Varad, Ali Goli, S. Sriram, and Pradeep K. Chintagunta \"User Engagement with Online Discussion Content: Does it A\u21b5ect Attrition?,\" (2024) https://dx.doi.org/10.2139/ ssrn.4755183.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Balancing Exploration and Exploitation Through Structural Design: The Isolation of Subgroups and Organizational Learning",
                "authors": [
                    {
                        "first": "Christina",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Jeho",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Melissa",
                        "middle": [
                            "A"
                        ],
                        "last": "Schilling",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Organization Science",
                "volume": "21",
                "issue": "3",
                "pages": "625--642",
                "other_ids": {
                    "DOI": [
                        "10.1287/orsc.1090.0468"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fang, Christina, Jeho Lee, and Melissa A. Schilling (2010), \"Balancing Exploration and Exploita- tion Through Structural Design: The Isolation of Subgroups and Organizational Learning,\" Organization Science, 21 (3), 625-642 https://doi.org/10.1287/orsc.1090.0468.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "The Creative Cycle and the Growth of Psychological Science",
                "authors": [
                    {
                        "first": "Klaus",
                        "middle": [],
                        "last": "Fiedler",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Perspectives on Psychological Science",
                "volume": "13",
                "issue": "4",
                "pages": "433--438",
                "other_ids": {
                    "DOI": [
                        "10.1177/1745691617745651"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fiedler, Klaus (2018), \"The Creative Cycle and the Growth of Psychological Science,\" Perspectives on Psychological Science, 13 (4), 433-438 https://doi.org/10.1177/1745691617745651, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Creating a Market Orientation: A Longitudinal, Multifirm, Grounded Analysis of Cultural Transformation",
                "authors": [
                    {
                        "first": "Gary",
                        "middle": [
                            "F"
                        ],
                        "last": "Gebhardt",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Gregory",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "F"
                        ],
                        "last": "Carpenter",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sherry",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Journal of Marketing",
                "volume": "70",
                "issue": "4",
                "pages": "37--55",
                "other_ids": {
                    "DOI": [
                        "10.1509/jmkg.70.4.037"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gebhardt, Gary F., Gregory S. Carpenter, and John F. Sherry (2006), \"Creating a Market Orien- tation: A Longitudinal, Multifirm, Grounded Analysis of Cultural Transformation,\" Journal of Marketing, 70 (4), 37-55 https://doi.org/10.1509/jmkg.70.4.037, publisher: SAGE Pub- lications Inc.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Linguistic e\u21b5ects on news headline success: Evidence from thousands of online field experiments",
                "authors": [
                    {
                        "first": "Kristina",
                        "middle": [],
                        "last": "Gligori\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Lifchits",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [],
                        "last": "West",
                        "suffix": ""
                    },
                    {
                        "first": "Ashton",
                        "middle": [],
                        "last": "Anderson",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "PLOS ONE",
                "volume": "18",
                "issue": "3",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0281682"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Gligori\u0107, Kristina, George Lifchits, Robert West, and Ashton Anderson (2023), \"Linguistic e\u21b5ects on news headline success: Evidence from thousands of online field experiments (Registered Report),\" PLOS ONE, 18 (3), e0281682 https://journals.plos.org/plosone/article?id= 10.1371/journal.pone.0281682, publisher: Public Library of Science.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "No Substitute for the Real Thing: The Importance of In-Context Field Experiments in Fundraising",
                "authors": [
                    {
                        "first": "Indranil",
                        "middle": [],
                        "last": "Goswami",
                        "suffix": ""
                    },
                    {
                        "first": "Oleg",
                        "middle": [],
                        "last": "Urminsky",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Marketing Science",
                "volume": "39",
                "issue": "6",
                "pages": "1052--1070",
                "other_ids": {
                    "DOI": [
                        "10.1287/mksc.2020.1252"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Goswami, Indranil and Oleg Urminsky (2020), \"No Substitute for the Real Thing: The Impor- tance of In-Context Field Experiments in Fundraising,\" Marketing Science, 39 (6), 1052- 1070 https://pubsonline.informs.org/doi/abs/10.1287/mksc.2020.1252, publisher: IN- FORMS.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Why Many Behavioral Interventions Have Unpredictable E\u21b5ects in the Wild: The Conflicting Consequences Problem",
                "authors": [
                    {
                        "first": "Indranil",
                        "middle": [],
                        "last": "Goswami",
                        "suffix": ""
                    },
                    {
                        "first": "Oleg",
                        "middle": [],
                        "last": "Urminsky",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Goswami, Indranil and Oleg Urminsky \"Why Many Behavioral Interventions Have Unpre- dictable E\u21b5ects in the Wild: The Conflicting Consequences Problem,\" (2022) https: //papers.ssrn.com/abstract=4199453.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Sending Signals: Strategic Displays of Warmth and Competence",
                "authors": [
                    {
                        "first": "Bushra",
                        "middle": [
                            "S"
                        ],
                        "last": "Guenoun",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [
                            "J"
                        ],
                        "last": "Zlatev",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guenoun, Bushra S. and Julian J. Zlatev \"Sending Signals: Strategic Displays of Warmth and Competence,\" (2023).",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
                "authors": [
                    {
                        "first": "Thilo",
                        "middle": [],
                        "last": "Hagendor\u21b5",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Fabi",
                        "suffix": ""
                    },
                    {
                        "first": "Michal",
                        "middle": [],
                        "last": "Kosinski",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Nature Computational Science",
                "volume": "3",
                "issue": "10",
                "pages": "833--838",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hagendor\u21b5, Thilo, Sarah Fabi, and Michal Kosinski (2023), \"Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT,\" Nature Computational Science, 3 (10), 833-838 https://www.nature.com/articles/s43588-023- 00527-x, publisher: Nature Publishing Group.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Comparing automated text classification methods",
                "authors": [
                    {
                        "first": "Jochen",
                        "middle": [],
                        "last": "Hartmann",
                        "suffix": ""
                    },
                    {
                        "first": "Juliana",
                        "middle": [],
                        "last": "Huppertz",
                        "suffix": ""
                    },
                    {
                        "first": "Christina",
                        "middle": [],
                        "last": "Schamp",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Heitmann",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "International Journal of Research in Marketing",
                "volume": "36",
                "issue": "1",
                "pages": "20--38",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hartmann, Jochen, Juliana Huppertz, Christina Schamp, and Mark Heitmann (2019), \"Com- paring automated text classification methods,\" International Journal of Research in Marketing, 36 (1), 20-38 https://www.sciencedirect.com/science/article/pii/ S0167811618300545.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Natural Language Processing in Marketing",
                "authors": [
                    {
                        "first": "Jochen",
                        "middle": [],
                        "last": "Hartmann",
                        "suffix": ""
                    },
                    {
                        "first": "Oded",
                        "middle": [],
                        "last": "Netzer",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Review of Marketing Research",
                "volume": "20",
                "issue": "",
                "pages": "191--215",
                "other_ids": {
                    "DOI": [
                        "10.1108/S1548-643520230000020011"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hartmann, Jochen and Oded Netzer \"Natural Language Processing in Marketing,\" K. Sudhir and Olivier Toubia, editors, \"Artificial Intelligence in Marketing,\" Vol. 20. of Review of Marketing Research, pages 191-215, Emerald Publishing Limited (2023) https://doi.org/10.1108/ S1548-643520230000020011.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Ownership, Learning, and Beliefs*",
                "authors": [
                    {
                        "first": "Samuel",
                        "middle": [
                            "M"
                        ],
                        "last": "Hartzmark",
                        "suffix": ""
                    },
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Samuel D Hirshman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Imas",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "The Quarterly Journal of Economics",
                "volume": "136",
                "issue": "3",
                "pages": "1665--1717",
                "other_ids": {
                    "DOI": [
                        "10.1093/qje/qjab010"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hartzmark, Samuel M, Samuel D Hirshman, and Alex Imas (2021), \"Ownership, Learning, and Be- liefs*,\" The Quarterly Journal of Economics, 136 (3), 1665-1717 https://doi.org/10.1093/ qje/qjab010.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Research on Innovation: A Review and Agenda for Marketing Science",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Hauser",
                        "suffix": ""
                    },
                    {
                        "first": "Gerard",
                        "middle": [
                            "J"
                        ],
                        "last": "Tellis",
                        "suffix": ""
                    },
                    {
                        "first": "Abbie",
                        "middle": [],
                        "last": "Gri N",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Marketing Science",
                "volume": "25",
                "issue": "6",
                "pages": "687--717",
                "other_ids": {
                    "DOI": [
                        "10.1287/mksc.1050.0144"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hauser, John, Gerard J. Tellis, and Abbie Gri n (2006), \"Research on Innovation: A Re- view and Agenda for Marketing Science,\" Marketing Science, 25 (6), 687-717 https: //pubsonline.informs.org/doi/abs/10.1287/mksc.1050.0144, publisher: INFORMS.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "The two-pizza rule and the secret of Amazon's success",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Hern",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "The Guardian",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hern, Alex (2018), \"The two-pizza rule and the secret of Amazon's success,\" The Guardian https://www.theguardian.com/technology/2018/apr/24/the-two-pizza- rule-and-the-secret-of-amazons-success.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Brand Buzz in the Echoverse",
                "authors": [
                    {
                        "first": "Kelly",
                        "middle": [],
                        "last": "Hewett",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Rand",
                        "suffix": ""
                    },
                    {
                        "first": "Roland",
                        "middle": [
                            "T"
                        ],
                        "last": "Rust",
                        "suffix": ""
                    },
                    {
                        "first": "Harald",
                        "middle": [
                            "J"
                        ],
                        "last": "Van Heerde",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Journal of Marketing",
                "volume": "80",
                "issue": "3",
                "pages": "1--24",
                "other_ids": {
                    "DOI": [
                        "10.1509/jm.15.0033"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hewett, Kelly, William Rand, Roland T. Rust, and Harald J. van Heerde (2016), \"Brand Buzz in the Echoverse,\" Journal of Marketing, 80 (3), 1-24 https://doi.org/10.1509/jm.15.0033, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "The Rise of and Demand for Identity-Oriented Media Coverage",
                "authors": [
                    {
                        "first": "Daniel",
                        "middle": [
                            "J"
                        ],
                        "last": "Hopkins",
                        "suffix": ""
                    },
                    {
                        "first": "Samuel",
                        "middle": [],
                        "last": "Yphtach Lelkes",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wolken",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hopkins, Daniel J., Yphtach Lelkes, and Samuel Wolken \"The Rise of and Demand for Identity- Oriented Media Coverage,\" (2023) https://papers.ssrn.com/abstract=4578004.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Bridging Language and Items for Retrieval and Recommendation",
                "authors": [
                    {
                        "first": "Yupeng",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Jiacheng",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhankui",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "An",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "Xiusi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Mcauley",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2403.03952"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hou, Yupeng, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, and Julian McAuley \"Bridging Language and Items for Retrieval and Recommendation,\" (2024) http://arxiv.org/abs/ 2403.03952, arXiv:2403.03952 [cs].",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Preference reversals between joint and separate evaluations of options: A review and theoretical analysis",
                "authors": [
                    {
                        "first": "Chrisopher",
                        "middle": [
                            "K"
                        ],
                        "last": "Hsee",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "George",
                        "suffix": ""
                    },
                    {
                        "first": "Sally",
                        "middle": [],
                        "last": "Loewenstein",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [
                            "H"
                        ],
                        "last": "Blount",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bazerman",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Psychological Bulletin",
                "volume": "125",
                "issue": "5",
                "pages": "576--590",
                "other_ids": {
                    "DOI": [
                        "http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.125.5.576"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hsee, Chrisopher K., George F. Loewenstein, Sally Blount, and Max H. Bazerman (1999), \"Pref- erence reversals between joint and separate evaluations of options: A review and theoretical analysis.,\" Psychological Bulletin, 125 (5), 576-590 http://doi.apa.org/getdoi.cfm?doi= 10.1037/0033-2909.125.5.576.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Automated Text Analysis for Consumer Research",
                "authors": [
                    {
                        "first": "Ashlee",
                        "middle": [],
                        "last": "Humphreys",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Jen-Hui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Journal of Consumer Research",
                "volume": "44",
                "issue": "6",
                "pages": "1274--1306",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucx104"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Humphreys, Ashlee and Rebecca Jen-Hui Wang (2018), \"Automated Text Analysis for Consumer Research,\" Journal of Consumer Research, 44 (6), 1274-1306 https://doi.org/10.1093/ jcr/ucx104.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Hypotheses devised by AI could find 'blind spots' in research",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Hutson",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Nature Index Publisher: Nature Publishing Group Subject term: Machine learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hutson, Matthew (2023), \"Hypotheses devised by AI could find 'blind spots' in research,\" Nature https://www.nature.com/articles/d41586-023-03596-0, bandiera abtest: a Cg type: Na- ture Index Publisher: Nature Publishing Group Subject term: Machine learning, Computer science, Technology.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "From Text to Thought: How Analyzing Language Can Advance Psychological Science",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Jackson",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Conrad",
                        "suffix": ""
                    },
                    {
                        "first": "Johann-Mattis",
                        "middle": [],
                        "last": "Watts",
                        "suffix": ""
                    },
                    {
                        "first": "Curtis",
                        "middle": [],
                        "last": "List",
                        "suffix": ""
                    },
                    {
                        "first": "Ryan",
                        "middle": [],
                        "last": "Puryear",
                        "suffix": ""
                    },
                    {
                        "first": "Kristen",
                        "middle": [
                            "A"
                        ],
                        "last": "Drabble",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lindquist",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Perspectives on Psychological Science",
                "volume": "17",
                "issue": "3",
                "pages": "805--826",
                "other_ids": {
                    "DOI": [
                        "10.1177/17456916211004899"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jackson, Joshua Conrad, Joseph Watts, Johann-Mattis List, Curtis Puryear, Ryan Drabble, and Kristen A. Lindquist (2022), \"From Text to Thought: How Analyzing Language Can Advance Psychological Science,\" Perspectives on Psychological Science, 17 (3), 805-826 https:// doi.org/10.1177/17456916211004899, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "The Benefits of Candidly Reporting Consumer Research",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Janiszewski",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "J"
                        ],
                        "last": "Stijn",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Van Osselaer",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Journal of Consumer Psychology",
                "volume": "31",
                "issue": "4",
                "pages": "633--646",
                "other_ids": {
                    "DOI": [
                        "10.1002/jcpy.1263"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Janiszewski, Chris and Stijn M. J. van Osselaer (2021), \"The Benefits of Can- didly Reporting Consumer Research,\" Journal of Consumer Psychology, 31 (4), 633-646 https://onlinelibrary.wiley.com/doi/abs/10.1002/jcpy.1263, eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/jcpy.1263.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Consumer Rational (In)Attention to Favorable and Unfavorable Product Information, and Firm Information Design",
                "authors": [
                    {
                        "first": "Kinshuk",
                        "middle": [],
                        "last": "Jerath",
                        "suffix": ""
                    },
                    {
                        "first": "Qitian",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Journal of Marketing Research",
                "volume": "58",
                "issue": "2",
                "pages": "343--362",
                "other_ids": {
                    "DOI": [
                        "10.1177/0022243720977830"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jerath, Kinshuk and Qitian Ren (2021), \"Consumer Rational (In)Attention to Favorable and Unfa- vorable Product Information, and Firm Information Design,\" Journal of Marketing Research, 58 (2), 343-362 https://doi.org/10.1177/0022243720977830, publisher: SAGE Publica- tions Inc.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Does \"Liking\" Lead to Loving? The Impact of Joining a Brand's Social Network on Marketing Outcomes",
                "authors": [
                    {
                        "first": "Leslie",
                        "middle": [
                            "K"
                        ],
                        "last": "John",
                        "suffix": ""
                    },
                    {
                        "first": "Sunil",
                        "middle": [],
                        "last": "Oliver Emrich",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "I"
                        ],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Norton",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Journal of Marketing Research",
                "volume": "54",
                "issue": "1",
                "pages": "144--155",
                "other_ids": {
                    "DOI": [
                        "10.1509/jmr.14.0237"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "John, Leslie K., Oliver Emrich, Sunil Gupta, and Michael I. Norton (2017), \"Does \"Liking\" Lead to Loving? The Impact of Joining a Brand's Social Network on Marketing Outcomes,\" Journal of Marketing Research, 54 (1), 144-155 https://doi.org/10.1509/jmr.14.0237, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Scheduling Content on Social Media: Theory, Evidence, and Application",
                "authors": [
                    {
                        "first": "Vamsi",
                        "middle": [
                            "K"
                        ],
                        "last": "Kanuri",
                        "suffix": ""
                    },
                    {
                        "first": "Yixing",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "(",
                        "middle": [],
                        "last": "Shrihari",
                        "suffix": ""
                    },
                    {
                        "first": ")",
                        "middle": [],
                        "last": "Hari",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sridhar",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Journal of Marketing",
                "volume": "82",
                "issue": "6",
                "pages": "89--108",
                "other_ids": {
                    "DOI": [
                        "10.1177/0022242918805411"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kanuri, Vamsi K., Yixing Chen, and Shrihari (Hari) Sridhar (2018), \"Scheduling Content on Social Media: Theory, Evidence, and Application,\" Journal of Marketing, 82 (6), 89-108 https://doi.org/10.1177/0022242918805411, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Leakage and the Reproducibility Crisis in ML-based Science",
                "authors": [
                    {
                        "first": "Sayash",
                        "middle": [],
                        "last": "Kapoor",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Narayanan",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2207.07048"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kapoor, Sayash and Arvind Narayanan \"Leakage and the Reproducibility Crisis in ML-based Science,\" (2022) http://arxiv.org/abs/2207.07048, arXiv:2207.07048 [cs, stat].",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Leakage and the reproducibility crisis in machinelearning-based science",
                "authors": [
                    {
                        "first": "Sayash",
                        "middle": [],
                        "last": "Kapoor",
                        "suffix": ""
                    },
                    {
                        "first": "Arvind",
                        "middle": [],
                        "last": "Narayanan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Patterns",
                "volume": "4",
                "issue": "9",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kapoor, Sayash and Arvind Narayanan (2023), \"Leakage and the reproducibility crisis in machine- learning-based science,\" Patterns, 4 (9), 100804 https://www.sciencedirect.com/science/ article/pii/S2666389923001599.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Call Me Maybe: Does Customer Feedback-Seeking Impact Non-Solicited Customers?",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kaul",
                        "suffix": ""
                    },
                    {
                        "first": "Stephen",
                        "middle": [
                            "J"
                        ],
                        "last": "Rupali",
                        "suffix": ""
                    },
                    {
                        "first": "Pradeep",
                        "middle": [
                            "K"
                        ],
                        "last": "Anderson",
                        "suffix": ""
                    },
                    {
                        "first": "Naufel",
                        "middle": [],
                        "last": "Chintagunta",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Vilcassim",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaul, Rupali, Stephen J. Anderson, Pradeep K. Chintagunta, and Naufel Vilcassim \"Call Me Maybe: Does Customer Feedback-Seeking Impact Non-Solicited Customers?,\" (2024) https: //papers.ssrn.com/abstract=4507183.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "HARKing: Hypothesizing After the Results are Known",
                "authors": [
                    {
                        "first": "Norbert",
                        "middle": [
                            "L"
                        ],
                        "last": "Kerr",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Personality and Social Psychology Review",
                "volume": "2",
                "issue": "3",
                "pages": "196--217",
                "other_ids": {
                    "DOI": [
                        "10.1207/s15327957pspr0203_4"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kerr, Norbert L. (1998), \"HARKing: Hypothesizing After the Results are Known,\" Personality and Social Psychology Review, 2 (3), 196-217 https://doi.org/10.1207/s15327957pspr0203 4, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Why and how do creative thinking techniques work?: Trading o\u21b5 originality and appropriateness to make more creative advertising",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Kilgour",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Koslow",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Journal of the Academy of Marketing Science",
                "volume": "37",
                "issue": "3",
                "pages": "298--309",
                "other_ids": {
                    "DOI": [
                        "10.1007/s11747-009-0133-5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kilgour, Mark and Scott Koslow (2009), \"Why and how do creative thinking techniques work?: Trading o\u21b5 originality and appropriateness to make more creative advertising,\" Journal of the Academy of Marketing Science, 37 (3), 298-309 https://doi.org/10.1007/s11747-009- 0133-5.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "Deconstructing disengagement: analyzing learner subpopulations in massive open online courses",
                "authors": [
                    {
                        "first": "Ren\u00e9",
                        "middle": [
                            "F"
                        ],
                        "last": "Kizilcec",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Piech",
                        "suffix": ""
                    },
                    {
                        "first": "Emily",
                        "middle": [],
                        "last": "Schneider",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of the Third International Conference on Learning Analytics and Knowledge",
                "volume": "",
                "issue": "",
                "pages": "170--179",
                "other_ids": {
                    "DOI": [
                        "10.1145/2460296.2460330"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kizilcec, Ren\u00e9 F., Chris Piech, and Emily Schneider \"Deconstructing disengagement: analyzing learner subpopulations in massive open online courses,\" \"Proceedings of the Third Interna- tional Conference on Learning Analytics and Knowledge,\" LAK '13, pages 170-179, New York, NY, USA: Association for Computing Machinery (2013) https://dl.acm.org/doi/10.1145/ 2460296.2460330.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "Confirmation, disconfirmation, and information in hypothesis testing",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Klayman",
                        "suffix": ""
                    },
                    {
                        "first": "Young-Won",
                        "middle": [],
                        "last": "Ha",
                        "suffix": ""
                    }
                ],
                "year": 1987,
                "venue": "Psychological Review",
                "volume": "94",
                "issue": "2",
                "pages": "211--228",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Klayman, Joshua and Young-won Ha (1987), \"Confirmation, disconfirmation, and information in hypothesis testing,\" Psychological Review, 94 (2), 211-228 Place: US Publisher: American Psychological Association.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Prediction Policy Problems",
                "authors": [
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Kleinberg",
                        "suffix": ""
                    },
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Ludwig",
                        "suffix": ""
                    },
                    {
                        "first": "Sendhil",
                        "middle": [],
                        "last": "Mullainathan",
                        "suffix": ""
                    },
                    {
                        "first": "Ziad",
                        "middle": [],
                        "last": "Obermeyer",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "American Economic Review",
                "volume": "105",
                "issue": "5",
                "pages": "491--495",
                "other_ids": {
                    "DOI": [
                        "https://www.aeaweb.org/articles?id=10.1257/aer.p20151023"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kleinberg, Jon, Jens Ludwig, Sendhil Mullainathan, and Ziad Obermeyer (2015), \"Prediction Policy Problems,\" American Economic Review, 105 (5), 491-495 https://www.aeaweb.org/ articles?id=10.1257/aer.p20151023.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Experimentation and Start-up Performance: Evidence from A/B Testing",
                "authors": [
                    {
                        "first": "Rembrand",
                        "middle": [],
                        "last": "Koning",
                        "suffix": ""
                    },
                    {
                        "first": "Sharique",
                        "middle": [],
                        "last": "Hasan",
                        "suffix": ""
                    },
                    {
                        "first": "Aaron",
                        "middle": [],
                        "last": "Chatterji",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Management Science",
                "volume": "68",
                "issue": "9",
                "pages": "6434--6453",
                "other_ids": {
                    "DOI": [
                        "10.1287/mnsc.2021.4209"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Koning, Rembrand, Sharique Hasan, and Aaron Chatterji (2022), \"Experimentation and Start-up Performance: Evidence from A/B Testing,\" Management Science, 68 (9), 6434-6453 https: //pubsonline.informs.org/doi/abs/10.1287/mnsc.2021.4209, publisher: INFORMS.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "The Small-Area Hypothesis: E\u21b5ects of Progress Monitoring on Goal Adherence",
                "authors": [
                    {
                        "first": "Minjung",
                        "middle": [],
                        "last": "Koo",
                        "suffix": ""
                    },
                    {
                        "first": "Ayelet",
                        "middle": [],
                        "last": "Fishbach",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Journal of Consumer Research",
                "volume": "39",
                "issue": "3",
                "pages": "493--509",
                "other_ids": {
                    "DOI": [
                        "10.1086/663827"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Koo, Minjung and Ayelet Fishbach (2012), \"The Small-Area Hypothesis: E\u21b5ects of Progress Monitoring on Goal Adherence,\" Journal of Consumer Research, 39 (3), 493-509 https: //doi.org/10.1086/663827.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "The Dual Challenge of Search and Coordination for Organizational Adaptation: How Structures of Influence Matter",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ko\u00e7ak",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "A"
                        ],
                        "last": "\u00d6zgecan",
                        "suffix": ""
                    },
                    {
                        "first": "Phanish",
                        "middle": [],
                        "last": "Levinthal",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Puranam",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Organization Science",
                "volume": "34",
                "issue": "2",
                "pages": "851--869",
                "other_ids": {
                    "DOI": [
                        "10.1287/orsc.2022.1601"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ko\u00e7ak, \u00d6zgecan, Daniel A. Levinthal, and Phanish Puranam (2023), \"The Dual Challenge of Search and Coordination for Organizational Adaptation: How Structures of Influence Mat- ter,\" Organization Science, 34 (2), 851-869 https://pubsonline.informs.org/doi/full/ 10.1287/orsc.2022.1601, publisher: INFORMS.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses",
                "authors": [
                    {
                        "first": "Dani\u00ebl",
                        "middle": [],
                        "last": "Lakens",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Social Psychological and Personality Science",
                "volume": "8",
                "issue": "4",
                "pages": "355--362",
                "other_ids": {
                    "DOI": [
                        "10.1177/1948550617697177"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lakens, Dani\u00ebl (2017), \"Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses,\" Social Psychological and Personality Science, 8 (4), 355-362 http: //journals.sagepub.com/doi/10.1177/1948550617697177.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "Crowdsourcing hypothesis tests: Making transparent how design choices shape research results",
                "authors": [
                    {
                        "first": "Justin",
                        "middle": [
                            "F"
                        ],
                        "last": "Landy",
                        "suffix": ""
                    },
                    {
                        "first": "(",
                        "middle": [],
                        "last": "Miaolei",
                        "suffix": ""
                    },
                    {
                        "first": ")",
                        "middle": [],
                        "last": "Liam",
                        "suffix": ""
                    },
                    {
                        "first": "Isabel",
                        "middle": [
                            "L"
                        ],
                        "last": "Jia",
                        "suffix": ""
                    },
                    {
                        "first": "Domenico",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Warren",
                        "middle": [],
                        "last": "Viganola",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Tierney",
                        "suffix": ""
                    },
                    {
                        "first": "Magnus",
                        "middle": [],
                        "last": "Dreber",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [],
                        "last": "Johannesson",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [
                            "R"
                        ],
                        "last": "Pfei\u21b5er",
                        "suffix": ""
                    },
                    {
                        "first": "Quentin",
                        "middle": [
                            "F"
                        ],
                        "last": "Ebersole",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Gronau",
                        "suffix": ""
                    },
                    {
                        "first": "Don",
                        "middle": [],
                        "last": "Ly",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Van Den",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Bergh",
                        "suffix": ""
                    },
                    {
                        "first": "Koen",
                        "middle": [],
                        "last": "Marsman",
                        "suffix": ""
                    },
                    {
                        "first": "Eric-Jan",
                        "middle": [],
                        "last": "Derks",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Wagenmakers",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "M"
                        ],
                        "last": "Proctor",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "W"
                        ],
                        "last": "Bartels",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "J"
                        ],
                        "last": "Bauman",
                        "suffix": ""
                    },
                    {
                        "first": "Felix",
                        "middle": [],
                        "last": "Brady",
                        "suffix": ""
                    },
                    {
                        "first": "Andrei",
                        "middle": [],
                        "last": "Cheung",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Cimpian",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Simone Dohle",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Brent Donnellan",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "P"
                        ],
                        "last": "Hahn",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Hall",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [
                            "J"
                        ],
                        "last": "Jim\u00e9nez-Leal",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [
                            "E"
                        ],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Beno\u00eet",
                        "middle": [],
                        "last": "Lucas",
                        "suffix": ""
                    },
                    {
                        "first": "Andres",
                        "middle": [],
                        "last": "Monin",
                        "suffix": ""
                    },
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Montealegre",
                        "suffix": ""
                    },
                    {
                        "first": "Jun",
                        "middle": [],
                        "last": "Mullen",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [],
                        "last": "Pang",
                        "suffix": ""
                    },
                    {
                        "first": "Diego",
                        "middle": [
                            "A"
                        ],
                        "last": "Ray",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Reinero",
                        "suffix": ""
                    },
                    {
                        "first": "Walter",
                        "middle": [],
                        "last": "Reynolds",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Sowden",
                        "suffix": ""
                    },
                    {
                        "first": "Runkun",
                        "middle": [],
                        "last": "Storage",
                        "suffix": ""
                    },
                    {
                        "first": "Christina",
                        "middle": [
                            "M"
                        ],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [
                            "J"
                        ],
                        "last": "Tworek",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Van Bavel",
                        "suffix": ""
                    },
                    {
                        "first": "Julian",
                        "middle": [],
                        "last": "Walco",
                        "suffix": ""
                    },
                    {
                        "first": ";",
                        "middle": [],
                        "last": "Wills",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Eric",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Uhlmann",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "The Crowdsourcing Hypothesis Tests Collaboration",
                "volume": "146",
                "issue": "",
                "pages": "451--479",
                "other_ids": {
                    "DOI": [
                        "10.1037/bul0000220"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Landy, Justin F., Miaolei (Liam) Jia, Isabel L. Ding, Domenico Viganola, Warren Tierney, Anna Dreber, Magnus Johannesson, Thomas Pfei\u21b5er, Charles R. Ebersole, Quentin F. Gronau, Alexander Ly, Don Van Den Bergh, Maarten Marsman, Koen Derks, Eric-Jan Wagenmak- ers, Andrew Proctor, Daniel M. Bartels, Christopher W. Bauman, William J. Brady, Fe- lix Cheung, Andrei Cimpian, Simone Dohle, M. Brent Donnellan, Adam Hahn, Michael P. Hall, William Jim\u00e9nez-Leal, David J. Johnson, Richard E. Lucas, Beno\u00eet Monin, Andres Montealegre, Elizabeth Mullen, Jun Pang, Jennifer Ray, Diego A. Reinero, Jesse Reynolds, Walter Sowden, Daniel Storage, Runkun Su, Christina M. Tworek, Jay J. Van Bavel, Daniel Walco, Julian Wills, Xiaobing Xu, Kai Chi Yam, Xiaoyu Yang, William A. Cun- ningham, Martin Schweinsberg, Molly Urwitz, The Crowdsourcing Hypothesis Tests Collab- oration, and Eric L. Uhlmann (2020), \"Crowdsourcing hypothesis tests: Making transpar- ent how design choices shape research results.,\" Psychological Bulletin, 146 (5), 451-479 https://doi.apa.org/doi/10.1037/bul0000220.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Advertising Content and Consumer Engagement on Social Media: Evidence from Facebook",
                "authors": [
                    {
                        "first": "Dokyun",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Kartik",
                        "middle": [],
                        "last": "Hosanagar",
                        "suffix": ""
                    },
                    {
                        "first": "Harikesh",
                        "middle": [
                            "S"
                        ],
                        "last": "Nair",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Management Science",
                "volume": "64",
                "issue": "11",
                "pages": "5105--5131",
                "other_ids": {
                    "DOI": [
                        "10.1287/mnsc.2017.2902"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lee, Dokyun, Kartik Hosanagar, and Harikesh S. Nair (2018), \"Advertising Content and Consumer Engagement on Social Media: Evidence from Facebook,\" Management Science, 64 (11), 5105- 5131 https://doi.org/10.1287/mnsc.2017.2902, publisher: INFORMS.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "The formality e\u21b5ect",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Linos",
                        "suffix": ""
                    },
                    {
                        "first": "Jessica",
                        "middle": [],
                        "last": "Lasky-Fink",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Larkin",
                        "suffix": ""
                    },
                    {
                        "first": "Lindsay",
                        "middle": [],
                        "last": "Moore",
                        "suffix": ""
                    },
                    {
                        "first": "Elspeth",
                        "middle": [],
                        "last": "Kirkman",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Nature Human Behaviour",
                "volume": "8",
                "issue": "2",
                "pages": "300--310",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Linos, Elizabeth, Jessica Lasky-Fink, Chris Larkin, Lindsay Moore, and Elspeth Kirkman (2024), \"The formality e\u21b5ect,\" Nature Human Behaviour, 8 (2), 300-310 https://www.nature.com/ articles/s41562-023-01761-z, publisher: Nature Publishing Group.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "Can Nudges Increase Take-Up of the EITC? Evidence from Multiple Field Experiments",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Linos",
                        "suffix": ""
                    },
                    {
                        "first": "Allen",
                        "middle": [],
                        "last": "Prohofsky",
                        "suffix": ""
                    },
                    {
                        "first": "Aparna",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Rothstein",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Unrath",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "American Economic Journal: Economic Policy",
                "volume": "14",
                "issue": "4",
                "pages": "432--452",
                "other_ids": {
                    "DOI": [
                        "https://www.aeaweb.org/articles?id=10.1257/pol.20200603"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Linos, Elizabeth, Allen Prohofsky, Aparna Ramesh, Jesse Rothstein, and Matthew Unrath (2022), \"Can Nudges Increase Take-Up of the EITC? Evidence from Multiple Field Experiments,\" American Economic Journal: Economic Policy, 14 (4), 432-452 https://www.aeaweb.org/ articles?id=10.1257/pol.20200603.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "The creative cli\u21b5 illusion",
                "authors": [
                    {
                        "first": "Brian",
                        "middle": [
                            "J"
                        ],
                        "last": "Lucas",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Loran",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Nordgren",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the National Academy of Sciences",
                "volume": "117",
                "issue": "33",
                "pages": "19830--19836",
                "other_ids": {
                    "DOI": [
                        "10.1073/pnas.2005620117"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lucas, Brian J. and Loran F. Nordgren (2020), \"The creative cli\u21b5 illusion,\" Proceedings of the National Academy of Sciences, 117 (33), 19830-19836 https://pnas.org/doi/full/10.1073/ pnas.2005620117.",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "Machine Learning as a Tool for Hypothesis Generation*",
                "authors": [
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Ludwig",
                        "suffix": ""
                    },
                    {
                        "first": "Sendhil",
                        "middle": [],
                        "last": "Mullainathan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "The Quarterly Journal of Economics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1093/qje/qjad055"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ludwig, Jens and Sendhil Mullainathan (2024), \"Machine Learning as a Tool for Hypothesis Generation*,\" The Quarterly Journal of Economics, page qjad055 https://doi.org/10.1093/ qje/qjad055.",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "Knowledge creation in consumer research: Multiple routes, multiple criteria",
                "authors": [
                    {
                        "first": "John",
                        "middle": [
                            "G"
                        ],
                        "last": "Lynch",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Joseph",
                        "suffix": ""
                    },
                    {
                        "first": "Aradhna",
                        "middle": [],
                        "last": "Alba",
                        "suffix": ""
                    },
                    {
                        "first": "Vicki",
                        "middle": [
                            "G"
                        ],
                        "last": "Krishna",
                        "suffix": ""
                    },
                    {
                        "first": "Zeynep",
                        "middle": [],
                        "last": "Morwitz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "G\u00fcrhan-Canli",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Journal of Consumer Psychology",
                "volume": "22",
                "issue": "4",
                "pages": "473--485",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Lynch, John G., Joseph W. Alba, Aradhna Krishna, Vicki G. Morwitz, and Zeynep G\u00fcrhan-Canli (2012), \"Knowledge creation in consumer research: Multiple routes, multiple criteria,\" Jour- nal of Consumer Psychology, 22 (4), 473-485 https://www.sciencedirect.com/science/ article/pii/S1057740812000952.",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "Creating Boundary-Breaking, Marketing-Relevant Consumer Research",
                "authors": [
                    {
                        "first": "Deborah",
                        "middle": [
                            "J"
                        ],
                        "last": "Macinnis",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Vicki",
                        "suffix": ""
                    },
                    {
                        "first": "Simona",
                        "middle": [],
                        "last": "Morwitz",
                        "suffix": ""
                    },
                    {
                        "first": "Donna",
                        "middle": [
                            "L"
                        ],
                        "last": "Botti",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "V"
                        ],
                        "last": "Ho\u21b5man",
                        "suffix": ""
                    },
                    {
                        "first": "Donald",
                        "middle": [
                            "R"
                        ],
                        "last": "Kozinets",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "G"
                        ],
                        "last": "Lehmann",
                        "suffix": ""
                    },
                    {
                        "first": "Cornelia",
                        "middle": [],
                        "last": "Lynch",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Pechmann",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Marketing",
                "volume": "84",
                "issue": "2",
                "pages": "1--23",
                "other_ids": {
                    "DOI": [
                        "10.1177/0022242919889876"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "MacInnis, Deborah J., Vicki G. Morwitz, Simona Botti, Donna L. Ho\u21b5man, Robert V. Kozinets, Donald R. Lehmann, John G. Lynch, and Cornelia Pechmann (2020), \"Creating Boundary- Breaking, Marketing-Relevant Consumer Research,\" Journal of Marketing, 84 (2), 1-23 https://doi.org/10.1177/0022242919889876, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "Divergent and Convergent Creativity Are Di\u21b5erent Kinds of Foraging",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Malaie",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "J"
                        ],
                        "last": "Soran",
                        "suffix": ""
                    },
                    {
                        "first": "Tyler",
                        "middle": [],
                        "last": "Spivey",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Marghetis",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Psychological Science",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1177/09567976241245695"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Malaie, Soran, Michael J. Spivey, and Tyler Marghetis (2024), \"Divergent and Convergent Cre- ativity Are Di\u21b5erent Kinds of Foraging,\" Psychological Science, page 09567976241245695 https://doi.org/10.1177/09567976241245695, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF82": {
                "ref_id": "b82",
                "title": "Knowing versus Naming: Similarity and the Linguistic Categorization of Artifacts",
                "authors": [
                    {
                        "first": "Barbara",
                        "middle": [
                            "C"
                        ],
                        "last": "Malt",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Steven",
                        "suffix": ""
                    },
                    {
                        "first": "Silvia",
                        "middle": [],
                        "last": "Sloman",
                        "suffix": ""
                    },
                    {
                        "first": "Meiyi",
                        "middle": [],
                        "last": "Gennari",
                        "suffix": ""
                    },
                    {
                        "first": "Yuan",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Journal of Memory and Language",
                "volume": "40",
                "issue": "2",
                "pages": "230--262",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Malt, Barbara C., Steven A. Sloman, Silvia Gennari, Meiyi Shi, and Yuan Wang (1999), \"Knowing versus Naming: Similarity and the Linguistic Categorization of Artifacts,\" Journal of Memory and Language, 40 (2), 230-262 https://www.sciencedirect.com/science/article/pii/ S0749596X98925931.",
                "links": null
            },
            "BIBREF83": {
                "ref_id": "b83",
                "title": "Automated Social Science: Language Models as Scientist and Subjects",
                "authors": [
                    {
                        "first": "Benjamin",
                        "middle": [
                            "S"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Kehang",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "J"
                        ],
                        "last": "Horton",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Manning, Benjamin S., Kehang Zhu, and John J. Horton \"Automated Social Science: Language Models as Scientist and Subjects,\" (2024) https://www.nber.org/papers/w32381.",
                "links": null
            },
            "BIBREF84": {
                "ref_id": "b84",
                "title": "The predictive utility of word familiarity for online engagements and funding",
                "authors": [
                    {
                        "first": "David",
                        "middle": [
                            "M"
                        ],
                        "last": "Markowitz",
                        "suffix": ""
                    },
                    {
                        "first": "Hillary",
                        "middle": [
                            "C"
                        ],
                        "last": "Shulman",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the National Academy of Sciences",
                "volume": "118",
                "issue": "18",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1073/pnas.2026045118"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Markowitz, David M. and Hillary C. Shulman (2021), \"The predictive utility of word familiarity for online engagements and funding,\" Proceedings of the National Academy of Sciences, 118 (18), e2026045118 https://www.pnas.org/doi/abs/10.1073/pnas.2026045118, publisher: Pro- ceedings of the National Academy of Sciences.",
                "links": null
            },
            "BIBREF85": {
                "ref_id": "b85",
                "title": "The Upworthy Research Archive, a time series of 32,487 experiments in U.S. media",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Matias",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Nathan",
                        "suffix": ""
                    },
                    {
                        "first": "Marianne",
                        "middle": [],
                        "last": "Munger",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Aubin Le Quere",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ebersole",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "8",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole (2021), \"The Upworthy Research Archive, a time series of 32,487 experiments in U.S. media,\" Scientific Data, 8 (1), 195 https://www.nature.com/articles/s41597-021-00934-7.",
                "links": null
            },
            "BIBREF86": {
                "ref_id": "b86",
                "title": "Creative Hypothesis Generating in Psychology: Some Useful Heuristics",
                "authors": [
                    {
                        "first": "William",
                        "middle": [
                            "J"
                        ],
                        "last": "Mcguire",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Annual Review of Psychology",
                "volume": "48",
                "issue": "1",
                "pages": "1--30",
                "other_ids": {
                    "DOI": [
                        "10.1146/annurev.psych.48.1.1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "McGuire, William J. (1997), \"Creative Hypothesis Generating in Psychology: Some Use- ful Heuristics,\" Annual Review of Psychology, 48 (1), 1-30 https://doi.org/10.1146/ annurev.psych.48.1.1.",
                "links": null
            },
            "BIBREF87": {
                "ref_id": "b87",
                "title": "Artificial intelligence and illusions of understanding in scientific research",
                "authors": [
                    {
                        "first": "Lisa",
                        "middle": [],
                        "last": "Messeri",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "J"
                        ],
                        "last": "Crockett",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Nature",
                "volume": "627",
                "issue": "8002",
                "pages": "49--58",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Messeri, Lisa and M. J. Crockett (2024), \"Artificial intelligence and illusions of understanding in scientific research,\" Nature, 627 (8002), 49-58 https://www.nature.com/articles/s41586- 024-07146-0, publisher: Nature Publishing Group.",
                "links": null
            },
            "BIBREF88": {
                "ref_id": "b88",
                "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
                "authors": [
                    {
                        "first": "Sewon",
                        "middle": [],
                        "last": "Min",
                        "suffix": ""
                    },
                    {
                        "first": "Xinxi",
                        "middle": [],
                        "last": "Lyu",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Mikel",
                        "middle": [],
                        "last": "Artetxe",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Hannaneh",
                        "middle": [],
                        "last": "Hajishirzi",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2202.12837"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Min, Sewon, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer \"Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?,\" (2022) http://arxiv.org/abs/2202.12837, arXiv:2202.12837 [cs].",
                "links": null
            },
            "BIBREF89": {
                "ref_id": "b89",
                "title": "Full-Cycle Social Psychology for Theory and Application",
                "authors": [
                    {
                        "first": "Christine",
                        "middle": [],
                        "last": "Moorman",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [
                            "S"
                        ],
                        "last": "Day ; Mortensen",
                        "suffix": ""
                    },
                    {
                        "first": "Chad",
                        "middle": [
                            "R"
                        ],
                        "last": "Robert",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Cialdini",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Social and Personality Psychology Compass",
                "volume": "80",
                "issue": "6",
                "pages": "53--63",
                "other_ids": {
                    "DOI": [
                        "10.1111/j.1751-9004.2009.00239.x"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Moorman, Christine and George S. Day (2016), \"Organizing for Marketing Excellence,\" Journal of Marketing, 80 (6), 6-35 https://doi.org/10.1509/jm.15.0423, publisher: SAGE Publications Inc. Mortensen, Chad R. and Robert B. Cialdini (2010), \"Full-Cycle Social Psychology for Theory and Application,\" Social and Personality Psychology Compass, 4 (1), 53-63 https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-9004.2009.00239.x, eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-9004.2009.00239.x.",
                "links": null
            },
            "BIBREF90": {
                "ref_id": "b90",
                "title": "Does Measuring Intent Change Behavior?",
                "authors": [
                    {
                        "first": "Vicki",
                        "middle": [
                            "G"
                        ],
                        "last": "Morwitz",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Schmittlein",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "Journal of Consumer Research",
                "volume": "20",
                "issue": "1",
                "pages": "46--61",
                "other_ids": {
                    "DOI": [
                        "10.1086/209332"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Morwitz, Vicki G., Eric Johnson, and David Schmittlein (1993), \"Does Measuring Intent Change Behavior?,\" Journal of Consumer Research, 20 (1), 46-61 https://doi.org/10.1086/209332.",
                "links": null
            },
            "BIBREF91": {
                "ref_id": "b91",
                "title": "Machine Learning: An Applied Econometric Approach",
                "authors": [
                    {
                        "first": "Sendhil",
                        "middle": [],
                        "last": "Mullainathan",
                        "suffix": ""
                    },
                    {
                        "first": "Jann",
                        "middle": [],
                        "last": "Spiess",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Journal of Economic Perspectives",
                "volume": "31",
                "issue": "2",
                "pages": "87--106",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mullainathan, Sendhil and Jann Spiess (2017), \"Machine Learning: An Applied Econometric Approach,\" Journal of Economic Perspectives, 31 (2), 87-106 https://www.aeaweb.org/ articles?id=10.1257%2Fjep.31.2.87&ref=ds-econ.",
                "links": null
            },
            "BIBREF92": {
                "ref_id": "b92",
                "title": "What is an A/B Test?",
                "authors": [
                    {
                        "first": "Netflix",
                        "middle": [],
                        "last": "Technology",
                        "suffix": ""
                    },
                    {
                        "first": "Blog",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Netflix Technology Blog \"What is an A/B Test?,\" (2022) https://netflixtechblog.com/what- is-an-a-b-test-b08cc1b57962.",
                "links": null
            },
            "BIBREF93": {
                "ref_id": "b93",
                "title": "Mine Your Own Business: Market-Structure Surveillance Through Text Mining",
                "authors": [
                    {
                        "first": "Oded",
                        "middle": [],
                        "last": "Netzer",
                        "suffix": ""
                    },
                    {
                        "first": "Ronen",
                        "middle": [],
                        "last": "Feldman",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Goldenberg",
                        "suffix": ""
                    },
                    {
                        "first": "Moshe",
                        "middle": [],
                        "last": "Fresko",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Marketing Science",
                "volume": "31",
                "issue": "3",
                "pages": "521--543",
                "other_ids": {
                    "DOI": [
                        "10.1287/mksc.1120.0713"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Netzer, Oded, Ronen Feldman, Jacob Goldenberg, and Moshe Fresko (2012), \"Mine Your Own Busi- ness: Market-Structure Surveillance Through Text Mining,\" Marketing Science, 31 (3), 521- 543 https://pubsonline.informs.org/doi/abs/10.1287/mksc.1120.0713, publisher: IN- FORMS.",
                "links": null
            },
            "BIBREF94": {
                "ref_id": "b94",
                "title": "When Words Sweat: Identifying Signals for Loan Default in the Text of Loan Applications",
                "authors": [
                    {
                        "first": "Oded",
                        "middle": [],
                        "last": "Netzer",
                        "suffix": ""
                    },
                    {
                        "first": "Alain",
                        "middle": [],
                        "last": "Lemaire",
                        "suffix": ""
                    },
                    {
                        "first": "Michal",
                        "middle": [],
                        "last": "Herzenstein",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Journal of Marketing Research",
                "volume": "56",
                "issue": "6",
                "pages": "960--980",
                "other_ids": {
                    "DOI": [
                        "10.1177/0022243719852959"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Netzer, Oded, Alain Lemaire, and Michal Herzenstein (2019), \"When Words Sweat: Identifying Signals for Loan Default in the Text of Loan Applications,\" Journal of Marketing Research, 56 (6), 960-980 https://doi.org/10.1177/0022243719852959, publisher: SAGE Publica- tions Inc.",
                "links": null
            },
            "BIBREF95": {
                "ref_id": "b95",
                "title": "The GPT Surprise: O\u21b5ering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances",
                "authors": [
                    {
                        "first": "Allen",
                        "middle": [],
                        "last": "Nie",
                        "suffix": ""
                    },
                    {
                        "first": "Yash",
                        "middle": [],
                        "last": "Chandak",
                        "suffix": ""
                    },
                    {
                        "first": "Miroslav",
                        "middle": [],
                        "last": "Suzara",
                        "suffix": ""
                    },
                    {
                        "first": "Malika",
                        "middle": [],
                        "last": "Ali",
                        "suffix": ""
                    },
                    {
                        "first": "Juliette",
                        "middle": [],
                        "last": "Woodrow",
                        "suffix": ""
                    },
                    {
                        "first": "Matt",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Mehran",
                        "middle": [],
                        "last": "Sahami",
                        "suffix": ""
                    },
                    {
                        "first": "Emma",
                        "middle": [],
                        "last": "Brunskill",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Piech",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nie, Allen, Yash Chandak, Miroslav Suzara, Malika Ali, Juliette Woodrow, Matt Peng, Mehran Sahami, Emma Brunskill, and Chris Piech \"The GPT Surprise: O\u21b5ering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances,\" (2024) https://osf.io/qy8zd.",
                "links": null
            },
            "BIBREF96": {
                "ref_id": "b96",
                "title": "Registered Reports: A Method to Increase the Credibility of Published Results",
                "authors": [
                    {
                        "first": "Brian",
                        "middle": [
                            "A"
                        ],
                        "last": "Nosek",
                        "suffix": ""
                    },
                    {
                        "first": "Dani\u00ebl",
                        "middle": [],
                        "last": "Lakens",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Social Psychology",
                "volume": "45",
                "issue": "3",
                "pages": "137--141",
                "other_ids": {
                    "DOI": [
                        "10.1027/1864-9335/a000192"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nosek, Brian A. and Dani\u00ebl Lakens (2014), \"Registered Reports: A Method to Increase the Credibility of Published Results,\" Social Psychology, 45 (3), 137-141 https:// econtent.hogrefe.com/doi/10.1027/1864-9335/a000192.",
                "links": null
            },
            "BIBREF97": {
                "ref_id": "b97",
                "title": "Machine learning and data mining: strategies for hypothesis generation",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "A"
                        ],
                        "last": "Oquendo",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Baca-Garcia",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Art\u00e9s-Rodr\u00edguez",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Perez-Cruz",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [
                            "C"
                        ],
                        "last": "Galfalvy",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Blasco-Fontecilla",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Madigan",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Molecular Psychiatry",
                "volume": "17",
                "issue": "10",
                "pages": "956--959",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oquendo, M. A., E. Baca-Garcia, A. Art\u00e9s-Rodr\u00edguez, F. Perez-Cruz, H. C. Galfalvy, H. Blasco- Fontecilla, D. Madigan, and N. Duan (2012), \"Machine learning and data mining: strategies for hypothesis generation,\" Molecular Psychiatry, 17 (10), 956-959 https://www.nature.com/ articles/mp2011173.",
                "links": null
            },
            "BIBREF98": {
                "ref_id": "b98",
                "title": "Policy Choice and the Wisdom of Crowds",
                "authors": [
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Otis",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Otis, Nicholas \"Policy Choice and the Wisdom of Crowds,\" (2022) https://papers.ssrn.com/ abstract=4200841.",
                "links": null
            },
            "BIBREF99": {
                "ref_id": "b99",
                "title": "The Emergence and Evolution of Consumer Language Research",
                "authors": [
                    {
                        "first": "Grant",
                        "middle": [],
                        "last": "Packard",
                        "suffix": ""
                    },
                    {
                        "first": "Jonah",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Journal of Consumer Research",
                "volume": "51",
                "issue": "1",
                "pages": "42--51",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucad013"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Packard, Grant and Jonah Berger (2024), \"The Emergence and Evolution of Consumer Language Research,\" Journal of Consumer Research, 51 (1), 42-51 https://doi.org/10.1093/jcr/ ucad013.",
                "links": null
            },
            "BIBREF100": {
                "ref_id": "b100",
                "title": "Using large-scale experiments and machine learning to discover theories of human decision-making",
                "authors": [
                    {
                        "first": "Joshua",
                        "middle": [
                            "C"
                        ],
                        "last": "Peterson",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "David",
                        "suffix": ""
                    },
                    {
                        "first": "Mayank",
                        "middle": [],
                        "last": "Bourgin",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Agrawal",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "L"
                        ],
                        "last": "Reichman",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Griffiths",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Science",
                "volume": "372",
                "issue": "6547",
                "pages": "1209--1214",
                "other_ids": {
                    "DOI": [
                        "10.1126/science.abe2629"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Peterson, Joshua C., David D. Bourgin, Mayank Agrawal, Daniel Reichman, and Thomas L. Grif- fiths (2021), \"Using large-scale experiments and machine learning to discover theories of human decision-making,\" Science, 372 (6547), 1209-1214 https://www.science.org/doi/ full/10.1126/science.abe2629.",
                "links": null
            },
            "BIBREF101": {
                "ref_id": "b101",
                "title": "The Elaboration Likelihood Model of Persuasion",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [
                            "E"
                        ],
                        "last": "Petty",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "T"
                        ],
                        "last": "Cacioppo",
                        "suffix": ""
                    }
                ],
                "year": 1986,
                "venue": "Advances in Experimental Social Psychology",
                "volume": "19",
                "issue": "",
                "pages": "123--205",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Petty, Richard E. and John T. Cacioppo \"The Elaboration Likelihood Model of Persuasion,\" \"Advances in Experimental Social Psychology,\" Vol. 19., pages 123-205, Elsevier (1986).",
                "links": null
            },
            "BIBREF102": {
                "ref_id": "b102",
                "title": "Narrative and Persuasion in Fashion Advertising",
                "authors": [
                    {
                        "first": "Barbara",
                        "middle": [
                            "J"
                        ],
                        "last": "Phillips",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Edward",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mcquarrie",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Journal of Consumer Research",
                "volume": "37",
                "issue": "3",
                "pages": "368--392",
                "other_ids": {
                    "DOI": [
                        "10.1086/653087"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Phillips, Barbara J. and Edward F. McQuarrie (2010), \"Narrative and Persuasion in Fashion Advertising,\" Journal of Consumer Research, 37 (3), 368-392 https://doi.org/10.1086/ 653087.",
                "links": null
            },
            "BIBREF103": {
                "ref_id": "b103",
                "title": "Distant Search, Narrow Attention: How Crowding Alters Organizations' Filtering of Suggestions in Crowdsourcing",
                "authors": [
                    {
                        "first": "Henning",
                        "middle": [],
                        "last": "Piezunka",
                        "suffix": ""
                    },
                    {
                        "first": "Linus",
                        "middle": [],
                        "last": "Dahlander",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Academy of Management Journal",
                "volume": "58",
                "issue": "3",
                "pages": "856--880",
                "other_ids": {
                    "DOI": [
                        "10.5465/amj.2012.0458"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Piezunka, Henning and Linus Dahlander (2015), \"Distant Search, Narrow Attention: How Crowd- ing Alters Organizations' Filtering of Suggestions in Crowdsourcing,\" Academy of Manage- ment Journal, 58 (3), 856-880 https://journals.aom.org/doi/abs/10.5465/amj.2012.0458, publisher: Academy of Management.",
                "links": null
            },
            "BIBREF104": {
                "ref_id": "b104",
                "title": "Language and consumer psychology",
                "authors": [
                    {
                        "first": "Ruth",
                        "middle": [],
                        "last": "Pogacar",
                        "suffix": ""
                    },
                    {
                        "first": "Alican",
                        "middle": [],
                        "last": "Mecit",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "J"
                        ],
                        "last": "Shrum",
                        "suffix": ""
                    },
                    {
                        "first": "Tina",
                        "middle": [
                            "M"
                        ],
                        "last": "Lowrey",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Pogacar, Ruth, Alican Mecit, Fei Gao, L. J. Shrum, and Tina M. Lowrey (2022), \"Language and consumer psychology.,\" ISBN: 1433836424 Publisher: American Psychological Association.",
                "links": null
            },
            "BIBREF105": {
                "ref_id": "b105",
                "title": "GPT is an e\u21b5ective tool for multilingual psychological text analysis",
                "authors": [
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Rathje",
                        "suffix": ""
                    },
                    {
                        "first": "Dan-Mircea",
                        "middle": [],
                        "last": "Mirea",
                        "suffix": ""
                    },
                    {
                        "first": "Ilia",
                        "middle": [],
                        "last": "Sucholutsky",
                        "suffix": ""
                    },
                    {
                        "first": "Raja",
                        "middle": [],
                        "last": "Marjieh",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Robertson",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [
                            "J"
                        ],
                        "last": "Van Bavel",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.31234/osf.io/sekf5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel \"GPT is an e\u21b5ective tool for multilingual psychological text analysis,\" (2023) https: //doi.org/10.31234/osf.io/sekf5.",
                "links": null
            },
            "BIBREF106": {
                "ref_id": "b106",
                "title": "When Impact Appeals Backfire: Evidence from a Multinational Field Experiment and the Lab",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [],
                        "last": "Rei\u21b5",
                        "suffix": ""
                    },
                    {
                        "first": "Hengchen",
                        "middle": [],
                        "last": "Dai",
                        "suffix": ""
                    },
                    {
                        "first": "Jana",
                        "middle": [],
                        "last": "Gallus",
                        "suffix": ""
                    },
                    {
                        "first": "Anita",
                        "middle": [],
                        "last": "Mcclough",
                        "suffix": ""
                    },
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Eitniear",
                        "suffix": ""
                    },
                    {
                        "first": "Michelle",
                        "middle": [],
                        "last": "Slick",
                        "suffix": ""
                    },
                    {
                        "first": "Charlotte",
                        "middle": [],
                        "last": "Blank",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rei\u21b5, Joseph, Hengchen Dai, Jana Gallus, Anita McClough, Steve Eitniear, Michelle Slick, and Charlotte Blank \"When Impact Appeals Backfire: Evidence from a Multinational Field Ex- periment and the Lab,\" (2023) https://papers.ssrn.com/abstract=3946685.",
                "links": null
            },
            "BIBREF107": {
                "ref_id": "b107",
                "title": "psych: Procedures for Psychological, Psychometric, and Personality Research",
                "authors": [
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Revelle",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Revelle, William \"psych: Procedures for Psychological, Psychometric, and Personality Research,\" (2007) https://CRAN.R-project.org/package=psych, institution: Comprehensive R Archive Network Pages: 2.4.3.",
                "links": null
            },
            "BIBREF108": {
                "ref_id": "b108",
                "title": "Negativity drives online news consumption",
                "authors": [
                    {
                        "first": "Claire",
                        "middle": [
                            "E"
                        ],
                        "last": "Robertson",
                        "suffix": ""
                    },
                    {
                        "first": "Nicolas",
                        "middle": [],
                        "last": "Pr\u00f6llochs",
                        "suffix": ""
                    },
                    {
                        "first": "Kaoru",
                        "middle": [],
                        "last": "Schwarzenegger",
                        "suffix": ""
                    },
                    {
                        "first": "Philip",
                        "middle": [],
                        "last": "P\u00e4rnamets",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [
                            "J"
                        ],
                        "last": "Van Bavel",
                        "suffix": ""
                    },
                    {
                        "first": "Stefan",
                        "middle": [],
                        "last": "Feuerriegel",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Nature Human Behaviour",
                "volume": "",
                "issue": "",
                "pages": "1--11",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robertson, Claire E., Nicolas Pr\u00f6llochs, Kaoru Schwarzenegger, Philip P\u00e4rnamets, Jay J. Van Bavel, and Stefan Feuerriegel (2023), \"Negativity drives online news consumption,\" Na- ture Human Behaviour, pages 1-11 https://www.nature.com/articles/s41562-023-01538- 4.",
                "links": null
            },
            "BIBREF109": {
                "ref_id": "b109",
                "title": "A Meta-Analysis of When and How Advertising Creativity Works",
                "authors": [
                    {
                        "first": "Sara",
                        "middle": [],
                        "last": "Rosengren",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Eisend",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Koslow",
                        "suffix": ""
                    },
                    {
                        "first": "Micael",
                        "middle": [],
                        "last": "Dahlen",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Journal of Marketing",
                "volume": "84",
                "issue": "6",
                "pages": "39--56",
                "other_ids": {
                    "DOI": [
                        "10.1177/0022242920929288"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rosengren, Sara, Martin Eisend, Scott Koslow, and Micael Dahlen (2020), \"A Meta-Analysis of When and How Advertising Creativity Works,\" Journal of Marketing, 84 (6), 39-56 https: //doi.org/10.1177/0022242920929288, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF110": {
                "ref_id": "b110",
                "title": "Choosing experiments to accelerate collective discovery",
                "authors": [
                    {
                        "first": "Andrey",
                        "middle": [],
                        "last": "Rzhetsky",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [
                            "G"
                        ],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [
                            "T"
                        ],
                        "last": "Foster",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "A"
                        ],
                        "last": "Evans",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the National Academy of Sciences",
                "volume": "112",
                "issue": "47",
                "pages": "14569--14574",
                "other_ids": {
                    "DOI": [
                        "10.1073/pnas.1509757112"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rzhetsky, Andrey, Jacob G. Foster, Ian T. Foster, and James A. Evans (2015), \"Choosing exper- iments to accelerate collective discovery,\" Proceedings of the National Academy of Sciences, 112 (47), 14569-14574 https://www.pnas.org/doi/abs/10.1073/pnas.1509757112.",
                "links": null
            },
            "BIBREF111": {
                "ref_id": "b111",
                "title": "The Architecture of Economic Systems: Hierarchies and Polyarchies",
                "authors": [
                    {
                        "first": "Raaj",
                        "middle": [],
                        "last": "Sah",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [
                            "E"
                        ],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Stiglitz",
                        "suffix": ""
                    }
                ],
                "year": 1986,
                "venue": "The American Economic Review",
                "volume": "76",
                "issue": "4",
                "pages": "716--727",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sah, Raaj Kumar and Joseph E. Stiglitz (1986), \"The Architecture of Economic Systems: Hi- erarchies and Polyarchies,\" The American Economic Review, 76 (4), 716-727 https: //www.jstor.org/stable/1806069, publisher: American Economic Association.",
                "links": null
            },
            "BIBREF112": {
                "ref_id": "b112",
                "title": "The empirical benefits of conceptual rigor: Systematic articulation of conceptual hypotheses can reduce the risk of non-replicable results (and facilitate novel discoveries too)",
                "authors": [
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Schaller",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Journal of Experimental Social Psychology",
                "volume": "66",
                "issue": "",
                "pages": "107--115",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Schaller, Mark (2016), \"The empirical benefits of conceptual rigor: Systematic articulation of conceptual hypotheses can reduce the risk of non-replicable results (and facilitate novel discoveries too),\" Journal of Experimental Social Psychology, 66, 107-115 https:// www.sciencedirect.com/science/article/pii/S0022103115001092.",
                "links": null
            },
            "BIBREF113": {
                "ref_id": "b113",
                "title": "Using Machine Learning to Generate Novel Hypotheses: Increasing Optimism About COVID-19 Makes People Less Willing to Justify Unethical Behaviors",
                "authors": [
                    {
                        "first": "Abhishek",
                        "middle": [],
                        "last": "Sheetal",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyu",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Krishna",
                        "middle": [],
                        "last": "Savani",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Psychological Science",
                "volume": "31",
                "issue": "10",
                "pages": "1222--1235",
                "other_ids": {
                    "DOI": [
                        "10.1177/0956797620959594"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sheetal, Abhishek, Zhiyu Feng, and Krishna Savani (2020), \"Using Machine Learning to Generate Novel Hypotheses: Increasing Optimism About COVID-19 Makes People Less Willing to Justify Unethical Behaviors,\" Psychological Science, 31 (10), 1222-1235 https://doi.org/ 10.1177/0956797620959594.",
                "links": null
            },
            "BIBREF114": {
                "ref_id": "b114",
                "title": "Superhuman artificial intelligence can improve human decision-making by increasing novelty",
                "authors": [
                    {
                        "first": "Minkyu",
                        "middle": [],
                        "last": "Shin",
                        "suffix": ""
                    },
                    {
                        "first": "Jin",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Bas",
                        "middle": [],
                        "last": "Van Opheusden",
                        "suffix": ""
                    },
                    {
                        "first": "Thomas",
                        "middle": [
                            "L"
                        ],
                        "last": "Gri",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the National Academy of Sciences",
                "volume": "120",
                "issue": "12",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1073/pnas.2214840120"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shin, Minkyu, Jin Kim, Bas van Opheusden, and Thomas L. Gri ths (2023), \"Superhuman artificial intelligence can improve human decision-making by increasing novelty,\" Proceedings of the National Academy of Sciences, 120 (12), e2214840120 https://www.pnas.org/doi/ full/10.1073/pnas.2214840120, publisher: Proceedings of the National Academy of Sciences.",
                "links": null
            },
            "BIBREF115": {
                "ref_id": "b115",
                "title": "Reading dies in complexity: Online news consumers prefer simple writing",
                "authors": [
                    {
                        "first": "Hillary",
                        "middle": [
                            "C"
                        ],
                        "last": "Shulman",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "David",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Markowitz",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Rogers",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Science Advances",
                "volume": "10",
                "issue": "23",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1126/sciadv.adn2555"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shulman, Hillary C., David M. Markowitz, and Todd Rogers (2024), \"Reading dies in complexity: Online news consumers prefer simple writing,\" Science Advances, 10 (23), eadn2555 https: //www.science.org/doi/10.1126/sciadv.adn2555, publisher: American Association for the Advancement of Science.",
                "links": null
            },
            "BIBREF116": {
                "ref_id": "b116",
                "title": "Temporarily Divide to Conquer: Centralized, Decentralized, and Reintegrated Organizational Approaches to Exploration and Adaptation",
                "authors": [
                    {
                        "first": "Nicolaj",
                        "middle": [],
                        "last": "Siggelkow",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "A"
                        ],
                        "last": "Levinthal",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Organization Science",
                "volume": "14",
                "issue": "6",
                "pages": "650--669",
                "other_ids": {
                    "DOI": [
                        "10.1287/orsc.14.6.650.24840"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Siggelkow, Nicolaj and Daniel A. Levinthal (2003), \"Temporarily Divide to Conquer: Centralized, Decentralized, and Reintegrated Organizational Approaches to Exploration and Adaptation,\" Organization Science, 14 (6), 650-669 https://pubsonline.informs.org/doi/abs/10.1287/ orsc.14.6.650.24840, publisher: INFORMS.",
                "links": null
            },
            "BIBREF117": {
                "ref_id": "b117",
                "title": "Pre-registration: Why and How",
                "authors": [
                    {
                        "first": "Joseph",
                        "middle": [
                            "P"
                        ],
                        "last": "Simmons",
                        "suffix": ""
                    },
                    {
                        "first": "Leif",
                        "middle": [
                            "D"
                        ],
                        "last": "Nelson",
                        "suffix": ""
                    },
                    {
                        "first": "Uri",
                        "middle": [],
                        "last": "Simonsohn",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Journal of Consumer Psychology",
                "volume": "31",
                "issue": "1",
                "pages": "151--162",
                "other_ids": {
                    "DOI": [
                        "10.1002/jcpy.1208"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn (2021), \"Pre-registration: Why and How,\" Journal of Consumer Psychology, 31 (1), 151-162 https://onlinelibrary.wiley.com/doi/ abs/10.1002/jcpy.1208.",
                "links": null
            },
            "BIBREF118": {
                "ref_id": "b118",
                "title": "Specification curve analysis",
                "authors": [
                    {
                        "first": "Uri",
                        "middle": [],
                        "last": "Simonsohn",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [
                            "P"
                        ],
                        "last": "Simmons",
                        "suffix": ""
                    },
                    {
                        "first": "Leif",
                        "middle": [
                            "D"
                        ],
                        "last": "Nelson",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Nature Human Behaviour",
                "volume": "4",
                "issue": "11",
                "pages": "1208--1214",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Simonsohn, Uri, Joseph P. Simmons, and Leif D. Nelson (2020), \"Specification curve analysis,\" Nature Human Behaviour, 4 (11), 1208-1214 https://www.nature.com/articles/s41562- 020-0912-z, publisher: Nature Publishing Group.",
                "links": null
            },
            "BIBREF119": {
                "ref_id": "b119",
                "title": "MPNet: Masked and Permuted Pre-training for Language Understanding",
                "authors": [
                    {
                        "first": "Kaitao",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Xu",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    },
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Qin",
                        "suffix": ""
                    },
                    {
                        "first": "Jianfeng",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Tie-Yan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Song, Kaitao, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu \"MPNet: Masked and Permuted Pre-training for Language Understanding,\" (2020) https://www.microsoft.com/en-us/ research/publication/mpnet-masked-and-permuted-pre-training-for-language- understanding/.",
                "links": null
            },
            "BIBREF120": {
                "ref_id": "b120",
                "title": "Accelerating science with human-aware artificial intelligence",
                "authors": [
                    {
                        "first": "Jamshid",
                        "middle": [],
                        "last": "Sourati",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "A"
                        ],
                        "last": "Evans",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Nature Human Behaviour",
                "volume": "7",
                "issue": "10",
                "pages": "1682--1696",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sourati, Jamshid and James A. Evans (2023), \"Accelerating science with human-aware artifi- cial intelligence,\" Nature Human Behaviour, 7 (10), 1682-1696 https://www.nature.com/ articles/s41562-023-01648-z, number: 10 Publisher: Nature Publishing Group.",
                "links": null
            },
            "BIBREF121": {
                "ref_id": "b121",
                "title": "Overlapping experiment infrastructure: more, better, faster experimentation",
                "authors": [
                    {
                        "first": "Diane",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Ashish",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Deirdre",
                        "middle": [
                            "O"
                        ],
                        "last": "'brien",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Meyer",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,\" KDD '10",
                "volume": "",
                "issue": "",
                "pages": "17--26",
                "other_ids": {
                    "DOI": [
                        "10.1145/1835804.1835810"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tang, Diane, Ashish Agarwal, Deirdre O'Brien, and Mike Meyer \"Overlapping experiment infras- tructure: more, better, faster experimentation,\" \"Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,\" KDD '10, pages 17-26, New York, NY, USA: Association for Computing Machinery (2010) https://dl.acm.org/ doi/10.1145/1835804.1835810.",
                "links": null
            },
            "BIBREF122": {
                "ref_id": "b122",
                "title": "The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods",
                "authors": [
                    {
                        "first": "Yla",
                        "middle": [
                            "R"
                        ],
                        "last": "Tausczik",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "W"
                        ],
                        "last": "Pennebaker",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Journal of Language and Social Psychology",
                "volume": "29",
                "issue": "1",
                "pages": "24--54",
                "other_ids": {
                    "DOI": [
                        "10.1177/0261927X09351676"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tausczik, Yla R. and James W. Pennebaker (2010), \"The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods,\" Journal of Language and Social Psychology, 29 (1), 24-54 https://doi.org/10.1177/0261927X09351676.",
                "links": null
            },
            "BIBREF123": {
                "ref_id": "b123",
                "title": "Nudge: improving decisions about health, wealth, and happiness",
                "authors": [
                    {
                        "first": "Richard",
                        "middle": [
                            "H"
                        ],
                        "last": "Thaler",
                        "suffix": ""
                    },
                    {
                        "first": "Cass",
                        "middle": [
                            "R"
                        ],
                        "last": "Sunstein",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Thaler, Richard H. and Cass R. Sunstein (2009), Nudge: improving decisions about health, wealth, and happiness New York: Penguin Books.",
                "links": null
            },
            "BIBREF124": {
                "ref_id": "b124",
                "title": "Adaptive Idea Screening Using Consumers",
                "authors": [
                    {
                        "first": "Olivier",
                        "middle": [],
                        "last": "Toubia",
                        "suffix": ""
                    },
                    {
                        "first": "Laurent",
                        "middle": [],
                        "last": "Flor\u00e8s",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Marketing Science",
                "volume": "26",
                "issue": "3",
                "pages": "342--360",
                "other_ids": {
                    "DOI": [
                        "10.1287/mksc.1070.0273"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Toubia, Olivier and Laurent Flor\u00e8s (2007), \"Adaptive Idea Screening Using Consumers,\" Marketing Science, 26 (3), 342-360 https://pubsonline.informs.org/doi/abs/10.1287/ mksc.1070.0273, publisher: INFORMS.",
                "links": null
            },
            "BIBREF125": {
                "ref_id": "b125",
                "title": "Idea generation, creativity, and prototypicality",
                "authors": [
                    {
                        "first": "Olivier",
                        "middle": [],
                        "last": "Toubia",
                        "suffix": ""
                    },
                    {
                        "first": "Oded",
                        "middle": [],
                        "last": "Netzer",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Marketing science",
                "volume": "36",
                "issue": "1",
                "pages": "1--20",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Toubia, Olivier and Oded Netzer (2017), \"Idea generation, creativity, and prototypicality,\" Mar- keting science, 36 (1), 1-20 ISBN: 0732-2399 Publisher: INFORMS.",
                "links": null
            },
            "BIBREF126": {
                "ref_id": "b126",
                "title": "Supercharging A/B Testing at Uber",
                "authors": [
                    {
                        "first": "Uber",
                        "middle": [],
                        "last": "Engineering",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Uber Engineering \"Supercharging A/B Testing at Uber,\" (2022) https://www.uber.com/blog/ supercharging-a-b-testing-at-uber/.",
                "links": null
            },
            "BIBREF127": {
                "ref_id": "b127",
                "title": "Taking the Full Measure: Integrating Replication into Research Practice to Assess Generalizability",
                "authors": [
                    {
                        "first": "Oleg",
                        "middle": [],
                        "last": "Urminsky",
                        "suffix": ""
                    },
                    {
                        "first": "Berkeley",
                        "middle": [
                            "J"
                        ],
                        "last": "Dietvorst",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "Journal of Consumer Research",
                "volume": "51",
                "issue": "1",
                "pages": "157--168",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucae007"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Urminsky, Oleg and Berkeley J Dietvorst (2024), \"Taking the Full Measure: Integrating Replication into Research Practice to Assess Generalizability,\" Journal of Consumer Research, 51 (1), 157-168 https://doi.org/10.1093/jcr/ucae007.",
                "links": null
            },
            "BIBREF128": {
                "ref_id": "b128",
                "title": "Sendhil Mullainathan, and Ashesh Rambachan \"Evaluating the World Model Implicit in a Generative Model",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Vafa",
                        "suffix": ""
                    },
                    {
                        "first": "Justin",
                        "middle": [
                            "Y"
                        ],
                        "last": "Keyon",
                        "suffix": ""
                    },
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Kleinberg",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2406.03689"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Vafa, Keyon, Justin Y. Chen, Jon Kleinberg, Sendhil Mullainathan, and Ashesh Rambachan \"Eval- uating the World Model Implicit in a Generative Model,\" (2024) http://arxiv.org/abs/ 2406.03689, arXiv:2406.03689 [cs].",
                "links": null
            },
            "BIBREF129": {
                "ref_id": "b129",
                "title": "How Many Creative Alternatives to Generate?",
                "authors": [
                    {
                        "first": "Vanden",
                        "middle": [],
                        "last": "Bergh",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Bruce",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Leonard",
                        "suffix": ""
                    },
                    {
                        "first": "Gerald",
                        "middle": [
                            "A"
                        ],
                        "last": "Reid",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Schorin",
                        "suffix": ""
                    }
                ],
                "year": 1983,
                "venue": "Journal of Advertising",
                "volume": "12",
                "issue": "4",
                "pages": "46--49",
                "other_ids": {
                    "DOI": [
                        "10.1080/00913367.1983.10672863"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Vanden Bergh, Bruce G., Leonard N. Reid, and Gerald A. Schorin (1983), \"How Many Creative Alternatives to Generate?,\" Journal of Advertising, 12 (4), 46- 49 https://doi.org/10.1080/00913367.1983.10672863, publisher: Routledge eprint: https://doi.org/10.1080/00913367.1983.10672863.",
                "links": null
            },
            "BIBREF130": {
                "ref_id": "b130",
                "title": "Scientific discovery in the age of artificial intelligence",
                "authors": [
                    {
                        "first": "Hanchen",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianfan",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuanqi",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhao",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Kexin",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Ziming",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Payal",
                        "middle": [],
                        "last": "Chandak",
                        "suffix": ""
                    },
                    {
                        "first": "Shengchao",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Van Katwyk",
                        "suffix": ""
                    },
                    {
                        "first": "Andreea",
                        "middle": [],
                        "last": "Deac",
                        "suffix": ""
                    },
                    {
                        "first": "Anima",
                        "middle": [],
                        "last": "Anandkumar",
                        "suffix": ""
                    },
                    {
                        "first": "Karianne",
                        "middle": [],
                        "last": "Bergen",
                        "suffix": ""
                    },
                    {
                        "first": "Carla",
                        "middle": [
                            "P"
                        ],
                        "last": "Gomes",
                        "suffix": ""
                    },
                    {
                        "first": "Shirley",
                        "middle": [],
                        "last": "Ho",
                        "suffix": ""
                    },
                    {
                        "first": "Pushmeet",
                        "middle": [],
                        "last": "Kohli",
                        "suffix": ""
                    },
                    {
                        "first": "Joan",
                        "middle": [],
                        "last": "Lasenby",
                        "suffix": ""
                    },
                    {
                        "first": "Jure",
                        "middle": [],
                        "last": "Leskovec",
                        "suffix": ""
                    },
                    {
                        "first": "Tie-Yan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Arjun",
                        "middle": [],
                        "last": "Manrai",
                        "suffix": ""
                    },
                    {
                        "first": "Debora",
                        "middle": [],
                        "last": "Marks",
                        "suffix": ""
                    },
                    {
                        "first": "Bharath",
                        "middle": [],
                        "last": "Ramsundar",
                        "suffix": ""
                    },
                    {
                        "first": "Le",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Jimeng",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Petar",
                        "middle": [],
                        "last": "Veli\u010dkovi\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Welling",
                        "suffix": ""
                    },
                    {
                        "first": "Linfeng",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Connor",
                        "middle": [
                            "W"
                        ],
                        "last": "Coley",
                        "suffix": ""
                    },
                    {
                        "first": "Yoshua",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Marinka",
                        "middle": [],
                        "last": "Zitnik",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Nature",
                "volume": "620",
                "issue": "7972",
                "pages": "47--60",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wang, Hanchen, Tianfan Fu, Yuanqi Du, Wenhao Gao, Kexin Huang, Ziming Liu, Payal Chandak, Shengchao Liu, Peter Van Katwyk, Andreea Deac, Anima Anandkumar, Karianne Bergen, Carla P. Gomes, Shirley Ho, Pushmeet Kohli, Joan Lasenby, Jure Leskovec, Tie-Yan Liu, Arjun Manrai, Debora Marks, Bharath Ramsundar, Le Song, Jimeng Sun, Jian Tang, Petar Veli\u010dkovi\u0107, Max Welling, Linfeng Zhang, Connor W. Coley, Yoshua Bengio, and Marinka Zitnik (2023), \"Scientific discovery in the age of artificial intelligence,\" Nature, 620 (7972), 47- 60 https://www.nature.com/articles/s41586-023-06221-2, publisher: Nature Publishing Group.",
                "links": null
            },
            "BIBREF131": {
                "ref_id": "b131",
                "title": "Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking",
                "authors": [
                    {
                        "first": "Jelte",
                        "middle": [
                            "M"
                        ],
                        "last": "Wicherts",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "S"
                        ],
                        "last": "Coosje",
                        "suffix": ""
                    },
                    {
                        "first": "Hilde",
                        "middle": [
                            "E M"
                        ],
                        "last": "Veldkamp",
                        "suffix": ""
                    },
                    {
                        "first": "Marjan",
                        "middle": [],
                        "last": "Augusteijn",
                        "suffix": ""
                    },
                    {
                        "first": "Robbie",
                        "middle": [
                            "C M"
                        ],
                        "last": "Bakker",
                        "suffix": ""
                    },
                    {
                        "first": "Marcel",
                        "middle": [
                            "A L M"
                        ],
                        "last": "Van Aert",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Van Assen",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Frontiers in Psychology",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.3389/fpsyg.2016.01832/full"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn, Marjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen (2016), \"Degrees of Freedom in Planning, Running, Analyzing, and Reporting Psychological Studies: A Checklist to Avoid p-Hacking,\" Frontiers in Psychology, 7 https://www.frontiersin.org/journals/psychology/articles/10.3389/ fpsyg.2016.01832/full, publisher: Frontiers.",
                "links": null
            },
            "BIBREF132": {
                "ref_id": "b132",
                "title": "Large teams develop and small teams disrupt science and technology",
                "authors": [
                    {
                        "first": "Lingfei",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Dashun",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "A"
                        ],
                        "last": "Evans",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Nature",
                "volume": "566",
                "issue": "7744",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wu, Lingfei, Dashun Wang, and James A. Evans (2019), \"Large teams develop and small teams disrupt science and technology,\" Nature, 566 (7744), 378-382 https://www.nature.com/ articles/s41586-019-0941-9, number: 7744 Publisher: Nature Publishing Group.",
                "links": null
            },
            "BIBREF133": {
                "ref_id": "b133",
                "title": "Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet)",
                "authors": [
                    {
                        "first": "Eunice",
                        "middle": [],
                        "last": "Yiu",
                        "suffix": ""
                    },
                    {
                        "first": "Eliza",
                        "middle": [],
                        "last": "Kosoy",
                        "suffix": ""
                    },
                    {
                        "first": "Alison",
                        "middle": [],
                        "last": "Gopnik",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Perspectives on Psychological Science",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1177/17456916231201401"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yiu, Eunice, Eliza Kosoy, and Alison Gopnik (2023), \"Transmission Versus Truth, Imitation Versus Innovation: What Children Can Do That Large Language and Language-and-Vision Models Cannot (Yet),\" Perspectives on Psychological Science, page 17456916231201401 https:// doi.org/10.1177/17456916231201401, publisher: SAGE Publications Inc.",
                "links": null
            },
            "BIBREF134": {
                "ref_id": "b134",
                "title": "Hypothesis Generation with Large Language Models",
                "authors": [
                    {
                        "first": "Yangqiaoyu",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Haokun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Tejes",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "Hongyuan",
                        "middle": [],
                        "last": "Mei",
                        "suffix": ""
                    },
                    {
                        "first": "Chenhao",
                        "middle": [],
                        "last": "Tan",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2404.04326"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhou, Yangqiaoyu, Haokun Liu, Tejes Srivastava, Hongyuan Mei, and Chenhao Tan \"Hypoth- esis Generation with Large Language Models,\" (2024) http://arxiv.org/abs/2404.04326, arXiv:2404.04326 [cs].",
                "links": null
            },
            "BIBREF135": {
                "ref_id": "b135",
                "title": "Tweets We Like Aren't Alike: Time of Day A\u21b5ects Engagement with Vice and Virtue Tweets",
                "authors": [
                    {
                        "first": "Ozum",
                        "middle": [],
                        "last": "Zor",
                        "suffix": ""
                    },
                    {
                        "first": "Kihyun",
                        "middle": [],
                        "last": "Hannah Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Ashwani",
                        "middle": [],
                        "last": "Monga",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Journal of Consumer Research",
                "volume": "49",
                "issue": "3",
                "pages": "473--495",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucab072"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zor, Ozum, Kihyun Hannah Kim, and Ashwani Monga (2022), \"Tweets We Like Aren't Alike: Time of Day A\u21b5ects Engagement with Vice and Virtue Tweets,\" Journal of Consumer Research, 49 (3), 473-495 https://doi.org/10.1093/jcr/ucab072. REFERENCES",
                "links": null
            },
            "BIBREF136": {
                "ref_id": "b136",
                "title": "Pervasive randomization problems, here with headline experiments",
                "authors": [
                    {
                        "first": "Dean",
                        "middle": [],
                        "last": "Eckles",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eckles, Dean \"Pervasive randomization problems, here with headline experiments,\" (2024) https://statmodeling.stat.columbia.edu/2024/06/20/pervasive-randomization- problems-here-with-headline-experiments/.",
                "links": null
            },
            "BIBREF137": {
                "ref_id": "b137",
                "title": "Why Many Behavioral Interventions Have Unpredictable E\u21b5ects in the Wild: The Conflicting Consequences Problem",
                "authors": [
                    {
                        "first": "Indranil",
                        "middle": [],
                        "last": "Goswami",
                        "suffix": ""
                    },
                    {
                        "first": "Oleg",
                        "middle": [],
                        "last": "Urminsky",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Goswami, Indranil and Oleg Urminsky \"Why Many Behavioral Interventions Have Unpre- dictable E\u21b5ects in the Wild: The Conflicting Consequences Problem,\" (2022) https: //papers.ssrn.com/abstract=4199453.",
                "links": null
            },
            "BIBREF138": {
                "ref_id": "b138",
                "title": "Automated Text Analysis for Consumer Research",
                "authors": [
                    {
                        "first": "Ashlee",
                        "middle": [],
                        "last": "Humphreys",
                        "suffix": ""
                    },
                    {
                        "first": "Rebecca",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Jen-Hui",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Journal of Consumer Research",
                "volume": "44",
                "issue": "6",
                "pages": "1274--1306",
                "other_ids": {
                    "DOI": [
                        "10.1093/jcr/ucx104"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Humphreys, Ashlee and Rebecca Jen-Hui Wang (2018), \"Automated Text Analysis for Consumer Research,\" Journal of Consumer Research, 44 (6), 1274-1306 https://doi.org/10.1093/ jcr/ucx104.",
                "links": null
            },
            "BIBREF139": {
                "ref_id": "b139",
                "title": "How Apple's Mail Privacy Changes A\u21b5ect Email Open Tracking",
                "authors": [
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Kaczanowski",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaczanowski, Rob \"How Apple's Mail Privacy Changes A\u21b5ect Email Open Tracking,\" (2021) https://postmarkapp.com/blog/how-apples-mail-privacy-changes-affect- email-open-tracking.",
                "links": null
            },
            "BIBREF140": {
                "ref_id": "b140",
                "title": "Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses",
                "authors": [
                    {
                        "first": "Dani\u00ebl",
                        "middle": [],
                        "last": "Lakens",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Social Psychological and Personality Science",
                "volume": "8",
                "issue": "4",
                "pages": "355--362",
                "other_ids": {
                    "DOI": [
                        "10.1177/1948550617697177"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Lakens, Dani\u00ebl (2017), \"Equivalence Tests: A Practical Primer for t Tests, Correlations, and Meta-Analyses,\" Social Psychological and Personality Science, 8 (4), 355-362 http: //journals.sagepub.com/doi/10.1177/1948550617697177.",
                "links": null
            },
            "BIBREF141": {
                "ref_id": "b141",
                "title": "Three Ways Apple's Privacy Changes Will Impact Your Business",
                "authors": [
                    {
                        "first": "Clate",
                        "middle": [],
                        "last": "Mask",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mask, Clate (2021), \"Three Ways Apple's Privacy Changes Will Impact Your Busi- ness,\" Forbes https://www.forbes.com/sites/forbestechcouncil/2021/10/12/three- ways-apples-privacy-changes-will-impact-your-business/, section: Innovation.",
                "links": null
            },
            "BIBREF142": {
                "ref_id": "b142",
                "title": "The Upworthy Research Archive, a time series of 32,487 experiments in U.S. media",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Matias",
                        "suffix": ""
                    },
                    {
                        "first": "Kevin",
                        "middle": [],
                        "last": "Nathan",
                        "suffix": ""
                    },
                    {
                        "first": "Marianne",
                        "middle": [],
                        "last": "Munger",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Aubin Le Quere",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ebersole",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "8",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Matias, J. Nathan, Kevin Munger, Marianne Aubin Le Quere, and Charles Ebersole (2021), \"The Upworthy Research Archive, a time series of 32,487 experiments in U.S. media,\" Scientific Data, 8 (1), 195 https://www.nature.com/articles/s41597-021-00934-7.",
                "links": null
            },
            "BIBREF143": {
                "ref_id": "b143",
                "title": "GPT is an e\u21b5ective tool for multilingual psychological text analysis",
                "authors": [
                    {
                        "first": "Steve",
                        "middle": [],
                        "last": "Rathje",
                        "suffix": ""
                    },
                    {
                        "first": "Dan-Mircea",
                        "middle": [],
                        "last": "Mirea",
                        "suffix": ""
                    },
                    {
                        "first": "Ilia",
                        "middle": [],
                        "last": "Sucholutsky",
                        "suffix": ""
                    },
                    {
                        "first": "Raja",
                        "middle": [],
                        "last": "Marjieh",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Robertson",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [
                            "J"
                        ],
                        "last": "Van Bavel",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.31234/osf.io/sekf5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rathje, Steve, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J. Van Bavel \"GPT is an e\u21b5ective tool for multilingual psychological text analysis,\" (2023) https: //doi.org/10.31234/osf.io/sekf5.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "21 A trial with three arms, {A, B, C} would have six pairs, A-B, A-C, B-C, B-A, C-A, C-B. However, for nearly all our analyses, we randomly drew at most one pair per trial.",
                "fig_num": null
            },
            "FIGREF1": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "without a formal theory to connect them. Science requires both(Mortensen and Cialdini 2010;Alba 2012;Lynch et al. 2012) and, in fact, in marketing, both approaches are regularly usedJaniszewski and van Osselaer (2021). The framework presented here adds to the toolkit of data-driven approaches. At the same time, the transparency of the outputs leaves room for researchers to search through the set with an eye for theoretically relevant insights.",
                "fig_num": null
            },
            "FIGREF2": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 1: Overview of steps for generating and selecting hypotheses",
                "fig_num": "12"
            },
            "FIGREF3": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 3: Mean change in CTR for headlines conditional on whether they had the larger or smaller labelled feature value within a pair, as a fraction of the average CTR of the lower-rated group. Bands represent 95% confidence intervals for the change.",
                "fig_num": "3"
            },
            "FIGREF4": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 4: Coe cient estimates for all hypotheses, across two datasets, where the outcome is CTR for Headlines and CTR for Social Media. Values shown are pulled from regressions, where a one unit change in variable corresponds to a b -unit change in the standard deviation of CTR for the respective dataset.",
                "fig_num": "4"
            },
            "FIGREF5": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Assume you are a copywriter for an online news platform . Here are some examples of recent headlines from your company : rewrite Headline A below according to the given instructions . Keep the content of the story as similar as possible . Respond by writing out Headline B.",
                "fig_num": null
            },
            "FIGREF6": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 1: Human ratings for GPT-Generated Hypotheses.",
                "fig_num": "1"
            },
            "FIGREF7": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 2: Average Euclidean distance between pairs.",
                "fig_num": "2"
            },
            "FIGREF8": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 3: Coe cients tend to increase as a function of average PTE. Error bars show 2 standard errors of the coe cient values within each stratum.",
                "fig_num": "3"
            },
            "FIGREF9": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 4: The significance of predictors increases as a function of average PTE. Error bars show 2 standard errors of the shown proportion within each stratum.",
                "fig_num": "4"
            },
            "FIGREF10": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 5: The coe cients for a range of outcomes and hypothesized features. Shown is the coe cient estimates with 95% confidence intervals for each outcome and feature combination.",
                "fig_num": "5"
            },
            "FIGREF11": {
                "num": null,
                "type_str": "figure",
                "uris": null,
                "text": "Figure 6: Most features have a non-significant relationship to most outcomes of interest.Shown is the coe cient estimates with 95% confidence intervals for each outcome and feature combination.",
                "fig_num": "6"
            },
            "TABREF3": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td/><td/><td>Splits</td><td/><td/></tr><tr><td/><td colspan=\"4\">Train Morph Regression Lock-Box</td><td>Total</td></tr><tr><td>Headline-Level</td><td/><td/><td/><td/><td/></tr><tr><td>Total Headlines</td><td>36173</td><td>9434</td><td>8779</td><td>36866</td><td>91252</td></tr><tr><td>Unique Headlines</td><td>25759</td><td>6673</td><td>6282</td><td>26324</td><td>64958</td></tr><tr><td>Pair-Level</td><td/><td/><td/><td/><td/></tr><tr><td>Total Pairs</td><td colspan=\"2\">112350 29600</td><td>27206</td><td>112998</td><td>282154</td></tr><tr><td>Unique Pairs</td><td colspan=\"2\">56175 14800</td><td>13603</td><td>56499</td><td>141077</td></tr><tr><td>Unique Headlines</td><td>25520</td><td>6612</td><td>6220</td><td>26084</td><td>64377</td></tr><tr><td>Trial-Level</td><td/><td/><td/><td/><td/></tr><tr><td>Total Trials</td><td>12800</td><td>3366</td><td>3136</td><td>13185</td><td>32487</td></tr><tr><td>Total Components</td><td>3438</td><td>869</td><td>837</td><td>3609</td><td>8753</td></tr><tr><td>Average # of Headlines</td><td>2.83</td><td>2.80</td><td>2.80</td><td>2.80</td><td>2.81</td></tr></table>",
                "text": "Counts for Headline Data"
            },
            "TABREF4": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Mean</td><td>SD</td><td>Median</td></tr></table>",
                "text": "Summary Statistics for Headline Data"
            },
            "TABREF5": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td>Adj R 2</td><td/><td colspan=\"2\">Binary accuracy</td><td colspan=\"2\">Binary AUC</td></tr><tr><td>Baseline features</td><td colspan=\"2\">No ML Plus ML</td><td colspan=\"2\">No ML Plus ML</td><td colspan=\"2\">No ML Plus ML</td></tr><tr><td>B&amp;U</td><td>0.042</td><td>0.133</td><td>0.569</td><td>0.636</td><td>0.596</td><td>0.688</td></tr><tr><td>B&amp;U (non-linear)</td><td>0.041</td><td>0.134</td><td>0.564</td><td>0.639</td><td>0.591</td><td>0.688</td></tr><tr><td>Human guess</td><td>0.008</td><td>0.134</td><td>0.530</td><td>0.632</td><td>0.550</td><td>0.690</td></tr><tr><td>B&amp;U + Human guess</td><td>0.049</td><td>0.136</td><td>0.568</td><td>0.629</td><td>0.608</td><td>0.692</td></tr><tr><td>ML only</td><td>-</td><td>0.130</td><td>-</td><td>0.639</td><td>-</td><td>0.687</td></tr></table>",
                "text": "Out-of-sample regression performances with and without ML model predictions included as a predictor"
            },
            "TABREF6": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Headline A</td><td>Headline B</td><td>Hypothesis</td></tr><tr><td>A Holocaust Survivor's Com-</td><td>A 90-Second Message From A</td><td>Specifying the length of con-</td></tr><tr><td>passionate Message To The</td><td>90-Year-Old Holocaust Sur-</td><td>tent in the headline results in</td></tr><tr><td>German Population</td><td>vivor</td><td>more engagement with a mes-</td></tr><tr><td/><td/><td>sage</td></tr><tr><td>These Kids Don't Pass Go</td><td>Behind These Numbers Sit</td><td>Incorporating emotional lan-</td></tr><tr><td>And They Don't Collect $200.</td><td>Really Sad Truths About Our</td><td>guage results in more engage-</td></tr><tr><td/><td>Justice System -And Some</td><td>ment with a message.</td></tr><tr><td/><td>Really Young People</td><td/></tr><tr><td>It's Probably Your 2nd Fa-</td><td>If You Think It Feels Great,</td><td>Framing a message to high-</td></tr><tr><td>vorite Thing To Do And Now</td><td>You Should See What Else</td><td>light unexpected benefits in-</td></tr><tr><td>Science Wants You To Do</td><td>It's Doing To You</td><td>creases engagement with a</td></tr><tr><td>More Of It</td><td/><td>message.</td></tr><tr><td>I Used To Think Adaptation</td><td>Baby Polar Bear: 'What Use</td><td>Personifying animals in the</td></tr><tr><td>Was A Good Thing Until I</td><td>Is All This Fur If There's No</td><td>messaging a\u21b5ects engagement</td></tr><tr><td>Realized How Humans Do It</td><td>Ice?' Mama Bear: 'Hush Up</td><td>with a message.</td></tr><tr><td/><td>And Adapt'.</td><td/></tr><tr><td>She Wanted To Make Sure</td><td>She Wants Everyone To</td><td>Using past tense instead of</td></tr><tr><td>Everyone Knew That Her</td><td>Know That She's A Proud</td><td>present tense decreases en-</td></tr><tr><td>Baby Was A Boy. So She</td><td>Mother Of A Boy, So She</td><td>gagement with a message.</td></tr><tr><td>Dressed Him In Pink.</td><td>Dresses Him In Pink</td><td/></tr><tr><td>Elizabeth Warren Forced To</td><td>Elizabeth Warren Teaches A</td><td>Using a condescending tone</td></tr><tr><td>Lecture Bank Regulator Like</td><td>Bank Regulator How To Do</td><td>decreases engagement with a</td></tr><tr><td>He's A Child Who Did Some-</td><td>His Job Like A Big Boy</td><td>message.</td></tr><tr><td>thing Awful</td><td/><td/></tr><tr><td colspan=\"3\">Note: To view more examples, visit https://bit.ly/jmp-hyp-samp. Complete set available on</td></tr><tr><td>OSF.</td><td/><td/></tr></table>",
                "text": "Examples of sampled headline pairs and generated hypotheses"
            },
            "TABREF7": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Hypothesis</td><td>Original Headline</td><td colspan=\"2\">Morphed Headline</td></tr><tr><td>Incorporating emotional trig-</td><td>That Cheap Stu\u21b5 I Just</td><td colspan=\"2\">Local Man's Walmart Bar-</td></tr><tr><td>gers and a geographic refer-</td><td>Bought At Walmart? Turns</td><td colspan=\"2\">gain Turns Nightmare: Hid-</td></tr><tr><td>ence into a headline a\u21b5ects</td><td>Out, It Cost Me $6000 More</td><td colspan=\"2\">den Costs Rack Up $6000!</td></tr><tr><td>engagement with a message.</td><td>Than I Thought</td><td/><td/></tr><tr><td>Personalizing a message by</td><td>11 Tweets That Sum Up The</td><td colspan=\"2\">North Carolina Resident's</td></tr><tr><td>focusing on an individual's</td><td>Horror In North Carolina</td><td>Heart-Wrenching</td><td>Reaction</td></tr><tr><td>story or reaction makes peo-</td><td/><td colspan=\"2\">Captures the Horror in 11</td></tr><tr><td>ple more likely to engage with</td><td/><td>Tweets</td><td/></tr><tr><td>a message.</td><td/><td/><td/></tr><tr><td>Excessive sensationalism and</td><td>An 11-year old ate a burger</td><td colspan=\"2\">11-Year-Old's Fatal Reaction</td></tr><tr><td>vague phrasing leads to less</td><td>with a surprise ingredient. It</td><td colspan=\"2\">to FDA-Approved Burger In-</td></tr><tr><td>engagement with a message.</td><td>was fatal, but ok according to</td><td colspan=\"2\">gredient Sparks Outrage</td></tr><tr><td/><td>the FDA.</td><td/><td/></tr><tr><td>Introducing a narrative arc</td><td>A woman shares some</td><td colspan=\"2\">A Brave Woman's Journey</td></tr><tr><td>and highlighting societal</td><td>thoughts on why 'being</td><td colspan=\"2\">From Conforming to Defying</td></tr><tr><td>themes leads to more engage-</td><td>normal' isn't all it's cracked</td><td colspan=\"2\">Society: Why Rejecting 'Nor-</td></tr><tr><td>ment with a message.</td><td>up to be.</td><td colspan=\"2\">mal' Opens the Door to True</td></tr><tr><td/><td/><td>Self-Discovery</td><td/></tr><tr><td>Introducing a sense of mys-</td><td>A Haunting Photo Of Martin</td><td colspan=\"2\">Discover the Mystery Behind</td></tr><tr><td>tery or unresolved tension af-</td><td>Luther King Jr. Plus His Im-</td><td colspan=\"2\">Martin Luther King Jr.'s Last</td></tr><tr><td>fects engagement with a mes-</td><td>mortal Audio Clip</td><td colspan=\"2\">Haunting Photo and Immor-</td></tr><tr><td>sage.</td><td/><td>tal Words</td><td/></tr><tr><td>Introducing an element of</td><td>Food Stamps Cannot Be Used</td><td colspan=\"2\">You Thought Food Stamps</td></tr><tr><td>surprise and emphasizing the</td><td>To Buy Weapons. Except In</td><td colspan=\"2\">Were Just for Groceries?</td></tr><tr><td>impact of unawareness leads</td><td>Alaska.</td><td colspan=\"2\">Guess Again, Especially in</td></tr><tr><td>to more engagement with a</td><td/><td>Alaska!</td><td/></tr><tr><td>message.</td><td/><td/><td/></tr><tr><td>Note:</td><td/><td/><td/></tr></table>",
                "text": "Examples of hypotheses, original headlines and the associated morphs To view more examples, visit https://bit.ly/jmp-morph-samp. Complete set available on OSF."
            },
            "TABREF8": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>No. Short Name</td><td>Hypotheses</td></tr></table>",
                "text": "Generated Hypotheses Predicted to Have a Positive E\u21b5ect on Engagement"
            },
            "TABREF9": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td/><td/><td colspan=\"4\">Dependent variable: CTR</td><td/></tr><tr><td/><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td><td>(7)</td></tr><tr><td>Surprise, Cli\u21b5hanger</td><td>0.055 \u21e4\u21e4\u21e4</td><td/><td/><td/><td/><td/><td>0.056 \u21e4\u21e4\u21e4</td></tr><tr><td/><td>(0.014)</td><td/><td/><td/><td/><td/><td>(0.014)</td></tr><tr><td>Parody</td><td/><td>0.014</td><td/><td/><td/><td/><td>0.036 \u21e4</td></tr><tr><td/><td/><td>(0.014)</td><td/><td/><td/><td/><td>(0.014)</td></tr><tr><td>Multimedia</td><td/><td/><td>0.063 \u21e4\u21e4\u21e4</td><td/><td/><td/><td>0.067 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td>(0.014)</td><td/><td/><td/><td>(0.015)</td></tr><tr><td>Physical Reactions</td><td/><td/><td/><td>0.029 \u21e4</td><td/><td/><td>0.019</td></tr><tr><td/><td/><td/><td/><td>(0.014)</td><td/><td/><td>(0.015)</td></tr><tr><td>Short, Simple Phrases</td><td/><td/><td/><td/><td>0.023  \u2020</td><td/><td>0.024  \u2020</td></tr><tr><td/><td/><td/><td/><td/><td>(0.014)</td><td/><td>(0.014)</td></tr><tr><td>Positive Human Behavior</td><td/><td/><td/><td/><td/><td>0.027 \u21e4</td><td>0.047 \u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td/><td/><td>(0.014)</td><td>(0.014)</td></tr><tr><td>Constant</td><td>0.010</td><td>0.009</td><td>0.010</td><td>0.008</td><td>0.009</td><td>0.010</td><td>0.012</td></tr><tr><td/><td colspan=\"6\">(0.014) (0.014) (0.014) (0.014) (0.014) (0.014)</td><td>(0.013)</td></tr><tr><td>Observations</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td></tr><tr><td>R 2</td><td>0.010</td><td>0.001</td><td>0.013</td><td>0.003</td><td>0.002</td><td>0.002</td><td>0.032</td></tr><tr><td>Adjusted R 2</td><td colspan=\"3\">0.009 0.0001 0.012</td><td>0.002</td><td>0.001</td><td>0.002</td><td>0.029</td></tr></table>",
                "text": "How well do features explain pairwise di\u21b5erence in click-through?Note: \u2020 p<0.10; \u21e4 p<0.05; \u21e4\u21e4 p<0.01; \u21e4\u21e4\u21e4 p<0.001. To make coe cients interpretable, we have scaled the outcome variable, CTR, by dividing by the standard deviation of CTR (.0119), and scaled each of the hypothesized features to have unit variance. Hence, a one standard deviation increase in any of the hypothesized features produces a change in CTR equal to b times the standard deviation in CTR."
            },
            "TABREF10": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Feature</td><td>Rated Lower</td><td>Rated Higher</td></tr><tr><td/><td>\"It Was Hard To Hide The Bruises</td><td>\"She Took A Photo Every Day</td></tr><tr><td>Surprise, Cli\u21b5hanger</td><td>When She Took A Photo Every</td><td>During The Worst Year Of Her</td></tr><tr><td/><td>Single Day For A Year\"</td><td>Life. Here's What It Looked Like.\"</td></tr><tr><td>Parody</td><td>\"A Real Before And After View Of A Day With A Homeless Man\"</td><td>\"1 Man 2 Suits. See How Clothes Really Make The Man ... Beg Harder\"</td></tr><tr><td>Multimedia</td><td>\"A story about tides that's not about global warming\"</td><td>\"A time lapse video that allows you to watch the earth breath. It's wonderful.\"</td></tr><tr><td/><td>\"Questions You Should Never Ask</td><td>\"Questions To Never Ask Your</td></tr><tr><td>Physical Reactions</td><td>Your Biracial Bestie Unless You</td><td>Biracial Bestie Unless You Like To</td></tr><tr><td/><td>Want These Priceless Responses\"</td><td>See These Blank Faces\"</td></tr><tr><td>Short, Simple Phrases</td><td>\"Mary Engelbreit: No One Should Have To Teach Their Children This In The USA\"</td><td>\"4 Words That Leave Me With No Words\"</td></tr><tr><td/><td>\"I Don't Know About You, But If</td><td>\"A Mom Spoke About Her Baby</td></tr><tr><td>Positive Human Behav-</td><td>This Fierce Mom Spoke These</td><td>For 4 Minutes. Over 100 World</td></tr><tr><td>ior</td><td>Words To My Face, I'd Tremble A</td><td>Leaders Leapt To Their Feet In</td></tr><tr><td/><td>Bit\"</td><td>Applause.\"</td></tr></table>",
                "text": "Pairs of Upworthy headlines in which feature di\u21b5erences are most pronounced"
            },
            "TABREF11": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td/><td/><td colspan=\"4\">Dependent variable: CTR</td><td/></tr><tr><td/><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td><td>(7)</td></tr><tr><td>Surprise, Cli\u21b5hanger</td><td>0.044 \u21e4\u21e4</td><td/><td/><td/><td/><td/><td>0.043 \u21e4\u21e4</td></tr><tr><td/><td>(0.013)</td><td/><td/><td/><td/><td/><td>(0.014)</td></tr><tr><td>Parody</td><td/><td>0.008</td><td/><td/><td/><td/><td>0.027  \u2020</td></tr><tr><td/><td/><td>(0.013)</td><td/><td/><td/><td/><td>(0.014)</td></tr><tr><td>Multimedia</td><td/><td/><td>0.057 \u21e4\u21e4\u21e4</td><td/><td/><td/><td>0.063 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td>(0.013)</td><td/><td/><td/><td>(0.014)</td></tr><tr><td>Physical Reactions</td><td/><td/><td/><td>0.021</td><td/><td/><td>0.012</td></tr><tr><td/><td/><td/><td/><td>(0.013)</td><td/><td/><td>(0.015)</td></tr><tr><td>Short, Simple Phrases</td><td/><td/><td/><td/><td>0.006</td><td/><td>0.007</td></tr><tr><td/><td/><td/><td/><td/><td>(0.013)</td><td/><td>(0.014)</td></tr><tr><td>Positive Human Behavior</td><td/><td/><td/><td/><td/><td>0.024  \u2020</td><td>0.044 \u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td/><td/><td>(0.013)</td><td>(0.014)</td></tr><tr><td>BU predictor (linear)</td><td colspan=\"6\">0.908 \u21e4\u21e4\u21e4 0.939 \u21e4\u21e4\u21e4 0.918 \u21e4\u21e4\u21e4 0.931 \u21e4\u21e4\u21e4 0.936 \u21e4\u21e4\u21e4 0.938 \u21e4\u21e4\u21e4</td><td>0.844 \u21e4\u21e4\u21e4</td></tr><tr><td/><td colspan=\"6\">(0.109) (0.109) (0.108) (0.109) (0.110) (0.109)</td><td>(0.110)</td></tr><tr><td>Constant</td><td>0.010</td><td>0.010</td><td>0.011</td><td>0.009</td><td>0.009</td><td>0.010</td><td>0.012</td></tr><tr><td/><td colspan=\"6\">(0.013) (0.013) (0.013) (0.013) (0.013) (0.013)</td><td>(0.013)</td></tr><tr><td>Observations</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td></tr><tr><td>R 2</td><td>0.049</td><td>0.043</td><td>0.053</td><td>0.044</td><td>0.043</td><td>0.044</td><td>0.065</td></tr><tr><td>Adjusted R 2</td><td>0.047</td><td>0.042</td><td>0.052</td><td>0.043</td><td>0.041</td><td>0.043</td><td>0.061</td></tr><tr><td>Note:</td><td/><td/><td/><td colspan=\"4\">\u2020 p&lt;0.10; \u21e4 p&lt;0.05; \u21e4\u21e4 p&lt;0.01; \u21e4\u21e4\u21e4 p&lt;0.001</td></tr><tr><td colspan=\"8\">To make coe cients interpretable, we have scaled the outcome variable, CTR, by dividing by</td></tr><tr><td colspan=\"8\">the standard deviation of CTR (0.0119), and scaled each of the hypothesized features to have unit</td></tr><tr><td colspan=\"8\">variance. Hence, a one standard deviation increase in any of the hypothesized features produces a</td></tr></table>",
                "text": "How well do features explain pairwise di\u21b5erence in click-through when adjusting for the BU prediction?"
            },
            "TABREF12": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td/><td/><td colspan=\"3\">Dependent variable: b m</td><td/><td/></tr><tr><td/><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td><td>(7)</td></tr><tr><td>Surprise, Cli\u21b5hanger</td><td>0.047 \u21e4\u21e4\u21e4</td><td/><td/><td/><td/><td/><td>0.051 \u21e4\u21e4\u21e4</td></tr><tr><td/><td>(0.007)</td><td/><td/><td/><td/><td/><td>(0.007)</td></tr><tr><td>Parody</td><td/><td>0.026 \u21e4\u21e4\u21e4</td><td/><td/><td/><td/><td>0.047 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td>(0.007)</td><td/><td/><td/><td/><td>(0.007)</td></tr><tr><td>Multimedia</td><td/><td/><td>0.042 \u21e4\u21e4\u21e4</td><td/><td/><td/><td>0.041 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td>(0.007)</td><td/><td/><td/><td>(0.007)</td></tr><tr><td>Physical Reactions</td><td/><td/><td/><td>0.036 \u21e4\u21e4\u21e4</td><td/><td/><td>0.038 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td>(0.007)</td><td/><td/><td>(0.008)</td></tr><tr><td>Short, Simple Phrases</td><td/><td/><td/><td/><td>0.035 \u21e4\u21e4\u21e4</td><td/><td>0.036 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td/><td>(0.007)</td><td/><td>(0.007)</td></tr><tr><td>Positive Human Behavior</td><td/><td/><td/><td/><td/><td>0.024 \u21e4\u21e4\u21e4</td><td>0.038 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td/><td/><td>(0.007)</td><td>(0.007)</td></tr><tr><td>Constant</td><td>0.009</td><td>0.009</td><td>0.008</td><td>0.010</td><td>0.009</td><td>0.009</td><td>0.007</td></tr><tr><td/><td colspan=\"5\">(0.007) (0.007) (0.007) (0.007) (0.007)</td><td>(0.007)</td><td>(0.007)</td></tr><tr><td>Observations</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td><td>1,693</td></tr><tr><td>R 2</td><td>0.025</td><td>0.008</td><td>0.020</td><td>0.015</td><td>0.014</td><td>0.007</td><td>0.102</td></tr><tr><td>Adjusted R 2</td><td>0.025</td><td>0.007</td><td>0.019</td><td>0.014</td><td>0.013</td><td>0.006</td><td>0.098</td></tr><tr><td>Note:</td><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "text": "How well do features explain the ML model predictions? \u21e4\u21e4 p<0.01; \u21e4\u21e4\u21e4 p<0.001To make coe cients interpretable, we have scaled the outcome variable, CTR, by dividing by the standard deviation of CTR (.0119), and scaled each of the hypothesized features to have unit variance. Hence, a one standard deviation increase in any of the hypothesized features produces a change in CTR equal to b times the standard deviation in CTR."
            },
            "TABREF13": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td colspan=\"2\">Raters</td><td/><td/></tr><tr><td>Variable</td><td>Mean</td><td colspan=\"2\">SD Median</td></tr><tr><td>Number of Raters</td><td>5.96</td><td>2.23</td><td>6</td></tr><tr><td colspan=\"2\">Contexts</td><td/><td/></tr><tr><td>SMS Political</td><td>0.642</td><td/><td/></tr><tr><td>Product Description</td><td>0.340</td><td/><td/></tr><tr><td>Email Charity</td><td>0.547</td><td/><td/></tr><tr><td>Headlines</td><td>0.575</td><td/><td/></tr><tr><td>Social Media Posts</td><td>0.896</td><td/><td/></tr><tr><td>Billboard Advertisement</td><td>0.434</td><td/><td/></tr><tr><td colspan=\"2\">Email from Doctor's O ce 0.094</td><td/><td/></tr><tr><td>No Additional Contexts</td><td>0</td><td/><td/></tr><tr><td colspan=\"2\">Outcomes</td><td/><td/></tr><tr><td>Donation</td><td colspan=\"2\">0.568 0.665</td><td>0.732</td></tr><tr><td>Donation Amount</td><td colspan=\"2\">0.472 0.629</td><td>0.6</td></tr><tr><td>Register to Vote</td><td colspan=\"2\">0.563 0.569</td><td>0.667</td></tr><tr><td>Vote</td><td colspan=\"2\">0.580 0.579</td><td>0.667</td></tr><tr><td>Open</td><td colspan=\"2\">0.684 0.685</td><td>0.866</td></tr><tr><td>Respond</td><td colspan=\"2\">0.479 0.702</td><td>0.5</td></tr><tr><td>Share</td><td colspan=\"2\">0.478 0.728</td><td>0.586</td></tr><tr><td>Unsubscribe</td><td colspan=\"2\">0.0707 0.556</td><td>0</td></tr><tr><td>Block</td><td colspan=\"2\">-0.0491 0.521</td><td>0</td></tr><tr><td>Click</td><td colspan=\"2\">0.775 0.763</td><td>1</td></tr><tr><td>Scan</td><td colspan=\"2\">0.303 0.643</td><td>0.4</td></tr><tr><td>Note:</td><td/><td/><td/></tr></table>",
                "text": "Summary of Participant Forecasts of Contexts and Outcomes"
            },
            "TABREF14": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Measure</td><td>Morph</td><td colspan=\"3\">Upworthy t-Statistic Cohen's d (95% CI)</td><td>P-Value</td></tr><tr><td>Interest</td><td colspan=\"2\">2.62 (1.91) 2.62 (1.92)</td><td>0.01</td><td>0.00 [ 0.08, 0.08]</td><td>0.992</td></tr><tr><td>Click</td><td colspan=\"2\">2.64 (1.97) 2.64 (1.97)</td><td>0.06</td><td>0.00 [ 0.08, 0.08]</td><td>0.955</td></tr><tr><td>Self Impression</td><td colspan=\"2\">0.10 (1.68) 0.02 (1.67)</td><td>1.22</td><td>0.05 [ 0.03, 0.13]</td><td>0.224</td></tr><tr><td colspan=\"3\">Other Impression 0.13 (1.58) 0.07 (1.64)</td><td>0.90</td><td>0.04 [ 0.04, 0.12]</td><td>0.368</td></tr><tr><td>Quality</td><td>0.02 (1.82)</td><td>0.07 (1.85)</td><td>1.21</td><td>0.05 [ 0.03, 0.13]</td><td>0.228</td></tr><tr><td colspan=\"4\">Note: Mean and standard deviation reported for each.</td><td/><td/></tr></table>",
                "text": "Quality of Morphs: Mean Attitude Ratings"
            },
            "TABREF16": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table/>",
                "text": ""
            },
            "TABREF17": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td colspan=\"2\">Source Strategy</td><td colspan=\"2\">Mean Median 75th percentile</td></tr><tr><td>GPT</td><td colspan=\"2\">Aggregate 0.597 0.600</td><td>0.722</td></tr><tr><td>GPT</td><td>Pairwise</td><td>0.857 0.851</td><td>1.025</td></tr><tr><td colspan=\"3\">human Aggregate 0.768 0.747</td><td>0.897</td></tr><tr><td colspan=\"2\">human Pairwise</td><td>0.873 0.864</td><td>1.016</td></tr></table>",
                "text": "Pairwise distances between embedding vectors for hypothesis, generated from various strategies."
            },
            "TABREF18": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Splits</td><td/></tr><tr><td>Training Regression Morphing Lock-Box</td><td>Total</td></tr></table>",
                "text": "Counts for Social Media Data"
            },
            "TABREF19": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td/><td/><td colspan=\"3\">Dependent variable: CTR</td><td/><td/></tr><tr><td/><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td><td>(7)</td></tr><tr><td>Surprise, Cli\u21b5hanger</td><td>-0.006</td><td/><td/><td/><td/><td/><td>-0.002</td></tr><tr><td/><td>(0.012)</td><td/><td/><td/><td/><td/><td>(0.012)</td></tr><tr><td>Parody</td><td/><td>0.009</td><td/><td/><td/><td/><td>0.019</td></tr><tr><td/><td/><td>(0.012)</td><td/><td/><td/><td/><td>(0.013)</td></tr><tr><td>Multimedia</td><td/><td/><td>-0.060 \u21e4\u21e4\u21e4</td><td/><td/><td/><td>-0.069 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td>(0.012)</td><td/><td/><td/><td>(0.012)</td></tr><tr><td>Physical Reactions</td><td/><td/><td/><td>0.034 \u21e4\u21e4</td><td/><td/><td>0.059 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td>(0.012)</td><td/><td/><td>(0.013)</td></tr><tr><td>Short, Simple Phrases</td><td/><td/><td/><td/><td>-0.082 \u21e4\u21e4\u21e4</td><td/><td>-0.078 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td/><td>(0.012)</td><td/><td>(0.012)</td></tr><tr><td>Positive Human Behavior</td><td/><td/><td/><td/><td colspan=\"2\">-0.084 \u21e4\u21e4\u21e4</td><td>-0.077 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td/><td colspan=\"2\">(0.012)</td><td>(0.012)</td></tr><tr><td>Constant</td><td colspan=\"6\">0.618 (0.012) (0.012) (0.012) (0.012) (0.012) (0.012)</td><td>(0.012)</td></tr><tr><td>Observations</td><td>5,056</td><td>5,056</td><td>5,056</td><td>5,056</td><td>5,056</td><td>5,056</td><td>5,056</td></tr><tr><td>R 2</td><td>0.000</td><td>0.000</td><td>0.005</td><td>0.002</td><td>0.010</td><td>0.010</td><td>0.027</td></tr><tr><td>Adjusted R 2</td><td>0.000</td><td>0.000</td><td>0.005</td><td>0.001</td><td>0.010</td><td>0.010</td><td>0.026</td></tr><tr><td>Note:</td><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "text": "How well do features explain click through rates, for social media organizational partner's data? \u21e4\u21e4\u21e4 0.618 \u21e4\u21e4\u21e4 0.618 \u21e4\u21e4\u21e4 0.618 \u21e4\u21e4\u21e4 0.618 \u21e4\u21e4\u21e4 0.618 \u21e4\u21e4\u21e4 0.618 \u21e4\u21e4\u21e4"
            },
            "TABREF20": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td>Splits</td><td/></tr><tr><td>EDA Regression</td><td>Total</td></tr></table>",
                "text": "Counts for Progressive Outreach Partner Data"
            },
            "TABREF21": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td/><td/><td colspan=\"4\">Dependent variable: CTR</td><td/></tr><tr><td/><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td><td>(7)</td></tr><tr><td>Surprise, Cli\u21b5hanger</td><td>0.001</td><td/><td/><td/><td/><td/><td>-0.023</td></tr><tr><td/><td>(0.018)</td><td/><td/><td/><td/><td/><td>(0.022)</td></tr><tr><td>Parody</td><td/><td>0.019</td><td/><td/><td/><td/><td>0.004</td></tr><tr><td/><td/><td>(0.019)</td><td/><td/><td/><td/><td>(0.023)</td></tr><tr><td>Multimedia</td><td/><td/><td>0.020</td><td/><td/><td/><td>0.009</td></tr><tr><td/><td/><td/><td>(0.019)</td><td/><td/><td/><td>(0.023)</td></tr><tr><td>Physical Reaction</td><td/><td/><td/><td>0.059 \u21e4</td><td/><td/><td>0.061 \u21e4</td></tr><tr><td/><td/><td/><td/><td>(0.024)</td><td/><td/><td>(0.025)</td></tr><tr><td>Short, Simple Phrases</td><td/><td/><td/><td/><td>-0.056 \u21e4</td><td/><td>-0.054 \u21e4</td></tr><tr><td/><td/><td/><td/><td/><td>(0.023)</td><td/><td>(0.021)</td></tr><tr><td>Positive Human Behavior</td><td/><td/><td/><td/><td/><td>0.025</td><td>-0.008</td></tr><tr><td/><td/><td/><td/><td/><td/><td colspan=\"2\">(0.028) (0.030)</td></tr><tr><td>Observations</td><td>506</td><td>506</td><td>506</td><td>506</td><td>506</td><td>506</td><td>506</td></tr><tr><td>R 2</td><td colspan=\"6\">-0.002 0.000 0.008 0.021 0.019 0.002</td><td>0.042</td></tr><tr><td>Adjusted R 2</td><td colspan=\"6\">-0.002 0.000 0.001 0.021 0.019 0.002</td><td>0.033</td></tr><tr><td>Note:</td><td/><td/><td/><td/><td/><td/><td/></tr></table>",
                "text": "How well do features explain pairwise di\u21b5erence unique clicks, for progressive outreach partner's data?"
            },
            "TABREF22": {
                "num": null,
                "type_str": "table",
                "html": null,
                "content": "<table><tr><td/><td/><td/><td colspan=\"4\">Dependent variable: CTR</td></tr><tr><td/><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td><td>(5)</td><td>(6)</td><td>(7)</td></tr><tr><td>Surprise, Cli\u21b5hanger</td><td>0.055 \u21e4\u21e4\u21e4</td><td/><td/><td/><td/><td/><td>0.062 \u21e4\u21e4\u21e4</td></tr><tr><td/><td>(0.012)</td><td/><td/><td/><td/><td/><td>(0.013)</td></tr><tr><td>Parody</td><td/><td>0.028 \u21e4</td><td/><td/><td/><td/><td>0.047 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td>(0.013)</td><td/><td/><td/><td/><td>(0.013)</td></tr><tr><td>Multimedia</td><td/><td/><td>0.039 \u21e4\u21e4</td><td/><td/><td/><td>0.045 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td>(0.013)</td><td/><td/><td/><td>(0.014)</td></tr><tr><td>Physical Reactions</td><td/><td/><td/><td>0.018</td><td/><td/><td>0.020</td></tr><tr><td/><td/><td/><td/><td>(0.013)</td><td/><td/><td>(0.014)</td></tr><tr><td>Short, Simple Phrases</td><td/><td/><td/><td/><td>0.018</td><td/><td>0.018</td></tr><tr><td/><td/><td/><td/><td/><td>(0.013)</td><td/><td>(0.013)</td></tr><tr><td>Positive Human Behavior</td><td/><td/><td/><td/><td/><td>0.031 \u21e4</td><td>0.048 \u21e4\u21e4\u21e4</td></tr><tr><td/><td/><td/><td/><td/><td/><td>(0.013)</td><td>(0.014)</td></tr><tr><td>Constant</td><td>0.001</td><td>0.001</td><td colspan=\"4\">0.001 0.0002 0.0002 0.0002</td><td>0.003</td></tr><tr><td/><td colspan=\"6\">(0.013) (0.013) (0.013) (0.013) (0.013) (0.013)</td><td>(0.013)</td></tr><tr><td>Observations</td><td>1,337</td><td>1,337</td><td colspan=\"2\">1,337 1,337</td><td>1,337</td><td>1,337</td><td>1,337</td></tr><tr><td>R 2</td><td>0.015</td><td>0.004</td><td colspan=\"2\">0.007 0.002</td><td>0.002</td><td>0.004</td><td>0.040</td></tr><tr><td>Adjusted R 2</td><td>0.014</td><td>0.003</td><td colspan=\"2\">0.006 0.001</td><td>0.001</td><td>0.004</td><td>0.036</td></tr></table>",
                "text": "How well do features explain pairwise di\u21b5erence in click-through? (Excluding non-randomized trials) Note: \u2020 p<0.10; \u21e4 p<0.05; \u21e4\u21e4 p<0.01; \u21e4\u21e4\u21e4 p<0.001. To make coe cients interpretable, we have scaled the outcome variable, CTR, by dividing by the standard deviation of CTR (.0119), and scaled each of the hypothesized features to have unit variance. Hence, a one standard deviation increase in any of the hypothesized features produces a change in CTR equal to b times the standard deviation in CTR."
            }
        }
    }
}