{
    "paper_id": "s10489-022-03281-1",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-09-19T13:39:07.883040Z"
    },
    "title": "Word-level human interpretable scoring mechanism for novel text detection using Tsetlin Machines",
    "authors": [
        {
            "first": "Bimal",
            "middle": [],
            "last": "Bhattarai",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Ole-Christoffer",
            "middle": [],
            "last": "Granmo",
            "suffix": "",
            "affiliation": {},
            "email": ""
        },
        {
            "first": "Lei",
            "middle": [],
            "last": "Jiao",
            "suffix": "",
            "affiliation": {},
            "email": ""
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Recent research in novelty detection focuses mainly on document-level classification, employing deep neural networks (DNN). However, the black-box nature of DNNs makes it difficult to extract an exact explanation of why a document is considered novel. In addition, dealing with novelty at the word level is crucial to provide a more fine-grained analysis than what is available at the document level. In this work, we propose a Tsetlin Machine (TM)-based architecture for scoring individual words according to their contribution to novelty. Our approach encodes a description of the novel documents using the linguistic patterns captured by TM clauses. We then adapt this description to measure how much a word contributes to making documents novel. Our experimental results demonstrate how our approach breaks down novelty into interpretable phrases, successfully measuring novelty.",
    "pdf_parse": {
        "paper_id": "s10489-022-03281-1",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Recent research in novelty detection focuses mainly on document-level classification, employing deep neural networks (DNN). However, the black-box nature of DNNs makes it difficult to extract an exact explanation of why a document is considered novel. In addition, dealing with novelty at the word level is crucial to provide a more fine-grained analysis than what is available at the document level. In this work, we propose a Tsetlin Machine (TM)-based architecture for scoring individual words according to their contribution to novelty. Our approach encodes a description of the novel documents using the linguistic patterns captured by TM clauses. We then adapt this description to measure how much a word contributes to making documents novel. Our experimental results demonstrate how our approach breaks down novelty into interpretable phrases, successfully measuring novelty.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "The fundamental principle underlying machine learning classifiers is a generalization -the ability to form a decision boundary that differentiates new input into known classes. When training a supervised classifier, it is common to assume that the classes to be recognized are present both in the training and test data [49] . However, given an open world, training on all conceivable classes of input is impractical. This problem introduces the need for novelty detection -the task of spotting input classes that one has not seen before. The problem is particularly severe in textbased supervised classification due to the many-faceted nature of natural language, which gives rise to multiple application-dependent interpretations. Indeed, researchers Bimal Bhattarai bimal.bhattarai@uia.no; bobsbimal58@gmail.com Ole-Christoffer Granmo ole.granmo@uia.no Lei Jiao lei.jiao@uia.no 1 Centre for Artificial Intelligence Research (CAIR), University of Agder, Grimstad, Norway have for a long time tried to address novelty detection in natural language. So far, no single best model has appeared. Indeed, the success of each model relies on the properties of each particular dataset.",
                "cite_spans": [
                    {
                        "start": 320,
                        "end": 324,
                        "text": "[49]",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The problem of novelty detection arises in many tasks, such as fault detection [16] and handwritten alphabet recognition [54] . In general, one applies novelty detection when it is required to know whether a given input is similar to or significantly different from the training data. For natural language text, the novelty detector should discern that a text does not belong to a predefined set of topics. Several challenges make such novelty detection particularly difficult:",
                "cite_spans": [
                    {
                        "start": 79,
                        "end": 83,
                        "text": "[16]",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 121,
                        "end": 125,
                        "text": "[54]",
                        "ref_id": "BIBREF53"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1. Textual information tends to be diverse, composed of large vocabularies. 2. Language and topics are typically evolving, making the novelty detection problem dynamic [21] .",
                "cite_spans": [
                    {
                        "start": 168,
                        "end": 172,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Lately, the aforementioned challenges have manifested when using supervised learning to build chatbots, an application area that is gaining traction. A chatbot typically needs to handle the language of a multitude of users with evolving information requirements. As such, it must be able to determine when it is capable of answering a query and when it faces a new topic.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Majority of the existing literature on text-based novelty detection addresses one of the following granularity levels:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "1. Event-level techniques [4] perform topic detection and tracking on a stream of documents. 2. Document-level techniques [17] classify an incoming document as known or novel based on its content. 3. Sentence-level techniques [6] look for novel sentences within a particular document.",
                "cite_spans": [
                    {
                        "start": 26,
                        "end": 29,
                        "text": "[4]",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 122,
                        "end": 126,
                        "text": "[17]",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 226,
                        "end": 229,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Usually, the sentences/documents are ranked based on some sort of similarity score, obtained from comparing them with previously seen sentences/documents. For instance, the Maximal Marginal Relevance model (MMR) proposed in [14] assigns low scores to previously seen sentences/documents, while assigning high scores to novel ones.",
                "cite_spans": [
                    {
                        "start": 224,
                        "end": 228,
                        "text": "[14]",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Figure 1 illustrates the problem of novelty detection, contrasting it against anomaly and outlier detection. Anomaly detection [15] concerns discovering anomalies, which are invalid data points. Outlier detection [3, 29] , on the other hand, flags legitimate data points that deviate significantly from the mean. Finally, novelty detection [43] is the discovery of entirely new types of data points.",
                "cite_spans": [
                    {
                        "start": 127,
                        "end": 131,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 213,
                        "end": 216,
                        "text": "[3,",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 217,
                        "end": 220,
                        "text": "29]",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 340,
                        "end": 344,
                        "text": "[43]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In contrast to prior work, we here focus on novelty detection at the word level. To this end, we propose a new interpretable machine learning approach for calculating novelty scores for the words within a sentence. The calculation is based on the linguistic patterns captured by a Tsetlin Machine (TM) in the form of AND-rules (i.e., conjunctive clauses). To the best of our knowledge, this is the first study of its kind on this problem.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "In the supervised classification setting, i pre-labeled data points D = {(v 1 , y 1 ), (v 2 , y 2 ), . . . , (v i , y i )} is used for training. Here, v i is the i th input example and y i is its class. The input v i is an t-dimensional real-valued vector (x 1 , x 2 , . . . , x o ) \u2208 R t , where x o refers to the o th element of the vector. The class y i \u2208 Y = {1, 2, . . . , C l }, in turn, is an integer class index referring to one out of C l classes. Learning a classifier entails constructing a classification function f (v; D), f : R t \u2192 Y , based on the data D. The function simply assigns a label y to the data point v. Our emphasis is novelty scoring, which can be seen as another function z(v; D), z : R t \u2192 R.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem definition",
                "sec_num": null
            },
            {
                "text": "The function computes a real-valued novelty score for input data point v, with the purpose of discerning new classes not found in Y . In this way, a classifier can return the correct class label while flagging novel examples. Considering each element in v to represent a specific word, this paper further extend the novelty detection by introducing a method for breaking down the overall score z(v; D) for v into the contribution of each element x o . By doing so, we break down novelty into interpretable phrases.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Problem definition",
                "sec_num": null
            },
            {
                "text": "In this paper, we use the TM to construct conjunctive clauses in propositional logic. In this manner, we capture frequent patterns in the data D, which we then utilize to characterize the known classes Y comprehensively. The novelty score is then calculated based on examining the clauses that match the given input. By further looking into the composition of each clause, we are able to break down the novelty score into the contribution of the different phrases. This decomposition is based on training clauses for the novel data and then measuring the relative frequency of each word inside the clauses for the known classes, contrasted against the relative frequency obtained from the clauses of the novel class. These scores can, in turn, be adopted as input features to machine learning classifiers for novelty detection. Similarly, contextual scores can be calculated simply by inspecting each word's clauses, providing a local perspective for both novel and known classes.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Paper contributions",
                "sec_num": null
            },
            {
                "text": "The remainder of the paper is organized as follows. In Section 2, we first summarize related work before we present the details of the TM in Section 3. This forms the basis for our novelty description architecture, covered in Section 4. In Section 5, we present our empirical results, concluding the work in the last section.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Paper contributions",
                "sec_num": null
            },
            {
                "text": "Several studies have been carried out on supervised multiclass classification in a closed-world setting [5] . There is a dearth of work addressing open-world settings [33] , with distance-based methods being one of the earliest approaches [28] . These approaches rely on nearest neighbor search, which introduces scalability issues when dealing with larger datasets. Another class of methods are based on single-class classifiers. These include One-Class SVM [50] and SVDD [55] . Further, the decision score from SVM has been used to produce a probability distribution for novelty detection [44] . As no negative training samples are used, single-class classifiers struggle with maximizing the class margin. To overcome the problem of One-Class SVMs, a new learning method named center-based similarity space (CBS) was proposed in [20] , which transforms each document in a closed boundary to a central similarity vector that can be used in a binary classifier. Probabilistic methods have also been utilized for novelty detection [43] . In [30] , a technique to threshold the entropy of the estimated class probability distribution is proposed. In that method, choosing the entropy threshold needs prior knowledge. Additionally, the class probability distribution can be misleading when novel data points fall far from the decision boundary. In [32] and [46] , an active learning model is proposed to both discover and classify novel classes during training. However, the appearance of novel instances during testing is not considered.",
                "cite_spans": [
                    {
                        "start": 104,
                        "end": 107,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 167,
                        "end": 171,
                        "text": "[33]",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 239,
                        "end": 243,
                        "text": "[28]",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 459,
                        "end": 463,
                        "text": "[50]",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 473,
                        "end": 477,
                        "text": "[55]",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 591,
                        "end": 595,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 831,
                        "end": 835,
                        "text": "[20]",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 1030,
                        "end": 1034,
                        "text": "[43]",
                        "ref_id": null
                    },
                    {
                        "start": 1040,
                        "end": 1044,
                        "text": "[30]",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 1345,
                        "end": 1349,
                        "text": "[32]",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 1354,
                        "end": 1358,
                        "text": "[46]",
                        "ref_id": "BIBREF45"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "DNNs have recently been used to address the problem of novelty detection. In [61] , a two-class SVM classifier is adopted to categorize known and novel classes. An adversarial sample generation (ASG) framework [23] is used to generate positive and negative samples. Similarly, [37] employs generative adversarial networks (GANs), where the generator produces a mixture of known and novel data. The generator is trained with so-called feature matching loss, and the discriminator performs simultaneous classification and novelty detection. In computer vision, the problem of novel image detection is addressed by introducing the concept of open space risk [49] . This is achieved by reducing the half-space of a binary SVM classifier with two parallel hyperplanes that bound the positive region. Although the binary SVM reduces the positive region to half-spaces, their open space risk is still infinite. In [5] , a method called OpenMAX is proposed, which estimates the probability of an input belonging to a novel class. In general, the major weaknesses of these methods are high computational complexity and uninterpretable inference. A state-of-the-art GAN-based method for unsupervised outlier detection called Single-Objective Generative Adversarial Active Learning (SO-GAAL) and Multi-Objective Generative Adversarial Active Learning (MO-GAAL) was proposed in [41] . The method is based on a min-max game between a generator and a discriminator. The training process of the generator is paused before convergence to synthesize outliers, which is subsequently used to train the discriminator to recognize the outliers.",
                "cite_spans": [
                    {
                        "start": 77,
                        "end": 81,
                        "text": "[61]",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 210,
                        "end": 214,
                        "text": "[23]",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 277,
                        "end": 281,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 655,
                        "end": 659,
                        "text": "[49]",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 907,
                        "end": 910,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 1366,
                        "end": 1370,
                        "text": "[41]",
                        "ref_id": "BIBREF40"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "However, the method is primarily designed for high-dimensional data, requiring extensive problem-specific hyperparameter tweaking. The unsupervised learning method COPOD [40] is a more recent approach that is inspired by copulas for modeling multivariate data distributions. In comparison to other methods, COPOD is computationally efficient, interpretable, and is unaffected by feature dimension. However, the method fails to handle complex features and intricate nonlinear relations.",
                "cite_spans": [
                    {
                        "start": 170,
                        "end": 174,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Apart from the studies on the document-level novelty detection, novelty detection at the event level arises from topic detection, which focuses on the online event and story detection [38] . The study at the event level primarily consists of clustering algorithms that measure the closeness of incoming events or stories to one of the clusters depending on a pre-defined threshold. Novelty detection at the sentence level was investigated in Text Retrieval Conferences (TREC) by highlighting sentences that include novel information given a topic and a list of documents [52] . Based on TREC, many studies have been conducted on novelty detection at sentence level [56, 63] , including term translations, Principal Component Analysis (PCA) vectors, Support Vector Machine (SVM) classification, named entities patterns, etc. Likewise, a few approaches have been introduced for learning sentence embeddings, including SkipThought [36] , Conceptual Sentence Embedding [58] , and FastSent [31] . However, these approaches on embeddings are very dependent on the domain-specific downstream tasks. Recently introduced powerful language models, such as ELMo [42] and BERT [18] , have been successful for transfer learning and they are able to learn dynamic sentence embedding in an unsupervised manner.",
                "cite_spans": [
                    {
                        "start": 184,
                        "end": 188,
                        "text": "[38]",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 571,
                        "end": 575,
                        "text": "[52]",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 665,
                        "end": 669,
                        "text": "[56,",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 670,
                        "end": 673,
                        "text": "63]",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 928,
                        "end": 932,
                        "text": "[36]",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 965,
                        "end": 969,
                        "text": "[58]",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 985,
                        "end": 989,
                        "text": "[31]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 1151,
                        "end": 1155,
                        "text": "[42]",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 1165,
                        "end": 1169,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "In [22] , a unified attention architecture is proposed to deal with vector representations of text input in NLP. The authors investigate how information can be retrieved from attention in NLP. Further, [51] checks whether the attention weights provide any interpretability by manipulating the weights in pretrained text classification models. They used an intermediate representation erasure method to demonstrate that attention weights are unreliable predictors of the relative significance of the specific input. They thus do not accurately explain the model's decision-making. Additionally, [53] employed a novel approach for visualizing the attention score for each token. This is the first study on interpretability analysis by visualizing and scoring at the word level. However, as explained in [34] , the scoring acquired using attention methods does not provide a meaningful explanation. A more advanced scoring method known as Masked Language Model (MLM) [48] uses pretrained MLM to score sentences using pseudo-log-likelihood scores (PLLs), which involves masking each token one by one. The method becomes unsuitable for scoring the entire tokens of the dataset as the computational complexity rises.",
                "cite_spans": [
                    {
                        "start": 3,
                        "end": 7,
                        "text": "[22]",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 202,
                        "end": 206,
                        "text": "[51]",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 594,
                        "end": 598,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    },
                    {
                        "start": 801,
                        "end": 805,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 964,
                        "end": 968,
                        "text": "[48]",
                        "ref_id": "BIBREF47"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "Likewise, recent keyword extraction (KE) algorithms such as YAKE [13] and KeyBERT [26] are also used to extract the top-scoring tokens from the trained model. To the best of our knowledge, in novelty detection, there exists no such method to measure each word's contribution to the novelty. In this study, we expand the study on novelty detection with a method for scoring each word's contribution to the overall novelty, which offers a clear view to the researchers for the reasoning and the interpretation of the results that the algorithm offers.",
                "cite_spans": [
                    {
                        "start": 65,
                        "end": 69,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 82,
                        "end": 86,
                        "text": "[26]",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Related work",
                "sec_num": "2"
            },
            {
                "text": "The TM, proposed in [24] , is a recent approach to pattern classification, regression, and novelty detection [1, 8, 25] . It captures the frequent patterns of the learning problem using conjunctive clauses in propositional logic. Each clause is a conjunction of literals, where a literal is a propositional/Boolean variable or its negation. Recent research reports that the TM performs competitively with state-ofthe-art deep learning networks in text classification [7, 47, 59, 60] along with parallel and asynchronous architecture [2] for faster learning across diverse tasks. Further, theoretical studies have uncovered robust convergence properties [35, 62] .",
                "cite_spans": [
                    {
                        "start": 20,
                        "end": 24,
                        "text": "[24]",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 109,
                        "end": 112,
                        "text": "[1,",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 113,
                        "end": 115,
                        "text": "8,",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 116,
                        "end": 119,
                        "text": "25]",
                        "ref_id": null
                    },
                    {
                        "start": 467,
                        "end": 470,
                        "text": "[7,",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 471,
                        "end": 474,
                        "text": "47,",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 475,
                        "end": 478,
                        "text": "59,",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 479,
                        "end": 482,
                        "text": "60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 533,
                        "end": 536,
                        "text": "[2]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 653,
                        "end": 657,
                        "text": "[35,",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 658,
                        "end": 661,
                        "text": "62]",
                        "ref_id": "BIBREF61"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "A basic TM accepts a vector X = (x 1 , . . . , x o ) \u2208 {0, 1} o of o Boolean features as input. For text input, it is typical to booleanize the text to form a Boolean set of words, as suggested in [7] . The input features, together with their negated counterparts, x = \u00acx = 1x , form a literal set L = {x 1 , . . . , x o , \u00acx 1 , . . . , \u00acx o }. For classification problems, the sub-patterns associated with the classes are captured by the TM using m conjunctive clauses C + j or C - j . The j = 1, . . . , m/2 subscript denotes the clause index, while the superscript indicates the polarity of a clause. In brief, half of the clauses are assigned positive polarity, i.e., C + j , and the other half are assigned negative polarity, i.e., C - j . The positive polarity clauses vote for the input belonging to the class favored by the TM, while the negative polarity clauses vote against that class, that is, for other classes.",
                "cite_spans": [
                    {
                        "start": 197,
                        "end": 200,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "A clause C \u03be j , \u03be \u2208 {-, +}, is formed by ANDing a subset L \u03be j \u2286 L of the literal set. That is, the set of literals for clause C \u03be j with polarity \u03be can be written as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "C \u03be j (X) = l\u2208L \u03be j l = l\u2208L \u03be j l.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "( 1 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "The clause evaluates to 1 if and only if all the literals of the clause also evaluate to 1. For example, the clause C \u03be j (X) = x 1 x 2 consists of the literals L \u03be j = {x 1 , x 2 } and outputs 1, if",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "x 1 = x 2 = 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "The final classification decision is obtained by subtracting the negative votes from the positive votes, and then thresholding the resulting sum using the unit step function u:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u0177 = u \u239b \u239d m/2 j =1 C + j (X) - m/2 j =1 C - j (X) \u239e \u23a0 . (",
                        "eq_num": "2"
                    }
                ],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "For example, the classifier \u0177 = u(x",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "1 x2 + x1 x 2 -x 1 x 2 -x1 x2 ) captures the XOR-relation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "For learning, the TM employs a team of Tsetlin Automata (TA), one TA per literal l \u2208 L. Each TA performs one of two actions: either include or exclude its designated literal. Each clause statistically forwards the feedback to its individual TA. The TM employs Type I and Type II feedback. These feedback types control the reward, penalty or inaction received by TAs depending on six factors: (1) target output (y = 0 or y = 1), ( 2) clause polarity, (3) clause output (C j = 0 or 1), (4) literals value (x = 1, or \u00acx = 1), ( 5) vote sum, and ( 6) the current state of the TA. Type I feedback is designed to produce frequent patterns, while Type II feedback increases the discriminating power of the patterns (see [25] for details). The feedback guides the complete system of TAs towards a Nash equilibrium. At any point in the training process, we have m conjunctive clauses per class, half of them positive and half of them negative. These can be retrieved and deployed upon completion of training.",
                "cite_spans": [
                    {
                        "start": 713,
                        "end": 717,
                        "text": "[25]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Tsetlin machine (TM) architecture",
                "sec_num": "3"
            },
            {
                "text": "By novelty description, we mean the task of characterizing novel textual content at the word level. For instance, the known content may be reviews of mobile phones, while the novel content could be reviews of grocery stores. For this example, one may define the novel content using words associated with grocery stores. However, describing novelty at the word level is nontrivial because the meaning of words varies depending on the context they appear in. For example, consider the word \"bat\". This word typically manifests in two distinct contexts-it can denote either \"animal\" or \"sports\". Likewise, the word \"bank\" can refer to \"river bank\" or \"cash bank\". That is, when contextual meaning is considered, the novelty of the word \"bat\" and \"bank\" can be different based on their respective uses. As a result, measuring and describing novel content is a challenging problem.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Novelty description",
                "sec_num": "4"
            },
            {
                "text": "In general, one can detect and characterize novel content by contrasting against the probability of observing textual content X, given that the content is known. We denote this probability distribution by p known (X). Assume that the corresponding probability distribution p novel (X) for novel content also is available. Then, the optimal novelty detection test for a given false positive rate (\u03b1) can be obtained by thresholding the likelihood ratio p novel (X) / p known (X) [39] .",
                "cite_spans": [
                    {
                        "start": 478,
                        "end": 482,
                        "text": "[39]",
                        "ref_id": "BIBREF38"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Novelty description",
                "sec_num": "4"
            },
            {
                "text": "Since neither p known (X) or p novel (X) are available to us, we must estimate them using training examples. Inspired by the work in [9] on Semi-Supervised Novelty Detection (SSND), we use two sets of examples. One set represents known content, while the other represents novel content. We obtain these sets by employing a binary classifier that can distinguish between known and novel content, such as the one we proposed in [8] .",
                "cite_spans": [
                    {
                        "start": 133,
                        "end": 136,
                        "text": "[9]",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 426,
                        "end": 429,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Novelty description",
                "sec_num": "4"
            },
            {
                "text": "In our approach, we begin by training a TM on input texts represented as Boolean bag-of-words, i.e., as word sets. A propositional variable represents each word in the vocabulary, capturing the presence/absence of the corresponding word in the input text. We group the texts into two classes, Known and Novel. The first represents known content, and the second represents novel content. Our task is to describe how the second group of text is novel at the word level. To this end, we begin by identifying novel word candidates, followed by scoring and ranking the words based on their contribution to novelty.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying novel word candidates",
                "sec_num": "4.1"
            },
            {
                "text": "Figure 2 shows our architecture for identifying novel word candidates. As seen, upon training, we obtain the clauses of the two classes, Known and Novel. We extract all the words included in the clauses for each class. Each clause contains a combination of both plain (P L ) and negated (N L ) words. As such, the plain and the negated words serve two different roles. The plain words characterize the corresponding class, while the negated words characterize the other class. We exploit this property as follows, building two bag-of-words (BOW). The first is a bag of known words, referred to as B K , and the second is a bag of novel words, referred to as B N .",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "Identifying novel word candidates",
                "sec_num": "4.1"
            },
            {
                "text": "For class Known, we perform the following procedure:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying novel word candidates",
                "sec_num": "4.1"
            },
            {
                "text": "-We consider the words included in positive clauses first.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying novel word candidates",
                "sec_num": "4.1"
            },
            {
                "text": "Here, the plain words P L are added to the bag of known words B K , while the negated words are placed in the bag of novel words B N . -For negative clauses, we do the opposite. The plain words P L are added to the novel words bag B N . The negated words N L , on the other hand, are added to the known word bag B K .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying novel word candidates",
                "sec_num": "4.1"
            },
            {
                "text": "The above procedure is inverted for class Novel:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying novel word candidates",
                "sec_num": "4.1"
            },
            {
                "text": "-For the positive clauses, the plain words P L are added to the novel word bag B N , while the negated words are added to the known word bag B K . -Conversely, for the negative clauses, the plain words are added to B K , characterizing the known class, while the negated words N L are added to B N .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Identifying novel word candidates",
                "sec_num": "4.1"
            },
            {
                "text": "With the word bags B K and B N available, we calculate novelty scores at the word level as follows. From the unique words in the bags B K and B N , we produce two corresponding word sets, S K and S N . Assume these respectively contain K and N unique words: We next estimate the occurrence probability p s i of each word s i in S K , from the known class. The estimate is based on the relative frequency of s i in the word bag B K as given by (4):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "S K = {s",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p K s i = F K i K k=1 F K k .",
                        "eq_num": "( 4 )"
                    }
                ],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "Here, F K i is the frequency of word s i in B K , i.e., the number of times that word s i has the appropriate role in one of the clauses (as defined in the previous section). To prevent infinite or zero scores, we assume that every word has a minimum frequency of 1. In the following, we denote the set of relative frequencies for the words from B K by p K , while p N is the set of relative frequencies for the words from B N , as captured by (5) :",
                "cite_spans": [
                    {
                        "start": 444,
                        "end": 447,
                        "text": "(5)",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p K = {p K s 1 , p K s 2 , . . . , P K s K }, p N = {p N s 1 , p N s 2 , . . . , p N s N }. (",
                        "eq_num": "5"
                    }
                ],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "The calculation of the novelty score for each word depends on whether s i \u2208 S K , s i \u2208 S N , or both, as shown in (6):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "Score(s i ) = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 p N s i p K s i if s k \u2208 S K \u2229 S N , 0 ifs k \u2208 S K \\ S N , \u221e if s k \u2208 S N \\ S K . (6)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "Here, p N s i and p K s i denote the estimated occurrence probabilities of the word s i from p N and p K , respectively. The score defines how much a word contributes in a sentence/document to make it novel. That is, a higher score signals higher novelty and vice versa. Figure 3 shows the resulting TM-based architecture and flow of information for the above scoring approach.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 278,
                        "end": 279,
                        "text": "3",
                        "ref_id": "FIGREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "Additionally, we also propose a contextual scoring approach to capture multiple word meanings determined by context. We presume that words that appear in the same clause are related semantically, and accordingly, we use clause co-occurrence of words to measure semantic relations. The intent is to differentiate between, for example, the meaning of \"apple\" in \"apple phone\" and the meaning of \"apple\" in \"apple fruit\". We achieve this through leveraging clauses that capture \"apple\" and \"phone\" in combination with other clauses that capture \"apple\" and \"fruit\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "The scoring is again performed in two steps:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "1. Rather than measuring the frequency of individual words, we now measure frequency of co-occurrence among the TM clauses. For instance, let us consider the word pair (s 1 , s 2 ) and novel class, associated with a total number of m clauses. The frequency of the word pair occurring together in the clauses is then given as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "p N s 1 ,s 2 = F N s 1 ,s 2 m . (",
                        "eq_num": "7"
                    }
                ],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "Here, F N s 1 ,s 2 is the number of times the word pair occur together across the m clauses of the novel class. 2. Finally, the contextual score for the word pair (s 1 , s 2 ) in class Novel can be defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Score N context (s 1 , s 2 ) = p N s 1 ,s 2 p N s 1 \u00d7 p N s 2 . (",
                        "eq_num": "8"
                    }
                ],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "Above, p N s 2 and p N s 1 are the individual frequencies of each word across the novel clauses, from the previous subsection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "Notice how the above score increases with lower individual frequencies and higher joint frequency, measuring ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Scoring word novelty",
                "sec_num": "4.2"
            },
            {
                "text": "We now demonstrate our novelty description approach, steb-by-step, using two example sentences from the sports domain. For illustration purposes, we consider the class Cricket to be Known and the class Rugby to be Novel.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "-Class : Cricket (Known)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "Text: England won the cricket match by hitting six in the last ball. Words: \"England\", \"won\", \"cricket\", \"match\", \"hit\", \"six\", \"ball\". -Class: Rugby (Novel)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "Text: England won the rugby match despite using old ball. Words: \"England\", \"won\", \"rugby\", \"match\", \"despite\", \"old\", \"ball\".",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "We first create the set of 10 unique words W = { \"England\", \"won\", \"cricket\", \"match\", \"hit\", \"six\", \"ball\", \"rugby\", \"despite\", \"old\"} from the words in the two sentences, each with a unique index o. From this set, we produce the input feature vector for the TM, X = [x 1 , x 2 , . . . , x 10 ].",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "Each propositional input x o in X refers to a particular word. Jointly, the propositional inputs are used to represent an input text. If a word w o \u2208 W is present in the document, the corresponding propositional input x o is set to 1, otherwise, it is set to 0.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "After TM training, we obtain a set of clauses, as examplified in Table 1 . The clauses",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 71,
                        "end": 72,
                        "text": "1",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "(C + 1 ) K , (C + 2 ) K , (C - 1 ) N , (C - 2 ) N vote for class Known, while (C - 1 ) K , (C - 2 ) K , (C + 1 ) N , (C + 2 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "N vote for class Novel. These clauses are then used to produce two bag-of-words, B K and B N . All the plain words in",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "(C + 1 ) K , (C + 2 ) K , (C - 1 ) N , (C - 2 ) N are placed in B K ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "while all the negated words are placed in B N . Since none of the words are negated in the clauses, we now have B K = (\"England\", \"cricket\", \"match\", \"hit\", \"six\", \"cricket\", \"six\", \"cricket\", \"won\", \"six\", \"ball\", \"cricket\", \"hit\", \"six\"). Correspondingly, all the plain words in",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "(C - 1 ) K , (C - 2 ) K , (C + 1 ) N , (C + 2 ) N are placed in B N ,",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "while all the negated words are placed in B K .",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "Within each bag-of-words, each word occurs with a certain frequency. For instance, the word \"match\" occurs once in B K and twice in B N . Notice that the total number of word occurrences are different for each class -14 words in class Known and 13 words in class Novel. Hence, the relative frequency for \"match\" in class Known becomes p K match = 1 14 = 0.071 while for class Novel it becomes p N match = 2 13 = 0.154. Table 2 lists the frequencies of the words per class.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 425,
                        "end": 426,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "We are now ready to calculate the novelty score for each word in W . Let us consider the word \"rugby\" from the novel word set and the word \"cricket\" from the known word set. For \"rugby\", we first calculate its relative frequency (4). In the bag-of-word B N for class Novel, \"rugby\" occurs four times, i.e., F N rugby = 4. Since we assume that a word has a minimum frequency of 1, we further have F K rugby = 1, despite \"rugby\" not appearing in the text from class Known.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "From Table 2 , we observe that the total word frequencies for the known and novel classes are 14 and 13, respectively. Hence, the relative frequencies for \"rugby\" becomes p rugby (K) = 0.307 for class Known and p rugby (N ) = 0.071 for class Novel (4).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 11,
                        "end": 12,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "Because the clauses characterize each class Known and Novel, notice how \"rugby\" gets the relatively high novelty score Score rugby = 4.651. That is, its relative frequency is high in the novel class and low in the known class. Conversely, the word \"cricket\" is repeated four times in B K and once in B N . Its relative frequencies thus becomes p cricket (K) = 0.28 for class Known and p cricket (N ) = 0.076 for class Novel. Accordingly, the novelty score becomes Score cricket = 0.271, which is a low score denoting a strong inclination of the word towards the known class.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "Overall, Table 2 shows how the words characterizing class Known get a relatively low novelty score, while those characterizing class Novel obtain high scores.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 15,
                        "end": 16,
                        "text": "2",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "Case study",
                "sec_num": "4.3"
            },
            {
                "text": "In this section, we evaluate our proposed novelty description approach on two publicly available datasets: BBC Sports and Twenty Newsgroups. The performance of the TM framework for novelty detection was previously investigated in [8] and is summarized in Table 3 . Notably, as has been found across several datasets, a one-class SVM on the simple mean embeddings established a strong baseline. Here, we further explore our model's effectiveness at producing discriminative novelty scores at the word level using TM clauses. To obtain robust performance and ensure that the results are not influenced by the data, we perform a one-class classification using leave-one-out evaluation on 20 Newsgroup dataset. This paper deals with the post-processing after novelty detection to deal with the novelty scoring at the word level. However, the leave-oneout evaluation is necessary because this study leverage the performance of the TM framework in terms of novelty detection. We employ the ROC AUC to quantify the novelty detection performance by using the ground truth labels during testing. Table 4 shows the performance comparison of our method and the baseline algorithms, including a one-class classifier. In the leave-one-out setup, one of the classes is considered a known class, while the remaining classes are treated as novel. The training is conducted using a known class, whereas testing is carried out on samples from a novel class. The ROC AUC is computed during testing with the assumption that the samples from the known class are labeled as y = 0 and from novel class as y = 1. Our method outperforms baselines algorithms in five out of six evaluation setups with a significant margin.",
                "cite_spans": [
                    {
                        "start": 230,
                        "end": 233,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 261,
                        "end": 262,
                        "text": "3",
                        "ref_id": "TABREF4"
                    },
                    {
                        "start": 1093,
                        "end": 1094,
                        "text": "4",
                        "ref_id": "TABREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Results and discussions",
                "sec_num": "5"
            },
            {
                "text": "(C + 1 ) K = \"England\" \u2227 \"cricket\" \u2227 \"match\" \u2227 \"hit\" \u2227 \"six\" (C + 1 ) N = \"England\" \u2227 \"won\" \u2227 \"rugby\" \u2227 \"old\" (C - 1 ) K = \"won\" \u2227 \"rugby\" \u2227 \"ball\" (C - 1 ) N = \"cricket\" \u2227 \"won\" \u2227 \"six\" \u2227 \"ball\" (C + 2 ) K = \"cricket\" \u2227 \"six\" (C + 2 ) N = \"rugby\" \u2227 \"match\" \u2227 \"despite\" \u2227 \"old\" (C - 2 ) K = \"rugby\" \u2227 \"match\" (C - 2 ) N = \"cricket\" \u2227 \"hit\" \u2227 \"six\"",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussions",
                "sec_num": "5"
            },
            {
                "text": "In the following, we compare the scoring mechanism of our framework with attention and TF-IDF as a baseline. To ensure a fair comparison, the attention score for each word is calculated as described in Section 5.1.1. For TF-IDF, We calculate TF separately for the known and novel classes. Conversely, IDF is calculated using all the documents from both classes (to suppress common words such as stop words). Unlike attention and TF-IDF, even if a word is present in most documents, our scoring considers both relevance and context. For example, if a word from class Novel also is present in class Known, our model can nevertheless assign more weight to that word. This happens when a word, while syntactically the same in both classes, acquires a novel meaning in the novel class due to its appearance in a novel context. The latter contextual information is captured through those clauses of the novel class that trigger for that word. As such, attention and TF-IDF are not context-aware. Moreover, these methods prove especially beneficial on more extensive datasets, such as 20 Newsgroups and BBC Sports, since they filter out general language contexts that are less discriminative for the characterization of a text corpus, making them a strong baseline for performance comparison.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussions",
                "sec_num": "5"
            },
            {
                "text": "To provide a comparison, we plot the cumulative frequency distribution (CFD) for the scores of (1) the words only found in the novel dataset, (2) the words only found in the known dataset, and (3) the words shared by both datasets. In brief, the CFD demonstrates that the word scores generated by the baseline are relatively similar for both known and novel classes. Thus, the baseline methods lack the discriminatory power necessary to distinguish between the two categories of words. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Results and discussions",
                "sec_num": "5"
            },
            {
                "text": "We utilize the weights from attention's layers input representation A of the trained model. The importance of each token is calculated based on the attention it receives. For instance, if attention to the token c \u2208 A is higher than the token d \u2208 A, then c is assumed to be \"more significant\" than d to the model's output. In our work, the scores are calculated using scaled-dot product attention mechanism [57] .",
                "cite_spans": [
                    {
                        "start": 406,
                        "end": 410,
                        "text": "[57]",
                        "ref_id": "BIBREF56"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "Let us consider an input sequence of length o, X = (x 1 , x 2 , . . . , x o ), where x i represents the i th token whose representation in the attention layer is h i \u2208 R t . The attention score for the i th token is as follows:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "\u03b1 i = h i \u00d7 V \u03b2 , (",
                        "eq_num": "9"
                    }
                ],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "where the parameter \u03b2 is the scaling factor, and V \u2208 R t is the context vector that can be seen as a fixed query requesting the \"most important token\" from input. Either the word embedding or the encoder's output can denote token representation h i . The attention weight can be expressed as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "a i = exp(a i ) i exp(a i ) . (",
                        "eq_num": "10"
                    }
                ],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "Finally, the complete input sequence is denoted as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "h = i (a i h i ). (",
                        "eq_num": "11"
                    }
                ],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "In our experiment, we retrieve the attention score and weights for each token using ( 9) and ( 10) respectively. We conducted experiments using scaled dot-product attention (DP ) and additive attention with varying scaling factors (\u03b2). The attention scores in our experiments are generated using a Long short-term memory (LSTM) with DP and an affine transformation layer as the input encoder. We used the Adagrad optimizer [19] for gradient descent and used dropout as regularization to prevent over-fitting. To eliminate the influence of prior knowledge, we learn all parameters from scratch and initialize the pre-trained word embeddings with a uniform distribution and dimension d = 100. A softmax function is applied over a linear layer for obtaining the final classification output. The readers are referred to [53] for a detailed theoretical explanation to generate the attention scores.",
                "cite_spans": [
                    {
                        "start": 423,
                        "end": 427,
                        "text": "[19]",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 816,
                        "end": 820,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Attention mechanism",
                "sec_num": "5.1.1"
            },
            {
                "text": "A commonly used method to analyze the importance of a word is the term frequency-inverse document frequency (TF-IDF) [45] . TF-IDF weighs each word to statistically measure the significance of the word in a given document. To this end, TF-IDF consists of two factors: normalized term frequency (TF) and inverse document frequency (IDF). TF measures the frequency of the word in the document, whereas IDF measures the uniqueness of the word across documents:",
                "cite_spans": [
                    {
                        "start": 117,
                        "end": 121,
                        "text": "[45]",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Term frequency-inverse document frequency (TF-IDF)",
                "sec_num": "5.1.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "T F -I DF s = F s F \u00d7 log 2 |D| |D s | + 1 . (",
                        "eq_num": "12"
                    }
                ],
                "section": "Term frequency-inverse document frequency (TF-IDF)",
                "sec_num": "5.1.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Term frequency-inverse document frequency (TF-IDF)",
                "sec_num": "5.1.2"
            },
            {
                "text": "Here, F s is the frequency of the word s in the target document, F is the sum of the target document word frequencies, |D| is the total number of documents, and |D s | is the number of documents containing the word s.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Term frequency-inverse document frequency (TF-IDF)",
                "sec_num": "5.1.2"
            },
            {
                "text": "Our method extracts keywords from known and novel classes based on the novelty scores. As a result, we also compare the significant words obtained by our method to those captured by existing keyword extraction (KE) algorithms. To do this, we first separate the text documents from known and novel classes before passing them to the KE algorithms. Additionally, we present the top 10 keywords captured by these algorithms. For the KE baselines mentioned below, we use the pke package [10] :",
                "cite_spans": [
                    {
                        "start": 483,
                        "end": 487,
                        "text": "[10]",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword extraction algorithms",
                "sec_num": "5.1.3"
            },
            {
                "text": "-TopicRank [12] : This is a graph-based KE method that depends on the extraction of the top-ranked topic. -YAKE [13] : A lightweight statistical approach for KE.",
                "cite_spans": [
                    {
                        "start": 11,
                        "end": 15,
                        "text": "[12]",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 112,
                        "end": 116,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword extraction algorithms",
                "sec_num": "5.1.3"
            },
            {
                "text": "-MultipartiteRank [11] : An unsupervised KE method for encoding topical information in a multipartite graph structure.",
                "cite_spans": [
                    {
                        "start": 18,
                        "end": 22,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword extraction algorithms",
                "sec_num": "5.1.3"
            },
            {
                "text": "-BERT-MMR [26] : A KE method that leverages Bidirectional Encoder Representations from Transformers (BERT) embeddings and Maximal Marginal Relevance (MMR).",
                "cite_spans": [
                    {
                        "start": 10,
                        "end": 14,
                        "text": "[26]",
                        "ref_id": "BIBREF25"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Keyword extraction algorithms",
                "sec_num": "5.1.3"
            },
            {
                "text": "We use the accuracy, Receiver Operating Characteristics (ROC) curve, precision, and recall to evaluate the performance of novelty detection using word scores obtained from the proposed method. In general, accuracy is a well-known parameter to measure the effectiveness of novelty detection models, which indicates the percentage of correct prediction by a model in a test set. The accuracy is calculated by:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Accuracy = T P + T N T P + T N + F P + F N , (",
                        "eq_num": "13"
                    }
                ],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "where T P , T N , F P , F N denotes the samples that are correct novel, correct normal, incorrect novel, and incorrect normal respectively. And P , N denotes the total novel and normal samples. The precision is defined by the percentage of correctly identified novel samples and is give as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "P recision = T P T P + F P . (",
                        "eq_num": "14"
                    }
                ],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "Recall is the percentage of the real novel samples identified and is given as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "Recall = T P T P + F N . (",
                        "eq_num": "15"
                    }
                ],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "In general, the higher the precision and the recall, the better the algorithm. However, the precision and recall are mutually constrained. For example, if only one novel sample is detected, the precision is 100%, while the recall is very low. And if all samples detected are novel, the recall will be 100%, while precision tends to be very low. Therefore, we present precision-recall graph in our evaluation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "The ROC is insensitive to the number of novel samples and is calculated by plotting all potential choices of the T P rate (the portion of novel data ranked among the total novel data) against the F P rate (the portion of normal data ranked among the total novel data). The ROC curve can be summarized using a single value defined as the area under ROC curve (AUC). The ROC value ranges between 0 and 1 and is regarded as average of the recall. The perfect detection of all test samples would result in ROC value of 1, whereas the null detection would result in ROC value of 0. In general, the greater the ROC AUC value, the better the algorithm. [27] established that the ROC AUC value corresponds to the probability of a pair (nov, nor), where nov is certain true novel samples and nor is certain true normal samples. The ROC AUC can then be defined by:",
                "cite_spans": [
                    {
                        "start": 646,
                        "end": 650,
                        "text": "[27]",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "ROC AU C = \u23a7 \u23a8 \u23a9 1, if score(nov) > score(nor), 0, if score(nov) < score(nor), 1/2, if score(nov) = score(nor).",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "Therefore, the ROC AUC has a direct probabilistic interpretation. The AUC can be also be defined as:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "AU C = 1 0 ROC(T ) d T , (",
                        "eq_num": "16"
                    }
                ],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": ")",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "where T denotes a threshold to control novel samples. The ROC AUC is the most often used evaluation metric for novelty detection that provides a ranking. Therefore, in this paper, we compare the evaluation alongside other methods, so it can give different aspects of the performance. To ensure fairness, effectiveness and reproducibility of the evaluation results, we use scikit-plot library1 to compute ROC and precision-recall graphs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Evaluation measures",
                "sec_num": "5.2"
            },
            {
                "text": "The BBC sports dataset comprises 737 documents from the BBC sport website organized in five sports article categories, collected from 2004 to 2005. The resulting vocabulary encompasses 4 613 terms. For our experiment, we consider the classes \"cricket\" and \"football\" to be known and the class \"rugby\" to be novel, thus creating an unbalanced dataset. For preprocessing, we perform tokenization, stopword removal, and lemmatization. We run the TM for 100 epochs with 10 000 clauses, a voting margin T of 50, and a sensitivity s of 25.0. We present overall novelty score statistics for the words captured by the clauses in Table 5 . The table demonstrates that words in the class Novel have distinctively higher average scores than words in the class Known. Also, notice that the shared words have the highest mean and standard deviation. As analyzed further below, this is the case because the TM will mainly use those words when forming the decision boundary between the two classes. As a result, the shared words will appear in more clauses as characterizing class features. That is, the clauses will either single out the words in one class or suppress the words in the other class.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 627,
                        "end": 628,
                        "text": "5",
                        "ref_id": "TABREF6"
                    }
                ],
                "eq_spans": [],
                "section": "BBC sports dataset",
                "sec_num": "5.3"
            },
            {
                "text": "To gain further insight into the properties of the novelty score, we plot the CFD for the scores of the novel, known, and shared words in Fig. 4 . We further compare these CFDs with the corresponding ones obtained using attention weights in Fig. 5 and TF-IDF in Fig. 10 . As can be observed from the plot, our approach produces more distinctive novelty scores than both attention and TF-IDF. The novel words typically produce high scores, while the known words produce low scores. In particular, as shown in Fig. 4a , 85% of the known words output scores lower than 1.0. On the other hand, as seen in Fig. 4b , only approximately 45% of the words unique for the novel class have scores below 1.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 143,
                        "end": 144,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 246,
                        "end": 247,
                        "text": "5",
                        "ref_id": null
                    },
                    {
                        "start": 267,
                        "end": 269,
                        "text": "10",
                        "ref_id": "FIGREF0"
                    },
                    {
                        "start": 513,
                        "end": 515,
                        "text": "4a",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 606,
                        "end": 608,
                        "text": "4b",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "BBC sports dataset",
                "sec_num": "5.3"
            },
            {
                "text": "The majority of the uniquely novel words produce scores greater than 1.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "BBC sports dataset",
                "sec_num": "5.3"
            },
            {
                "text": "We plot the TM and attention scores for each token in Fig. 6b and a , respectively. Due to the large span of the TM scores, the y-axis is plotted on a log scale. Nonetheless, we note that the scores are structured in successive layers, with known scores at the bottom, novel scores at the top, and shared scores in the center. We notice that even the attention score demonstrates a small degree of differentiation between known and novel categories. However, the variability of the score is quite low when compared to the score generated by TM as seen in Fig. 7 boxplot. Additionally, the shared word scores produced by the attention mechanism exhibit a high degree of resemblance to known word scores.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 59,
                        "end": 61,
                        "text": "6b",
                        "ref_id": "FIGREF4"
                    },
                    {
                        "start": 66,
                        "end": 67,
                        "text": "a",
                        "ref_id": null
                    },
                    {
                        "start": 560,
                        "end": 561,
                        "text": "7",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "BBC sports dataset",
                "sec_num": "5.3"
            },
            {
                "text": "Finally, we plot the scores for words that are shared between the known and novel classes in Fig. 4c . As can be observed, the words that are shared produce both high and low scores. To cast further light on this observation, we investigate the words that are shared further in Table 6 . We see that the words captured frequently by novel clauses have high scores, whereas the words that are frequent in known clauses have low scores. Additionally, common words (e.g., stopwords), also have low scores. For example, the word \"Rugby\", which is highly characteristic for class Novel, is repeated only 5 times in the clauses representing class Known. For the clauses that represent class Novel, on the other hand, it is repeated 215 times. In other words, the shared words constitute words that are either characteristic for class Known or class Novel. This finding also suggests that the scores can be calculated accurately even if the words are present in both categories. We analyze the most frequently used words to obtain an intuition of the overall theme captured by the clauses. We generate such lists by counting the top words according to the highest scores from known and novel classes. Such a list may assist a user in weighting and selecting relevant words in a specific application. Tables 7 and 8 show an example of a top word list for each class, from which we make the following observations. First, our proposed method assigns low scores to words belonging to known classes while assigning comparatively high scores to words belonging to the novel class. In general, the words that appear in a novel context are and MultipartiteRank all yield words with a high degree of similarity to our approach. Additionally, we notice that BERT-MMR exhibits the worst performance. This might be due to the fact that we utilized pre-trained sentence embedding for BERT, and the keywords are extracted from overall documents. Even though the words are not highly relevant to the classes, BERT is capable of producing words relating to the class's general theme. For example, sportsrelated words are included in both classes. We now investigate the degree of discrimination power our novelty scoring provides, and therefore uniquely describes novelty at the word level. To this end, we employ logistic regression for classifying novel text based on the word scores obtained from our method. The ROC and precision-recall curves of the experiment are depicted in Fig. 8 for our novelty scoring mechanism. Our method provides the competitive ROC value due to its ability to discriminate novel samples based on their scores. This capability enables our method to acquire a higher true positive T P rate since it makes separate analysis of both correct novel, i.e., true positive T P and correct normal, i.e., true negative T N . Figures 9a, 10, and 11 contains corresponding curves when TF-IDF and attention scores are used instead. We see that the classification performance for our novelty scores is substantially better than what is obtained with TF-IDF. Attention score outperforms our approach for BBC Sports dataset by a small percentage. However, our approach outperforms attention in 20 Newsgroups. This can be attributed to the capability of our approach to deal with a big dataset. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 98,
                        "end": 100,
                        "text": "4c",
                        "ref_id": "FIGREF3"
                    },
                    {
                        "start": 284,
                        "end": 285,
                        "text": "6",
                        "ref_id": "TABREF7"
                    },
                    {
                        "start": 1300,
                        "end": 1301,
                        "text": "7",
                        "ref_id": "TABREF8"
                    },
                    {
                        "start": 1306,
                        "end": 1307,
                        "text": "8",
                        "ref_id": "TABREF9"
                    },
                    {
                        "start": 2465,
                        "end": 2466,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "BBC sports dataset",
                "sec_num": "5.3"
            },
            {
                "text": "The 20 Newsgroups dataset contains a total of 18 828 documents partitioned equally into 20 separate classes. In our experiments, we treat the two classes \"comp.graphics\" and \"talk.politics.guns\" as Known topics, and then use the class \"rec.sport.baseball\" to represent a Novel topic. Again, we train a TM to produce our clause-based novelty scores. The overall statistics of the resulting word scores are shown in Tables 9 and 10 , where we observe similar behavior to that observed with the BBC Sports dataset. The CFD plot in Fig. 12 presents the score distribution among words per group (known, novel, shared). For known words, in Fig. 12a , we find that 90% of the scores of the words are below around 1.3. In Fig. 12b , however, only 45% of the novel word scores fall below approx. 1.3. From the plots, it is evident that the majority of the novel words have considerably higher scores than the known words. Note that some of the novel word's low scores are attributable to the presence of common words (e.g., stop words) in the novel bag-of-words. Since the common words, as such, do not signify novelty, the TM clauses do not frequently capture them. As a result, they receive relatively low scores despite their appearance among the novel documents.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 421,
                        "end": 422,
                        "text": "9",
                        "ref_id": "TABREF10"
                    },
                    {
                        "start": 427,
                        "end": 429,
                        "text": "10",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 533,
                        "end": 535,
                        "text": "12",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 639,
                        "end": 642,
                        "text": "12a",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 719,
                        "end": 722,
                        "text": "12b",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "20 newsgroups dataset",
                "sec_num": "5.4"
            },
            {
                "text": "The CFD plot for attention and TF-IDF both exhibit similar behaviour to that of BBC Sports, as seen in Fig. 13 and 18, respectively. Finally, we again observe that the clauses have used the shared words for discrimination (cf. Table 10 ), resulting in a mix of low and high novelty scores, as shown in Fig. 12c . 14 and 15 . Again, we observe a similar behavior as for the BBC Sports dataset. The ROC and precision-recall curves for our novelty scoring mechanism are illustrated in Figs. 16a, 17a, 18, and 19a include corresponding graphs when TF-IDF and attention scores are used instead. Our method outperforms the ROC value obtained from attention because of its ability to identify more number of correct novel samples, i.e., true positives T P . However, the TF-IDF surprisingly outperforms both of the methods because of its straightforward scoring system and the dataset's moderate size. We can see that our scoring approach outperforms the baselines by a wide margin.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 108,
                        "end": 110,
                        "text": "13",
                        "ref_id": "FIGREF2"
                    },
                    {
                        "start": 233,
                        "end": 235,
                        "text": "10",
                        "ref_id": "TABREF2"
                    },
                    {
                        "start": 307,
                        "end": 310,
                        "text": "12c",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 313,
                        "end": 315,
                        "text": "14",
                        "ref_id": "FIGREF8"
                    },
                    {
                        "start": 320,
                        "end": 322,
                        "text": "15",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "20 newsgroups dataset",
                "sec_num": "5.4"
            },
            {
                "text": "We also implement a context-based scoring approach to investigate how multiple words interact to capture novelty. As detailed in Section 4, we compute the combined novelty score by measuring word co-occurrence in clauses. That is, we intend to demonstrate how context can help uncover novelty when words have multiple meanings. The contextbased scoring is critical since the context can transform the word from being novel to known, such as the meaning of the word \"apple\" in \"apple fruit\" and \"apple phone\". For demonstration, we calculate our proposed contextbased novelty score for five words (i.e., two known, two novel, and one common word) in both datasets. For the BBC Sports dataset, the pairwise co-occurrence scores are presented in Table 13 . We see a significant degree of correspondence between words such as \"Manchester\" and \"Chelsea\" from class Known. Similarly, there is a high correspondence between words such as \"Rugby\" and \"Flyhalf\" from class Novel. The common word \"Particular\", on the other hand, shows similar correspondence with words from both of the classes. Similarly, for the 20 Newsgroups dataset, the co-occurrence scores for five words selected from the known, novel, and common word types are shown in Table 14 . The words \"Guns\" and \"Weapon\" are from class Known and manifest strong co-occurrence. Additionally the words \"Baseball\" and \"Player\" from class Novel correspond strongly as well. The common word \"Gather\", on the other hand, co-occurs within both of the classes. These examples demonstrate that the words that are most likely to appear in the same context have a high co-occurrence score. This can be explained by the fact that many clauses capture words that frequently occur together in a similar context. We compare the contextual scores obtained from our method with the Word2Vec similarity score. To do this, we utilize Gensim library to train custom Word2Vec on both datasets. Gensim library enabled us to create word embeddings by training own Word2Vec models on a custom corpus using either CBOW or skip-grams algorithms. Parameterwise, we used an embedding size of 200 and a window size of 5. We compute the cosine similarity between words by using their word vectors (embeddings). The findings are included in Tables 15 and 16 . We notice a significant degree of resemblance between the corresponding words from the known and the novel classes. However, unlike our method, the similarity scores are less distinct, and the common words are not discernible score-wise.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 749,
                        "end": 751,
                        "text": "13",
                        "ref_id": "TABREF13"
                    },
                    {
                        "start": 1241,
                        "end": 1243,
                        "text": "14",
                        "ref_id": "TABREF14"
                    },
                    {
                        "start": 2272,
                        "end": 2274,
                        "text": "15",
                        "ref_id": "TABREF15"
                    },
                    {
                        "start": 2279,
                        "end": 2281,
                        "text": "16",
                        "ref_id": "TABREF17"
                    }
                ],
                "eq_spans": [],
                "section": "Contextual scoring",
                "sec_num": "5.5"
            },
            {
                "text": "In this work, we propose a Tsetlin Machine (TM)-based solution for word-level novelty description. First, we employ the clauses from a trained TM to capture how the most significant words differentiate a group of novel documents apart from a group of known documents. Then, we calculate the score for each word based on the role it plays in the clauses. The analysis of our empirical results for BBC Sports and 20 Newsgroups demonstrate significantly better novelty discrimination power when compared to using attention and TF-IDF. Our empirical results also show that we can capture word relations through a contextual scoring mechanism that measures cooccurrence within TM clauses. By capturing non-linear relationships among words, we can enhance the capability of measuring novelty at the word level. However, training a TM is computationally more expensive than calculating TF-IDF, particularly for large datasets with an extensive vocabulary. We will address computation speed in our future work, employing indexing mechanisms and exploiting feature space sparsity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "6"
            },
            {
                "text": "https://scikit-plot.readthedocs.io/en/stable/Quickstart.html.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "Funding Open access funding provided by University of Agder.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "funding",
                "sec_num": null
            },
            {
                "text": "The authors declare that they have no conflict of interest.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons. org/licenses/by/4.0/. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conflict of Interests",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Progress in Artificial Intelligence, 19th EPIA Conference on Artificial Intelligence, EPIA",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "D"
                        ],
                        "last": "Abeyrathna",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Goodwin",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings, Part II",
                "volume": "11805",
                "issue": "",
                "pages": "268--280",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-030-30244-3_23"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Abeyrathna KD, Granmo O, Jiao L, Goodwin M (2019) The regression tsetlin machine: A tsetlin machine for con- tinuous output problems. In: Oliveira PM, Novais P, Reis LP (eds) Progress in Artificial Intelligence, 19th EPIA Con- ference on Artificial Intelligence, EPIA 2019, Vila Real, Por- tugal, September 3-6, 2019, Proceedings, Part II, Springer, Lecture Notes in Computer Science, vol 11805, pp 268-280. https://doi.org/10.1007/978-3-030-30244-3 23",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Massively parallel and asynchronous tsetlin machine architecture supporting almost constant-time scaling",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "D"
                        ],
                        "last": "Abeyrathna",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Bhattarai",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Goodwin",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "R"
                        ],
                        "last": "Gorji",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Saha",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "K"
                        ],
                        "last": "Yadav",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021",
                "volume": "139",
                "issue": "",
                "pages": "10--20",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Abeyrathna KD, Bhattarai B, Goodwin M, Gorji SR, Granmo O, Jiao L, Saha R, Yadav RK (2021) Massively parallel and asynchronous tsetlin machine architecture supporting almost constant-time scaling. In: Meila M, Zhang T (eds) Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, PMLR, Proceedings of Machine Learning Research, vol 139, pp 10-20. http:// proceedings.mlr.press/v139/abeyrathna21a.html",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "An introduction to outlier analysis",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "C"
                        ],
                        "last": "Aggarwal",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Outlier analysis",
                "volume": "",
                "issue": "",
                "pages": "1--34",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-319-47578-3_1"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aggarwal CC (2017) An introduction to outlier analysis. In: Outlier analysis. Springer International Publishing, pp 1-34. https://doi.org/10.1007/978-3-319-47578-3 1",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "On-line new event detection and tracking",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Allan",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Papka",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Lavrenko",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "SIGIR '98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "37--45",
                "other_ids": {
                    "DOI": [
                        "10.1145/290941.290954"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Allan J, Papka R, Lavrenko V (1998) On-line new event detection and tracking. In: Croft WB, Moffat A, van Rijsbergen CJ, Wilkin- son R, Zobel J (eds) SIGIR '98: Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Devel- opment in Information Retrieval, August 24-28, 1998. ACM, Australia, pp 37-45. https://doi.org/10.1145/290941.290954",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Towards open set deep networks",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bendale",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "E"
                        ],
                        "last": "Boult",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "1563--1572",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2016.173"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bendale A, Boult TE (2016) Towards open set deep net- works. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, IEEE Computer Society, pp 1563-1572. https://doi.org/10.1109/CVPR.2016.173",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "The seventh PASCAL recognizing textual entailment challenge",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Bentivogli",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Dagan",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Giampiccolo",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of the Fourth Text Analysis Conference",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bentivogli L, Clark P, Dagan I, Giampiccolo D (2011) The seventh PASCAL recognizing textual entailment chal- lenge. In: Proceedings of the Fourth Text Analysis Confer- ence, TAC 2011, Gaithersburg, Maryland, USA, November 14- 15, 2011, NIST. https://tac.nist.gov/publications/2011/additional. papers/RTE7 overview.proceedings.pdf",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Using the tsetlin machine to learn human-interpretable rules for high-accuracy text categorization with medical applications",
                "authors": [
                    {
                        "first": "G",
                        "middle": [
                            "T"
                        ],
                        "last": "Berge",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "O"
                        ],
                        "last": "Tveit",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Goodwin",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "V"
                        ],
                        "last": "Matheussen",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "IEEE Access",
                "volume": "7",
                "issue": "",
                "pages": "115134--115146",
                "other_ids": {
                    "DOI": [
                        "10.1109/ACCESS.2019.2935416"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Berge GT, Granmo O, Tveit TO, Goodwin M, Jiao L, Matheussen BV (2019) Using the tsetlin machine to learn human-interpretable rules for high-accuracy text categorization with medical applications. IEEE Access 7:115134-115146. https://doi.org/10.1109/ACCESS.2019.2935416",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Measuring the novelty of natural language text using the conjunctive clauses of a tsetlin machine text classifier",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Bhattarai",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 13th International Conference on Agents and Artificial Intelligence",
                "volume": "2021",
                "issue": "",
                "pages": "410--417",
                "other_ids": {
                    "DOI": [
                        "10.5220/0010382204100417"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Bhattarai B, Granmo O, Jiao L (2021) Measuring the novelty of natural language text using the conjunctive clauses of a tsetlin machine text classifier. In: Rocha AP, Steels L, van den Herik HJ (eds) Proceedings of the 13th International Conference on Agents and Artificial Intelligence, ICAART 2021, Volume 2, Online Streaming, February 4-6, 2021, SCITEPRESS, pp 410- 417. https://doi.org/10.5220/0010382204100417",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Semi-supervised novelty detection",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Blanchard",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Scott",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "J Mach Learn Res",
                "volume": "11",
                "issue": "",
                "pages": "2973--3009",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Blanchard G, Lee G, Scott C (2010) Semi-supervised novelty detection. J Mach Learn Res 11:2973-3009. http://portal.acm.org/ citation.cfm?id=1953028",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Pke: an open source python-based keyphrase extraction toolkit",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Boudin",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "69--73",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Boudin F (2016) Pke: an open source python-based keyphrase extraction toolkit. In: Watanabe H (ed) COLING 2016, 26th International Conference on Computational Linguistics, Proceed- ings of the Conference System Demonstrations, December 11-16, 2016, Osaka, Japan, ACL, pp 69-73. https://aclanthology.org/ C16-2015/",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Unsupervised keyphrase extraction with multipartite graphs",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Boudin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT",
                "volume": "2",
                "issue": "",
                "pages": "667--672",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/n18-2105"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Boudin F (2018) Unsupervised keyphrase extraction with mul- tipartite graphs. In: Walker MA, Ji H, Stent A (eds) Proceed- ings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT, New Orleans, Louisiana, USA, June 1-6, 2018 Short Papers, vol 2. Association for Computational Linguistics, pp 667-672. https://doi.org/10.18653/v1/n18-2105",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Topicrank: Graph-based topic ranking for keyphrase extraction",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bougouin",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Boudin",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Daille",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Sixth International Joint Conference on Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "543--551",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bougouin A, Boudin F, Daille B (2013) Topicrank: Graph-based topic ranking for keyphrase extraction. In: Sixth International Joint Conference on Natural Language Processing, IJCNLP 2013, Nagoya, Japan, October 14-18, 2013, Asian Federation of Natural Language Processing / ACL, pp 543-551. https://aclanthology. org/I13-1062/",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Yake! keyword extraction from single documents using multiple local features",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Campos",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Mangaravite",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Pasquali",
                        "suffix": ""
                    },
                    {
                        "first": "Jorge",
                        "middle": [
                            "A"
                        ],
                        "last": "Nunes",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Jatowt",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Inf Sci",
                "volume": "509",
                "issue": "",
                "pages": "257--289",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.ins.2019.09.013"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Campos R, Mangaravite V, Pasquali A, Jorge A, Nunes C, Jatowt A (2020) Yake! keyword extraction from single documents using multiple local features. Inf Sci 509:257-289. https://doi.org/10. 1016/j.ins.2019.09.013",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "The use of mmr, diversity-based reranking for reordering documents and producing summaries",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Carbinell",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Goldstein",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "SIGIR Forum",
                "volume": "51",
                "issue": "2",
                "pages": "209--210",
                "other_ids": {
                    "DOI": [
                        "10.1145/3130348.3130369"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Carbinell J, Goldstein J (2017) The use of mmr, diversity-based reranking for reordering documents and producing summaries. SIGIR Forum 51(2):209-210. https://doi.org/10.1145/3130348. 3130369",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Anomaly detection for discrete sequences: a survey",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Chandola",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "IEEE Trans Knowl Data Eng",
                "volume": "24",
                "issue": "5",
                "pages": "823--839",
                "other_ids": {
                    "DOI": [
                        "10.1109/TKDE.2010.235"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Chandola V, Banerjee A, Kumar V (2012) Anomaly detection for discrete sequences: a survey. IEEE Trans Knowl Data Eng 24(5):823-839. https://doi.org/10.1109/TKDE.2010.235",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "A comparison of negative and positive selection algorithms in novel pattern detection",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Dasgupta",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Nino",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Cybernetics evolving to systems, humans, organizations, and their complex interactions, sheraton music city hotel",
                "volume": "",
                "issue": "",
                "pages": "125--130",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICSMC.2000.884976"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Dasgupta D, Nino L (2000) A comparison of negative and posi- tive selection algorithms in novel pattern detection, Proceedings of the IEEE International Conference on Systems, Man & Cyber- netics. In: Cybernetics evolving to systems, humans, organiza- tions, and their complex interactions, sheraton music city hotel, nashville, tennessee, USA. 8-11 October, 2000. IEEE, pp 125- 130. https://doi.org/10.1109/ICSMC.2000.884976",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Automatic scoring for innovativeness of textual ideas",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Dasgupta",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Dey",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Papers from the 2016 AAAI Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dasgupta T, Dey L (2016) Automatic scoring for innovativeness of textual ideas. In: Fortuna B, Grobelnik M, ERH Jr, Witbrock MJ (eds) Knowledge Extraction from Text, Papers from the 2016 AAAI Workshop, Phoenix, Arizona, USA, February 12, 2016, AAAI Press, AAAI Workshops, vol WS-16-10. http://www.aaai. org/ocs/index.php/WS/AAAIW16/paper/view/12663",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "BERT: Pretraining of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "volume": "",
                "issue": "",
                "pages": "4171--4186",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/n19-1423"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Devlin J, Chang M, Lee K, Toutanova K (2019) BERT: Pre- training of deep bidirectional transformers for language under- standing. In: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019 vol 1 (Long and Short Papers). Association for Computational Linguistics, pp 4171-4186. https://doi.org/10. 18653/v1/n19-1423",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Adaptive subgradient methods for online learning and stochastic optimization",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Duchi",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Hazan",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Singer",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "J Mach Learn Res",
                "volume": "12",
                "issue": "",
                "pages": "2121--2159",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Duchi JC, Hazan E, Singer Y (2011) Adaptive subgradient methods for online learning and stochastic optimization. J Mach Learn Res 12:2121-2159. http://dl.acm.org/citation.cfm? id=2021068",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Social media text classification under negative covariate shift",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Fei",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Pighin",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Marton",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015",
                "volume": "",
                "issue": "",
                "pages": "2347--2356",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/d15-1282"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fei G, Liu B, Callison-burch C, Su J, Pighin D, Marton Y (2015) Social media text classification under negative covari- ate shift. In: M\u00e1rquez L (ed) Proceedings of the 2015 Con- ference on Empirical Methods in Natural Language Process- ing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, The Association for Computational Linguistics, pp 2347-2356. https://doi.org/10.18653/v1/d15-1282",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "The 2016 conference of the north american chapter of the association for computational linguistics: Human language technologies, san diego california",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Fei",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "NAACL HLT 2016",
                "volume": "",
                "issue": "",
                "pages": "506--514",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/n16-1061"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Fei G, Liu B (2016) Breaking the closed world assumption in text classification. In: Knight K, Nenkova A, Rambow O (eds) NAACL HLT 2016, The 2016 conference of the north american chapter of the association for computational linguistics: Human language technologies, san diego california, USA, June 12-17, 2016, The Association for Computational Linguistics, pp 506- 514. https://doi.org/10.18653/v1/n16-1061",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Attention in natural language processing",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Galassi",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Lippi",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Torroni",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "IEEE Trans Neural Networks Learn Syst",
                "volume": "32",
                "issue": "10",
                "pages": "4291--4308",
                "other_ids": {
                    "DOI": [
                        "10.1109/TNNLS.2020.3019893"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Galassi A, Lippi M, Torroni P (2021) Attention in natu- ral language processing. IEEE Trans Neural Networks Learn Syst 32(10):4291-4308. https://doi.org/10.1109/TNNLS.2020. 3019893",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Generative adversarial nets",
                "authors": [
                    {
                        "first": "I",
                        "middle": [
                            "J"
                        ],
                        "last": "Goodfellow",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Pouget-Abadie",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Warde",
                        "middle": [
                            "-"
                        ],
                        "last": "Farley",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ozair",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Courville",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "C"
                        ],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "2672--2680",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville AC, Bengio Y (2014) Generative adver- sarial nets. In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ (eds) Advances in Neural Information Pro- cessing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Que- bec, Canada, pp 2672-2680. https://proceedings.neurips.cc/paper/ 2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "The tsetlin machine -A game theoretic bandit driven approach to optimal pattern recognition with propositional logic",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1804.01508"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Granmo O (2018) The tsetlin machine -A game theoretic bandit driven approach to optimal pattern recognition with propositional logic. arXiv:1804.01508",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Keybert: Minimal keyword extraction with bert",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Grootendorst",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.5281/zenodo.4461265"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Grootendorst M (2020) Keybert: Minimal keyword extraction with bert. https://doi.org/10.5281/zenodo.4461265",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "The meaning and use of the area under a receiver operating characteristic (roc) curve",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "A"
                        ],
                        "last": "Hanley",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "J"
                        ],
                        "last": "Mcneil",
                        "suffix": ""
                    }
                ],
                "year": 1982,
                "venue": "Radiology",
                "volume": "143",
                "issue": "1",
                "pages": "29--36",
                "other_ids": {
                    "DOI": [
                        "10.1148/radiology.143.1.7063747"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hanley JA, McNeil BJ (1982) The meaning and use of the area under a receiver operating characteristic (roc) curve. Radiology 143(1):29-36. https://doi.org/10.1148/radiology.143.1.7063747",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Outlier detection using k-nearest neighbour graph",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Hautam\u00e4ki",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "K\u00e4rkk\u00e4inen",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Fr\u00e4nti",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "17th International Conference on Pattern Recognition, ICPR 2004",
                "volume": "",
                "issue": "",
                "pages": "430--433",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICPR.2004.1334558"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hautam\u00e4ki V, K\u00e4rkk\u00e4inen I, Fr\u00e4nti P (2004) Outlier detection using k-nearest neighbour graph. In: 17th International Con- ference on Pattern Recognition, ICPR 2004, Cambridge, UK, August 23-26, 2004, IEEE Computer Society, pp 430-433. https://doi.org/10.1109/ICPR.2004.1334558",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Identification of outliers, Monographs on Applied Probability and Statistics",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "M"
                        ],
                        "last": "Hawkins",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-94-015-3994-4"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hawkins DM (1980) Identification of outliers, Monographs on Applied Probability and Statistics. Springer, Berlin. https://doi. org/10.1007/978-94-015-3994-4",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Hendrycks",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Gimpel",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "5th International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hendrycks D, Gimpel K (2017) A baseline for detecting misclassified and out-of-distribution examples in neural net- works. In: 5th International Conference on Learning Represen- tations, ICLR 2017, Toulon, France, April 24-26, 2017, Confer- ence Track Proceedings, OpenReview.net. https://openreview.net/ forum?id=Hkg4TI9xl",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "The 2016 conference of the north american chapter of the association for computational linguistics: Human language technologies, san diego california",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Hill",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Cho",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Korhonen",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "The Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1367--1377",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/n16-1162"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hill F, Cho K, Korhonen A (2016) Learning distributed represen- tations of sentences from unlabelled data. In: Knight K, Nenkova A, Rambow O (eds) NAACL HLT 2016, The 2016 conference of the north american chapter of the association for computational linguistics: Human language technologies, san diego california, USA, June 12-17, 2016, The Association for Computational Lin- guistics, pp 1367-1377. https://doi.org/10.18653/v1/n16-1162",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Finding rare classes: Adapting generative and discriminative models in active learning",
                "authors": [
                    {
                        "first": "T",
                        "middle": [
                            "M"
                        ],
                        "last": "Hospedales",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Gong",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Xiang",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Advances in Knowledge Discovery and Data Mining -15th Pacific-Asia Conference, PAKDD 2011",
                "volume": "6635",
                "issue": "",
                "pages": "296--308",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-642-20847-8_25"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Hospedales TM, Gong S, Xiang T (2011) Finding rare classes: Adapting generative and discriminative models in active learning. In: Huang JZ, Cao L, Srivastava J (eds) Advances in Knowledge Discovery and Data Mining -15th Pacific-Asia Conference, PAKDD 2011, Shenzhen, China, May 24-27, 2011, Proceedings, Part II, Springer, Lecture Notes in Computer Science, vol 6635, pp 296-308. https://doi.org/10.1007/978-3-642-20847-8 25",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Multi-class open set recognition using probability of inclusion",
                "authors": [
                    {
                        "first": "L",
                        "middle": [
                            "P"
                        ],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [
                            "J"
                        ],
                        "last": "Scheirer",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "E"
                        ],
                        "last": "Boult",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Computer Vision -ECCV 2014 -13th European Conference",
                "volume": "8691",
                "issue": "",
                "pages": "393--409",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-319-10578-9_26"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jain LP, Scheirer WJ, Boult TE (2014) Multi-class open set recognition using probability of inclusion. In: Fleet DJ, Pajdla T, Schiele B, Tuytelaars T (eds) Computer Vision - ECCV 2014 -13th European Conference, Zurich, Switzer- land, September 6-12, 2014, Proceedings, Part III, Springer, Lecture Notes in Computer Science, vol 8691, pp 393-409. https://doi.org/10.1007/978-3-319-10578-9 26",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Attention is not explanation",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "C"
                        ],
                        "last": "Wallace",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019",
                "volume": "",
                "issue": "",
                "pages": "3543--3556",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/n19-1357"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jain S, Wallace BC (2019) Attention is not explanation. In: Burstein J, Doran C, Solorio T (eds) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, vol 1 (Long and Short Papers). Association for Computational Linguistics, pp 3543-3556. https://doi.org/10.18653/v1/n19-1357",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "On the convergence of tsetlin machines for the XOR operator",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [
                            "D"
                        ],
                        "last": "Abeyrathna",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2101.02547"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Jiao L, Zhang X, Granmo O, Abeyrathna KD (2021) On the convergence of tsetlin machines for the XOR operator. arXiv:2101.02547",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Skip-thought vectors",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Kiros",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Salakhutdinov",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "S"
                        ],
                        "last": "Zemel",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Urtasun",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Fidler",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015",
                "volume": "",
                "issue": "",
                "pages": "3294--3302",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kiros R, Zhu Y, Salakhutdinov R, Zemel RS, Urtasun R, Torralba A, Fidler S (2015) Skip-thought vectors. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, Garnett R (eds) Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Sys- tems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pp 3294-3302. https://proceedings.neurips.cc/paper/2015/hash/ f442d33fa06832082290ad8544a8da27-Abstract.html",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Novelty detection with GAN",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Kliger",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Fleishman",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1802.10560"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kliger M, Fleishman S (2018) Novelty detection with GAN. arXiv:1802.10560",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Text classification and named entities for new event detection",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Kumaran",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Allan",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "SIGIR 2004: Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "297--304",
                "other_ids": {
                    "DOI": [
                        "10.1145/1008992.1009044"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Kumaran G, Allan J (2004) Text classification and named enti- ties for new event detection. In: Sanderson M, J\u00e1rvelin K, Allan J, Bruza P (eds) SIGIR 2004: Proceedings of the 27th Annual Inter- national ACM SIGIR Conference on Research and Development in Information Retrieval, Sheffield, UK, July 25-29, 2004, ACM, pp 297-304. https://doi.org/10.1145/1008992.1009044",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Testing statistical hypotheses",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lann E",
                        "suffix": ""
                    }
                ],
                "year": 1959,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "LANN E (1959) Testing statistical hypotheses. Wiley, New York",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "COPOD: Copulabased outlier detection",
                "authors": [
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Botta",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Ionescu",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "20Th IEEE international conference on data mining, ICDM 2020",
                "volume": "",
                "issue": "",
                "pages": "1118--1123",
                "other_ids": {
                    "DOI": [
                        "10.1109/ICDM50108.2020.00135"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Li Z, Zhao Y, Botta N, Ionescu C, Hu X (2020) COPOD: Copula- based outlier detection. In: Plant C, Wang H, Cuzzocrea A, Zan- iolo C, Wu X (eds) 20Th IEEE international conference on data mining, ICDM 2020, sorrento, italy, november 17-20, 2020, IEEE, pp 1118-1123. https://doi.org/10.1109/ICDM50108.2020.00135",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Generative adversarial active learning for unsupervised outlier detection",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "IEEE Trans Knowl Data Eng",
                "volume": "32",
                "issue": "8",
                "pages": "1517--1528",
                "other_ids": {
                    "DOI": [
                        "10.1109/TKDE.2019.2905606"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Liu Y, Li Z, Zhou C, Jiang Y, Sun J, Wang M, He X (2020) Generative adversarial active learning for unsupervised outlier detection. IEEE Trans Knowl Data Eng 32(8):1517-1528. https://doi.org/10.1109/TKDE.2019.2905606",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Deep contextualized word representations",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "E"
                        ],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Neumann",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Gardner",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018",
                "volume": "1",
                "issue": "",
                "pages": "2227--2237",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/n18-1202"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L (2018) Deep contextualized word representations. In: Walker MA, Ji H, Stent A (eds) Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1- 6, 2018, vol 1 (Long Papers). Association for Computational Linguistics, pp 2227-2237. https://doi.org/10.18653/v1/n18-1202",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Platt",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Advances in large margin classifiers",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Platt JC (1999) Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In: Advances in large margin classifiers",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Using tf-idf to determine word relevance in document queries",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Ramos",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of the first instructional conference on machine learning",
                "volume": "",
                "issue": "",
                "pages": "29--48",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ramos J et al (2003) Using tf-idf to determine word relevance in document queries. In: Proceedings of the first instructional conference on machine learning, pp 29-48",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "icarl: Incremental classifier and representation learning",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Rebuffi",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Kolesnikov",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Sperl",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "H"
                        ],
                        "last": "Lampert",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
                "volume": "",
                "issue": "",
                "pages": "5533--5542",
                "other_ids": {
                    "DOI": [
                        "10.1109/CVPR.2017.587"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Rebuffi S, Kolesnikov A, Sperl G, Lampert CH (2017) icarl: Incremental classifier and representation learning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, IEEE Computer Society, pp 5533-5542. https://doi.org/10.1109/CVPR.2017.587",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Mining interpretable rules for sentiment and semantic relation analysis using tsetlin machines",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Saha",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Goodwin",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Artificial Intelligence XXXVII -40th SGAI International Conference on Artificial Intelligence, AI 2020",
                "volume": "12498",
                "issue": "",
                "pages": "67--78",
                "other_ids": {
                    "DOI": [
                        "10.1007/978-3-030-63799-6_5"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Saha R, Granmo O, Goodwin M (2020) Mining interpretable rules for sentiment and semantic relation analysis using tsetlin machines. In: Bramer M, Ellis R (eds) Artificial Intelligence XXXVII -40th SGAI International Conference on Artificial Intelligence, AI 2020, Cambridge, UK, December 15-17, 2020, Proceedings, Springer, Lecture Notes in Computer Science, vol 12498, pp 67-78. https://doi.org/10.1007/978-3-030-63799-6 5",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Masked language model scoring",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Salazar",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "Q"
                        ],
                        "last": "Nguyen",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Kirchhoff",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "2699--2712",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.240"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Salazar J, Liang D, Nguyen TQ, Kirchhoff K (2020) Masked language model scoring. In: Jurafsky D, Chai J, Schluter N, Tetreault JR (eds) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Association for Computational Linguistics, pp 2699-2712. https://doi.org/10.18653/v1/2020.acl-main.240",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Toward open set recognition",
                "authors": [
                    {
                        "first": "W",
                        "middle": [
                            "J"
                        ],
                        "last": "Scheirer",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "De Rezende Rocha",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Sapkota",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "E"
                        ],
                        "last": "Boult",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "IEEE Trans Pattern Anal Mach Intell",
                "volume": "35",
                "issue": "7",
                "pages": "1757--1772",
                "other_ids": {
                    "DOI": [
                        "10.1109/TPAMI.2012.256"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Scheirer WJ, de Rezende Rocha A, Sapkota A, Boult TE (2013) Toward open set recognition. IEEE Trans Pattern Anal Mach Intell 35(7):1757-1772. https://doi.org/10.1109/TPAMI.2012.256",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Estimating the support of a high-dimensional distribution",
                "authors": [
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Sch\u00f6lkopf",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "C"
                        ],
                        "last": "Platt",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Shawe-Taylor",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "J"
                        ],
                        "last": "Smola",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "C"
                        ],
                        "last": "Williamson",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Neural Comput",
                "volume": "13",
                "issue": "7",
                "pages": "1443--1471",
                "other_ids": {
                    "DOI": [
                        "10.1162/089976601750264965"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sch\u00f6lkopf B, Platt JC, Shawe-taylor J, Smola AJ, Williamson RC (2001) Estimating the support of a high-dimensional distri- bution. Neural Comput 13(7):1443-1471. https://doi.org/10.1162/ 089976601750264965",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Is attention interpretable?",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Serrano",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019",
                "volume": "",
                "issue": "",
                "pages": "2931--2951",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/p19-1282"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Serrano S, Smith NA (2019) Is attention interpretable?. In: Korhonen A, Traum DR, M\u00e1rquez L (eds) Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28-August 2, 2019, vol 1 (Long Papers). Association for Computational Linguistics, pp 2931- 2951. https://doi.org/10.18653/v1/p19-1282",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Novelty detection: The TREC experience",
                "authors": [
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Soboroff",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Harman",
                        "suffix": ""
                    }
                ],
                "year": 2005,
                "venue": "HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "105--112",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Soboroff I, Harman D (2005) Novelty detection: The TREC experience. In: HLT/EMNLP 2005, Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference, 6-8 October 2005, Vancouver, British Columbia, Canada, The Association for Computational Linguistics, pp 105-112. https://aclanthology.org/ H05-1014/",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Understanding attention for text classification",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020",
                "volume": "",
                "issue": "",
                "pages": "3418--3428",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2020.acl-main.312"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Sun X, Lu W (2020) Understanding attention for text classifi- cation. In: Jurafsky D, Chai J, Schluter N, Tetreault JR (eds) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, Association for Computational Linguistics, pp 3418-3428. https://doi.org/10.18653/v1/2020.acl-main.312",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Outlier detection using classifier instability",
                "authors": [
                    {
                        "first": "Dmj",
                        "middle": [],
                        "last": "Tax",
                        "suffix": ""
                    },
                    {
                        "first": "Rpw",
                        "middle": [],
                        "last": "Duin",
                        "suffix": ""
                    }
                ],
                "year": 1998,
                "venue": "Advances in pattern recognition, joint IAPR international workshops SSPR '98 and SPR '98",
                "volume": "1451",
                "issue": "",
                "pages": "593--601",
                "other_ids": {
                    "DOI": [
                        "10.1007/BFb0033283"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tax DMJ, Duin RPW (1998) Outlier detection using classi- fier instability. In: Amin A, Dori D, Pudil P, Freeman H (eds) Advances in pattern recognition, joint IAPR international work- shops SSPR '98 and SPR '98, sydney, NSW, Australia, August 11- 13, 1998 (Proceedings, Springer), Lecture Notes in Computer Sci- ence, vol 1451, pp 593-601. https://doi.org/10.1007/BFb0033283",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Support vector data description",
                "authors": [
                    {
                        "first": "Dmj",
                        "middle": [],
                        "last": "Tax",
                        "suffix": ""
                    },
                    {
                        "first": "Rpw",
                        "middle": [],
                        "last": "Duin",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Mach Learn",
                "volume": "54",
                "issue": "1",
                "pages": "45--66",
                "other_ids": {
                    "DOI": [
                        "10.1023/B:MACH.0000008084.60811.49"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tax DMJ, Duin RPW (2004) Support vector data descrip- tion. Mach Learn 54(1):45-66. https://doi.org/10.1023/B:MACH. 0000008084.60811.49",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Evaluation of novelty metrics for sentence-level novelty mining",
                "authors": [
                    {
                        "first": "F",
                        "middle": [
                            "S"
                        ],
                        "last": "Tsai",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [
                            "L"
                        ],
                        "last": "Chan",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Inf Sci",
                "volume": "180",
                "issue": "12",
                "pages": "2359--2374",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.ins.2010.02.020"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Tsai FS, Tang W, Chan KL (2010) Evaluation of novelty metrics for sentence-level novelty mining. Inf Sci 180(12):2359-2374. https://doi.org/10.1016/j.ins.2010.02.020",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Attention is all you need",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Vaswani",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Shazeer",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Parmar",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Uszkoreit",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "N"
                        ],
                        "last": "Gomez",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Polosukhin",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "; Guyon",
                        "suffix": ""
                    },
                    {
                        "first": "Von",
                        "middle": [],
                        "last": "Luxburg",
                        "suffix": ""
                    },
                    {
                        "first": "U",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Wallach",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [
                            "M"
                        ],
                        "last": "Fergus",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Vishwanathan",
                        "suffix": ""
                    },
                    {
                        "first": "Svn",
                        "middle": [],
                        "last": "Garnett",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "5998--6008",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention is all you need. In: Guyon I, von Luxburg U, Bengio S, Wallach HM, Fergus R, Vish- wanathan SVN, Garnett R (eds) Advances in Neural Information Processing Systems 30: Annual Conference on Neural Informa- tion Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp 5998-6008. https://proceedings.neurips.cc/paper/ 2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "CSE: Conceptual sentence embeddings based on attention model",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Q",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics",
                "volume": "1",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/p16-1048"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Wang Y, Huang H, Feng C, Zhou Q, Gu J, Gao X (2016) CSE: Conceptual sentence embeddings based on attention model. In: Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1 (Long Papers). The Association for Computer Linguistics. https://doi.org/10.18653/v1/p16-1048",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Humanlevel interpretable learning for aspect-based sentiment analysis",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "K"
                        ],
                        "last": "Yadav",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Goodwin",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event",
                "volume": "",
                "issue": "",
                "pages": "14203--14212",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yadav RK, Jiao L, Granmo O, Goodwin M (2021a) Human- level interpretable learning for aspect-based sentiment analysis. In: In: Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, AAAI Press, pp 14203-14212. https://ojs.aaai.org/index.php/AAAI/article/view/17671",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Enhancing interpretable clauses semantically using pretrained word representation",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "K"
                        ],
                        "last": "Yadav",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [
                            "C"
                        ],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Goodwin",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
                "volume": "",
                "issue": "",
                "pages": "265--274",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/2021.blackboxnlp-1.19"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yadav RK, Jiao L, Granmo OC, Goodwin M (2021b) Enhancing interpretable clauses semantically using pretrained word represen- tation. In: Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Association for Computational Linguistics, Punta Cana, Dominican Republic, pp 265-274 https://doi.org/10.18653/v1/2021.blackboxnlp-1.19. https://aclanthology.org/2021.blackboxnlp-1.19",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Open category classification by adversarial sample generation",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Qu",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence",
                "volume": "2017",
                "issue": "",
                "pages": "3357--3363",
                "other_ids": {
                    "DOI": [
                        "10.24963/ijcai.2017/469"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yu Y, Qu W, Li N, Guo Z (2017) Open category classification by adversarial sample generation. In: Sierra C (ed) Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelli- gence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, ijcai.org, pp 3357-3363. https://doi.org/10.24963/ijcai.2017/469",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "On the convergence of tsetlin machines for the identity-and not operators",
                "authors": [
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Granmo",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Goodwin",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence",
                "volume": "",
                "issue": "5555",
                "pages": "1--1",
                "other_ids": {
                    "DOI": [
                        "10.1109/TPAMI.2021.3085591"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhang X, Jiao L, Granmo O, Goodwin M (5555) On the convergence of tsetlin machines for the identity-and not operators. IEEE Transactions on Pattern Analysis & Machine Intelligence pp 1-1. https://doi.org/10.1109/TPAMI.2021.3085591",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Combining named entities and tags for novel sentence detection",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "S"
                        ],
                        "last": "Tsai",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Proceedings of the WSDM'09 Workshop on Exploiting Semantic Annotations in Information Retrieval",
                "volume": "",
                "issue": "",
                "pages": "30--34",
                "other_ids": {
                    "DOI": [
                        "10.1145/1506250.1506256."
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Zhang Y, Tsai FS (2009) Combining named entities and tags for novel sentence detection. In: Proceedings of the WSDM'09 Work- shop on Exploiting Semantic Annotations in Information Retrieval, Association for Computing Machinery, New York, NY, USA, ESAIR '09, p 30-34. https://doi.org/10.1145/1506250. 1506256.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": "1",
                "text": "Fig. 1 Visualization of outlier detection, anomaly detection and novelty detection",
                "type_str": "figure",
                "num": null
            },
            "FIGREF1": {
                "uris": null,
                "fig_num": "2",
                "text": "Fig. 2 Tsetlin Machine architecture for generating word sequences",
                "type_str": "figure",
                "num": null
            },
            "FIGREF2": {
                "uris": null,
                "fig_num": "3",
                "text": "Fig. 3 Novelty scoring calculation for each word",
                "type_str": "figure",
                "num": null
            },
            "FIGREF3": {
                "uris": null,
                "fig_num": "4",
                "text": "Fig.4 Cumulative frequency distribution (CFD) graph for word scores in different categories of BBC Sports using TM",
                "type_str": "figure",
                "num": null
            },
            "FIGREF4": {
                "uris": null,
                "fig_num": "6",
                "text": "Fig.6 Visualization of tokens in known, Novel and Shared categories from BBC Sports",
                "type_str": "figure",
                "num": null
            },
            "FIGREF5": {
                "uris": null,
                "fig_num": "7",
                "text": "Fig. 7 Boxplot of scores in known, Novel and Shared categories from BBC Sports",
                "type_str": "figure",
                "num": null
            },
            "FIGREF6": {
                "uris": null,
                "fig_num": "8910",
                "text": "Fig.8ROC curve and precision-recall of known/novel class classification of BBC Sports using word scores obtained from TM",
                "type_str": "figure",
                "num": null
            },
            "FIGREF7": {
                "uris": null,
                "fig_num": "1213",
                "text": "Fig.12 Cumulative frequency distribution (CFD) graph for word scores in different categories of 20 Newsgroups using TM",
                "type_str": "figure",
                "num": null
            },
            "FIGREF8": {
                "uris": null,
                "fig_num": "14",
                "text": "Fig. 14 Visualization of tokens in known, Novel and Shared categories from 20Newsgroups",
                "type_str": "figure",
                "num": null
            },
            "FIGREF9": {
                "uris": null,
                "fig_num": "151618",
                "text": "Fig. 15 Boxplot of scores in known, Novel and Shared categories from 20Newsgroups",
                "type_str": "figure",
                "num": null
            },
            "TABREF2": {
                "text": "Clauses with conjunctive word patterns for known and novel class",
                "content": "<table><tr><td>Known Clauses</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "text": "Relative frequency and score for each word",
                "content": "<table><tr><td>Known</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "text": "Performance comparison of TM framework with cluster and outlier-based novelty detection algorithms",
                "content": "<table><tr><td>Algorithms</td><td>20 Newsgroup</td><td>BBC sports</td></tr><tr><td>LOF</td><td>52.51 %</td><td>47.97 %</td></tr><tr><td>Feature Bagging</td><td>67.60 %</td><td>54.38 %</td></tr><tr><td>HBOS</td><td>55.03 %</td><td>49.53 %</td></tr><tr><td>Isolation Forest</td><td>52.01 %</td><td>49.35%</td></tr><tr><td>Average KNN</td><td>76.35 %</td><td>55.54 %</td></tr><tr><td>K-Means clustering</td><td>81.00 %</td><td>47.70 %</td></tr><tr><td>One-class SVM</td><td>83.70 %</td><td>83.53 %</td></tr><tr><td>SO-GAAL</td><td>80.2%</td><td>83.50%</td></tr><tr><td>MO-GAAL</td><td>82.9%</td><td>86.68%</td></tr><tr><td>COPOD</td><td>84.4%</td><td>86.09%</td></tr><tr><td>TM framework</td><td>82.50%</td><td>89.47%</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "text": "ROC AUC (%) of one-class classification with leave-one-out evaluation on 20 Newsgroup",
                "content": "<table><tr><td>Normal class</td><td>ABOD</td><td>CBLOF</td><td>HBOS</td><td>IForest</td><td>KNN</td><td>LOF</td><td>OCSVM</td><td>COPOD</td><td>TM</td></tr><tr><td>comp</td><td>0.506</td><td>0.618</td><td>0.625</td><td>0.62</td><td>0.622</td><td>0.62</td><td>0.614</td><td>0.627</td><td>0.55</td></tr><tr><td>rec</td><td>0.508</td><td>0.481</td><td>0.476</td><td>0.483</td><td>0.479</td><td>0.48</td><td>0.48</td><td>0.476</td><td>0.60</td></tr><tr><td>sci</td><td>0.50</td><td>0.435</td><td>0.449</td><td>0.454</td><td>0.434</td><td>0.435</td><td>0.433</td><td>0.45</td><td>0.53</td></tr><tr><td>misc</td><td>0.511</td><td>0.533</td><td>0.527</td><td>0.534</td><td>0.54</td><td>0.542</td><td>0.534</td><td>0.532</td><td>0.69</td></tr><tr><td>pol</td><td>0.492</td><td>0.452</td><td>0.436</td><td>0.445</td><td>0.451</td><td>0.451</td><td>0.454</td><td>0.435</td><td>0.71</td></tr><tr><td>rel</td><td>0.494</td><td>0.449</td><td>0.437</td><td>0.456</td><td>0.472</td><td>0.47</td><td>0.457</td><td>0.443</td><td>0.63</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "text": "Overall word statistics for BBC sport dataset",
                "content": "<table><tr><td>Category</td><td colspan=\"3\">Total word count Average score Standard deviation</td></tr><tr><td colspan=\"2\">Known words 6660</td><td>0.74</td><td>0.23</td></tr><tr><td colspan=\"2\">Novel words 1941</td><td>1.3125</td><td>3.75</td></tr><tr><td colspan=\"2\">Shared words 3135</td><td>11.30</td><td>316.93</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF7": {
                "text": "Composition of shared words in BBC Sport",
                "content": "<table><tr><td>Composition</td><td>Total word count</td><td>Average score</td><td>Standard deviation</td></tr><tr><td>Known words</td><td>10</td><td>0.11</td><td>0.070</td></tr><tr><td>Novel words</td><td>17</td><td>1941.13</td><td>3919.02</td></tr><tr><td>Common words</td><td>3051</td><td>1.03</td><td>0.99</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF8": {
                "text": "Example of top words extracted from KE baselines for the Known class in BBC Sports",
                "content": "<table><tr><td>TM</td><td/><td>TopicRank</td><td/><td>YAKE</td><td/><td>MultipartiteRank</td><td/><td>BERT-MMR</td><td/></tr><tr><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td></tr><tr><td>manchester</td><td>0.004</td><td>players</td><td>0.0065</td><td>said</td><td>2.65</td><td>players</td><td>0.0053</td><td>kickoff</td><td>0.25</td></tr><tr><td>manager</td><td>0.040</td><td>ball</td><td>0.0050</td><td>game</td><td>5.16</td><td>games</td><td>0.0050</td><td>fifaasian</td><td>0.29</td></tr><tr><td>arsenal</td><td>0.041</td><td>team</td><td>0.0050</td><td>england</td><td>6.48</td><td>goal</td><td>0.0038</td><td>espnstar</td><td>0.28</td></tr><tr><td>united</td><td>0.043</td><td>goal</td><td>0.0043</td><td>chelsea</td><td>7.01</td><td>chelsea</td><td>0.0029</td><td>fifa</td><td>0.24</td></tr><tr><td>cricket</td><td>0.046</td><td>chelsea</td><td>0.0038</td><td>players</td><td>7.08</td><td>team</td><td>0.0029</td><td>juventus</td><td>0.23</td></tr><tr><td>chelsea</td><td>0.049</td><td>wickets</td><td>0.0030</td><td>team</td><td>7.44</td><td>manchester</td><td>0.0026</td><td>matchwinner</td><td>0.22</td></tr><tr><td>oneday</td><td>0.051</td><td>oneday</td><td>0.0029</td><td>oneday</td><td>8.35</td><td>arsenal</td><td>0.0024</td><td>sportsweek</td><td>0.34</td></tr><tr><td>striker</td><td>0.074</td><td>england</td><td>0.0027</td><td>united</td><td>8.39</td><td>ball</td><td>0.0023</td><td>goalkick</td><td>0.20</td></tr><tr><td>batsman</td><td>0.114</td><td>arsenal</td><td>0.0027</td><td>league</td><td>8.39</td><td>england</td><td>0.0022</td><td>clubmate</td><td>0.12</td></tr><tr><td>bowler</td><td>0.133</td><td>tests</td><td>0.0026</td><td>arsenal</td><td>9.62</td><td>matches</td><td>0.0021</td><td>autobiography</td><td>0.13</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF9": {
                "text": "Example of top words extracted from KE baselines for the novel class in BBC Sports",
                "content": "<table><tr><td>TM</td><td/><td>TopicRank</td><td/><td>YAKE</td><td/><td colspan=\"2\">MultipartiteRank</td><td>BERT-MMR</td><td/></tr><tr><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td></tr><tr><td>rugby</td><td>11143.58</td><td>nations</td><td>0.012</td><td>england</td><td>0.006</td><td>nations</td><td>0.0089</td><td>nflstyle</td><td>0.29</td></tr><tr><td>nations</td><td>10000</td><td>game</td><td>0.011</td><td>rugby</td><td>0.101</td><td>england</td><td>0.0082</td><td>july</td><td>0.05</td></tr><tr><td>ireland</td><td>468.18</td><td>player</td><td>0.009</td><td>wales</td><td>0.112</td><td>game</td><td>0.0074</td><td>wednesday</td><td>0.08</td></tr><tr><td>flyhalf</td><td>54.79</td><td>side</td><td>0.007</td><td>nations</td><td>0.113</td><td>wales</td><td>0.0071</td><td>rugby</td><td>0.17</td></tr><tr><td>lions</td><td>38.18</td><td>wales</td><td>0.007</td><td>ireland</td><td>0.113</td><td>player</td><td>0.0064</td><td>wordclass</td><td>0.06</td></tr><tr><td>scrumhalf</td><td>25.09</td><td>ireland</td><td>0.006</td><td>game</td><td>0.125</td><td>side</td><td>0.0048</td><td>dropkicking</td><td>0.12</td></tr><tr><td>flanker</td><td/><td>years</td><td>0.006</td><td>coach</td><td>0.146</td><td>team</td><td>0.0038</td><td>goalkickers</td><td>0.21</td></tr><tr><td>irish</td><td>17.55</td><td>team</td><td>0.005</td><td>side</td><td>0.182</td><td>france</td><td>0.0036</td><td>sportsweek</td><td>0.28</td></tr><tr><td>centre</td><td>17.19</td><td>ball</td><td>0.004</td><td>players</td><td>0.206</td><td>win</td><td>0.0033</td><td>tournament</td><td>0.13</td></tr><tr><td>squad</td><td>9.28</td><td>win</td><td>0.003</td><td>win</td><td>0.220</td><td>squad</td><td>0.0030</td><td>kickoff</td><td>0.23</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF10": {
                "text": "Overall word statistics for 20 Newsgroups dataset",
                "content": "<table><tr><td>Category</td><td>Total word count</td><td>Average score</td><td>Standard deviation</td></tr><tr><td>Known words</td><td>23133</td><td>0.99</td><td>0.21</td></tr><tr><td>Novel words</td><td>6921</td><td>1.20</td><td>1.04</td></tr><tr><td>Shared words</td><td>5786</td><td>3.04</td><td>131.62</td></tr><tr><td colspan=\"2\">Table 10 Composition of shared words in 20 Newsgroups dataset</td><td/><td/></tr><tr><td>Composition</td><td>Total word count</td><td>Average score</td><td>Standard deviation</td></tr><tr><td>Known words</td><td/><td>0.14</td><td>0.074</td></tr><tr><td>Novel words</td><td>33</td><td>640.75</td><td>2378.87</td></tr><tr><td>Common words</td><td>5697</td><td>1.11</td><td>0.58</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF11": {
                "text": "Example of top words extracted from KE baselines for the known class in 20 Newsgroups",
                "content": "<table><tr><td>TM</td><td/><td>TopicRank</td><td/><td>YAKE</td><td/><td>MultipartiteRank</td><td/><td>BERT-MMR</td><td/></tr><tr><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td></tr><tr><td>program</td><td>0.103</td><td>people</td><td>0.074</td><td>image</td><td>4.60</td><td>people</td><td>0.054</td><td>lawbook</td><td>0.44</td></tr><tr><td>gun</td><td>0.104</td><td>gun</td><td>0.041</td><td>gun</td><td>6.77</td><td>guns</td><td>0.033</td><td>interpolation</td><td>0.42</td></tr><tr><td>use</td><td>0.117</td><td>article</td><td>0.038</td><td>people</td><td>7.19</td><td>article</td><td>0.027</td><td>literature</td><td>0.41</td></tr><tr><td>write</td><td>0.145</td><td>fire</td><td>0.032</td><td>file</td><td>7.25</td><td>government</td><td>0.026</td><td>reading</td><td>0.41</td></tr><tr><td>public</td><td>0.154</td><td>government</td><td>0.032</td><td>article</td><td>7.41</td><td>fire</td><td/><td>writing</td><td>0.41</td></tr><tr><td>file</td><td>0.175</td><td>image</td><td>0.027</td><td>like</td><td>10.1</td><td>time</td><td>0.019</td><td>translation</td><td>0.41</td></tr><tr><td>image</td><td>0.187</td><td>fbi</td><td>0.021</td><td>jpeg</td><td>13.1</td><td>day</td><td>0.018</td><td>lexidata</td><td>0.40</td></tr><tr><td>email</td><td>0.074</td><td>weapons</td><td>0.019</td><td>think</td><td>15.6</td><td>weapons</td><td>0.017</td><td>prisoncamp</td><td>0.17</td></tr><tr><td>police</td><td>0.358</td><td>problem</td><td>0.018</td><td>time</td><td>16.6</td><td>fbi</td><td>0.016</td><td>comparable</td><td>0.40</td></tr><tr><td>fbi</td><td>0.231</td><td>information</td><td>0.017</td><td>program</td><td>17.4</td><td>year</td><td>0.014</td><td>literate</td><td>0.40</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF12": {
                "text": "Example of top words extracted from KE baselines for the novel class in 20 Newsgroups",
                "content": "<table><tr><td>TM</td><td/><td>TopicRank</td><td/><td>YAKE</td><td/><td colspan=\"2\">MultipartiteRank</td><td>BERT-MMR</td><td/></tr><tr><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td><td>Word</td><td>Score</td></tr><tr><td>team</td><td>450.66</td><td>game</td><td>0.082</td><td>writes</td><td>1.48</td><td>game</td><td>0.065</td><td>baseball</td><td>0.38</td></tr><tr><td>baseball</td><td>243.62</td><td>team</td><td>0.074</td><td>game</td><td>1.55</td><td>team</td><td>0.048</td><td>reporters</td><td>0.33</td></tr><tr><td>pitcher</td><td>62.26</td><td>player</td><td>0.055</td><td>article</td><td>2.29</td><td>pitch</td><td>0.037</td><td>salaries</td><td>0.32</td></tr><tr><td>league</td><td>55.88</td><td>year</td><td>0.053</td><td>team</td><td>2.30</td><td>player</td><td>0.037</td><td>basketball</td><td>0.31</td></tr><tr><td>season</td><td>40.50</td><td>baseball</td><td>0.049</td><td>last</td><td>2.95</td><td>baseball</td><td>0.034</td><td>fittest</td><td>-0.07</td></tr><tr><td>hit</td><td>24.63</td><td>pitcher</td><td>0.042</td><td>baseball</td><td>3.41</td><td>pitcher</td><td>0.026</td><td>kickoff</td><td>0.15</td></tr><tr><td>play</td><td>22.68</td><td>ball</td><td>0.037</td><td>player</td><td>3.44</td><td>ball</td><td>0.026</td><td>nba</td><td>0.10</td></tr><tr><td>batting</td><td>22.25</td><td>runs</td><td>0.030</td><td>time</td><td>3.94</td><td>runs</td><td>0.022</td><td>secretive</td><td>0.30</td></tr><tr><td>pitching</td><td>22.00</td><td>season</td><td>0.030</td><td>hit</td><td>4.06</td><td>season</td><td>0.020</td><td>jerseys</td><td>0.30</td></tr><tr><td>player</td><td>21.86</td><td>braves</td><td>0.030</td><td>run</td><td>4.66</td><td>braves</td><td>0.019</td><td>catchers</td><td>0.30</td></tr><tr><td colspan=\"5\">boosted. Second, the words that are most representative</td><td/><td/><td/><td/><td/></tr><tr><td colspan=\"5\">for the respective classes are captured frequently by</td><td/><td/><td/><td/><td/></tr><tr><td colspan=\"5\">clauses, making them the most repeated ones. Third, the</td><td/><td/><td/><td/><td/></tr><tr><td colspan=\"5\">keywords captured by other KE baselines are comparable</td><td/><td/><td/><td/><td/></tr><tr><td colspan=\"5\">to those extracted by our method and accurately define</td><td/><td/><td/><td/><td/></tr><tr><td colspan=\"5\">corresponding classes. We observe that TopicRank, YAKE,</td><td/><td/><td/><td/><td/></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF13": {
                "text": "Co-occurrence matrix showing the information gain between words in BBC Sports",
                "content": "<table><tr><td/><td colspan=\"5\">Manchester Chelsea Particular Rugby Flyhalf</td></tr><tr><td colspan=\"3\">Manchester 14363.688 6.324</td><td>4.738</td><td>0.33</td><td>0.848</td></tr><tr><td>Chelsea</td><td>6.324</td><td colspan=\"2\">19801.49 6.18</td><td>0.466</td><td>1.326</td></tr><tr><td>Particular</td><td>4.738</td><td>6.18</td><td colspan=\"2\">30863.006 2.52</td><td>4.968</td></tr><tr><td>Rugby</td><td>0.33</td><td>0.466</td><td>2.52</td><td colspan=\"2\">486.758 3.952</td></tr><tr><td>Flyhalf</td><td>0.848</td><td>1.326</td><td>4.968</td><td>3.952</td><td>8888.888</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF14": {
                "text": "",
                "content": "<table><tr><td/><td colspan=\"5\">Co-occurrence matrix showing the information gain between</td></tr><tr><td colspan=\"3\">words in 20 Newsgroup</td><td/><td/><td/></tr><tr><td/><td>Guns</td><td>Weapon</td><td>Gather</td><td colspan=\"2\">Baseball Player</td></tr><tr><td>Guns</td><td colspan=\"2\">12302.96 17.648</td><td>15.754</td><td>4.036</td><td>4.268</td></tr><tr><td colspan=\"2\">Weapon 17.648</td><td colspan=\"2\">13888.888 12.108</td><td>4.66</td><td>5.102</td></tr><tr><td>Gather</td><td>15.754</td><td>12.108</td><td colspan=\"2\">14610.272 11.854</td><td>15.408</td></tr><tr><td colspan=\"2\">Baseball 4.036</td><td>4.66</td><td>11.854</td><td colspan=\"2\">4003.824 18.566</td></tr><tr><td>Player</td><td>4.268</td><td>5.102</td><td>15.408</td><td>18.566</td><td>9255.402</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF15": {
                "text": "Co-occurrence matrix showing the similarity between words in BBC Sports using Word2Vec",
                "content": "<table><tr><td/><td colspan=\"5\">Manchester Chelsea Particular Rugby Flyhalf</td></tr><tr><td colspan=\"2\">Manchester 1</td><td>0.782</td><td>0.653</td><td>0.598</td><td>0.718</td></tr><tr><td>Chelsea</td><td>0.782</td><td>1</td><td>0.891</td><td>0.820</td><td>0.829</td></tr><tr><td>Particular</td><td>0.653</td><td>0.891</td><td>1</td><td>0.821</td><td>0.941</td></tr><tr><td>Rugby</td><td>0.598</td><td>0.820</td><td>0.821</td><td>1</td><td>0.706</td></tr><tr><td>Flyhalf</td><td>0.718</td><td>0.829</td><td>0.941</td><td>0.706</td><td>1</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF16": {
                "text": "Table 12 provide examples of the highestscoring words captured by KE baselines, including TM, for both classes. The visualization of the scores are presented in Figs.",
                "content": "<table/>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF17": {
                "text": "Co-occurrence matrix showing the similarity between words in 20 Newsgroup using Word2Vec",
                "content": "<table><tr><td/><td>Guns</td><td>Weapon</td><td>Gather</td><td>Baseball</td><td>Player</td></tr><tr><td>Guns</td><td>1</td><td>0.709</td><td>0.726</td><td>0.673</td><td>0.701</td></tr><tr><td>Weapon</td><td>0.709</td><td>1</td><td>0.648</td><td>0.454</td><td>0.539</td></tr><tr><td>Gather</td><td>0.726</td><td>0.648</td><td>1</td><td>0.631</td><td>0.686</td></tr><tr><td>Baseball</td><td>0.673</td><td>0.454</td><td>0.631</td><td>1</td><td>0.764</td></tr><tr><td>Player</td><td>0.701</td><td>0.539</td><td>0.686</td><td>0.764</td><td>1</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            }
        }
    }
}