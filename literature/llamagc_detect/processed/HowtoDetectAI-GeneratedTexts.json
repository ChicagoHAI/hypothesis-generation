{
    "paper_id": "HowtoDetectAI-GeneratedTexts",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-09-19T13:38:13.606229Z"
    },
    "title": "How to Detect AI-Generated Texts?",
    "authors": [
        {
            "first": "Trung",
            "middle": [
                "T"
            ],
            "last": "Nguyen",
            "suffix": "",
            "affiliation": {},
            "email": "trung.nguyen@winona.edu"
        },
        {
            "first": "Amartya",
            "middle": [],
            "last": "Hatua",
            "suffix": "",
            "affiliation": {},
            "email": "amartya.hatua@fmr.com"
        },
        {
            "first": "Andrew",
            "middle": [
                "H"
            ],
            "last": "Sung",
            "suffix": "",
            "affiliation": {},
            "email": "andrew.sung@usm.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Recent advances in large language models (LLMs) have significantly improved the quality of synthetic text data. LLMs imitate human writing patterns to produce highly natural text, raising serious ethical, moral, legal, social, and economic concerns in various industries. To address these issues, we present a method to distinguish synthetically generated text (SGT) from human-written text (HWT). Our method includes methods for dataset creation, feature engineering, dataset comparison, and result analysis. As part of our research, we created two datasets -a Wikipedia-based dataset and a US Election 2024 related news article-based dataset using ChatGPT. These datasets can be used as open-source datasets in future studies. We also identified a set of handcrafted features that can serve as the baseline feature set for future research.",
    "pdf_parse": {
        "paper_id": "HowtoDetectAI-GeneratedTexts",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Recent advances in large language models (LLMs) have significantly improved the quality of synthetic text data. LLMs imitate human writing patterns to produce highly natural text, raising serious ethical, moral, legal, social, and economic concerns in various industries. To address these issues, we present a method to distinguish synthetically generated text (SGT) from human-written text (HWT). Our method includes methods for dataset creation, feature engineering, dataset comparison, and result analysis. As part of our research, we created two datasets -a Wikipedia-based dataset and a US Election 2024 related news article-based dataset using ChatGPT. These datasets can be used as open-source datasets in future studies. We also identified a set of handcrafted features that can serve as the baseline feature set for future research.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "In the recent past, the large language models (LLMs) [1] - [3] have achieved unprecedented success in Natural Language Understanding (NLU) [4] and Natural Language Generation (NLG) [5] based Natural Language Processing (NLP) [6] tasks such as text summarization, text classification, grammar correction, automated answering, text completion, etc. With prompt engineering, pre-trained LLMs such as GPT-3 [7] , , Llama [9] , etc. perform (NLP-based tasks) better than their predecessor transformer-based models such as BERT [10] , RoBERTa [11] , DistilBERT [12] , SPANBERT [13] , etc. The performance of automated question answering allows many downstream applications. Question answering systems are not only factually accurate but also can generate confident sounding, creative, and natural texts. On many occasions, it is very hard to distinguish between human written text and LLM generated synthetic text [14] - [16] .",
                "cite_spans": [
                    {
                        "start": 53,
                        "end": 56,
                        "text": "[1]",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 59,
                        "end": 62,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 139,
                        "end": 142,
                        "text": "[4]",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 181,
                        "end": 184,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 225,
                        "end": 228,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 403,
                        "end": 406,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 417,
                        "end": 420,
                        "text": "[9]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 522,
                        "end": 526,
                        "text": "[10]",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 537,
                        "end": 541,
                        "text": "[11]",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 555,
                        "end": 559,
                        "text": "[12]",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 571,
                        "end": 575,
                        "text": "[13]",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 908,
                        "end": 912,
                        "text": "[14]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 915,
                        "end": 919,
                        "text": "[16]",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I. INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "The advancement of synthetic text generation has raised some serious concerns, which have sociological and cyber effects. We can foresee that a large human workforce will be replaced by LLM-powered applications [17] . Chatbots are already performing many tasks in education, law, advertising, scientific/creative writing, entertainment, and many other industries. LLMs also empower cybercriminals with more widespread and dangerous capabilities [18] . Detecting the abuses of academic dishonesty, fake news/reviews, spam/phishing, etc., is more challenging [14] , [19] . All educational institutes perceive the ChatGPT as a major challenge because all the traditional plagiarism-checking software are not trained to identify the AI-generated text. Therefore identifying a synthetically generated text will be a very challenging task in the future. Hence, in this research initiative, we plan to explore different methods to identify distinguishable features and patterns of synthetically generated text data.",
                "cite_spans": [
                    {
                        "start": 211,
                        "end": 215,
                        "text": "[17]",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 445,
                        "end": 449,
                        "text": "[18]",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 557,
                        "end": 561,
                        "text": "[14]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 564,
                        "end": 568,
                        "text": "[19]",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I. INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "AI-generated text, or in general machine-generated text, detection problem has become a very popular topic started from the Turing test and then used in chatbot evaluation [20] . In this paper, we focused on automated detection methods rather than human detection or hybrid methods. As summarized by Crothers, Japkowicz, and Viktor, automatic AI-generated text detection methods can be categorized into two general approaches: a) Feature-based and b) Neural Language models [14] . On the other hand, other research focused on the detection of specific domains such as academic settings [21] - [25] , scientific [26] - [31] , fake news/fake reviews/misinformation [32] - [36] , etc.",
                "cite_spans": [
                    {
                        "start": 172,
                        "end": 176,
                        "text": "[20]",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 474,
                        "end": 478,
                        "text": "[14]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 586,
                        "end": 590,
                        "text": "[21]",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 593,
                        "end": 597,
                        "text": "[25]",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 611,
                        "end": 615,
                        "text": "[26]",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 618,
                        "end": 622,
                        "text": "[31]",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 663,
                        "end": 667,
                        "text": "[32]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 670,
                        "end": 674,
                        "text": "[36]",
                        "ref_id": "BIBREF34"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I. INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "In this work, we proposed two approaches to automatically detect AI-generated text: a) featured based with machine learning models and b) text similarity based methods. While our first approach follows the popular method in this domain, we improved the feature selection part and obtained higher detection results compared to existing methods. Later on, we verified our result with feature importance analysis and explainable models. In our second approach, given any text to be determined as AI-generated text, throughout the topic modeling and keyword extraction, we reverse the related questions and use AI models to generate the sample text. Based on the similarity analysis of the original text and the sample text, we can determine accurately whether it is AI-generated text or not. Our second approach works in general and in any specific domain without the requirement of ground truth data collection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I. INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "In summary, our contributions are listed below:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I. INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "\u2022 A comprehensive study on handcrafted feature design and selection process with machine learning models can achieve an AI-generated text detection success rate of up engineering and text similarity analysis. \u2022 An analysis of feature importance and explanation models to discuss our results on AI-generated detection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "I. INTRODUCTION",
                "sec_num": null
            },
            {
                "text": "In this section, we summarized related research on AIgenerated text detection methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "II. RELATED WORKS",
                "sec_num": null
            },
            {
                "text": "In this approach, different NLP techniques are used to extract useful features from the text. The motivation for this approach is from the observations that NLG models create different artifacts in generated text [14] . Some most important features that have been proposed are frequencies, linguistics, fluency, and fact verification [14] , [37] , [38] . These features are then trained with machine learning models, such as SVM, RF, or neural networks, to build classification models [39] - [43] .",
                "cite_spans": [
                    {
                        "start": 213,
                        "end": 217,
                        "text": "[14]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 334,
                        "end": 338,
                        "text": "[14]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 341,
                        "end": 345,
                        "text": "[37]",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 348,
                        "end": 352,
                        "text": "[38]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 485,
                        "end": 489,
                        "text": "[39]",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 492,
                        "end": 496,
                        "text": "[43]",
                        "ref_id": "BIBREF41"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Feature-Based Approach",
                "sec_num": null
            },
            {
                "text": "For this approach, neural language models (NMLs) are built to detect generated text from state-of-the-art NLG models. Many NLG models, such as GPT or Grover, can be used to detect their own outputs. However, the performance is generally lower than a simple TF-IDF baseline model if no fine-tuning is performed. Therefore, fine-tuning pre-trained large language models is preferred in order to differentiate the human-written vs. NLG model output samples [14] , [44] , [45] . The disadvantages of this approach are a) the requirement for large datasets and b) heavy computation power and resources for training. This approach has been applied in detecting machine-generated text in a specific domain rather than in a general context.",
                "cite_spans": [
                    {
                        "start": 454,
                        "end": 458,
                        "text": "[14]",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 461,
                        "end": 465,
                        "text": "[44]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 468,
                        "end": 472,
                        "text": "[45]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B. Neural Language Model Approaches",
                "sec_num": null
            },
            {
                "text": "Currently, there is no perfect AI-generated text detection in general (all possible domains). Much research focused on such detection tasks in specific domains, such as academic settings, scientific, fake news/reviews, or misinformation, as the scope can be narrowed down.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C. AI-Generated Text Detection on Specific Domains",
                "sec_num": null
            },
            {
                "text": "Academic settings: [21] collected submissions from computer science students and generated related ChatGPT responses. They evaluated 8 LLM text detectors on the above data. Their research has provided insights into the detectors' performance for the educator to maintain academic integrity. [22] conducted similar research but covered 12 publicly available AI-generated text tools and two commercial software (Turnitin and PlagiarismCheck). Similarly, other research, such as [23] - [25] contributed additional useful insights into the importance of AI-generated text detectors in academic settings.",
                "cite_spans": [
                    {
                        "start": 19,
                        "end": 23,
                        "text": "[21]",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 291,
                        "end": 295,
                        "text": "[22]",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 476,
                        "end": 480,
                        "text": "[23]",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 483,
                        "end": 487,
                        "text": "[25]",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C. AI-Generated Text Detection on Specific Domains",
                "sec_num": null
            },
            {
                "text": "Scientific: [26] studied the gap between the scientific content written by humans vs. generated by AI tools. The authors confirmed a \"writing style\" gap between them. [29] discussed the challenges of enforcing the policy on AI-generated papers/journals. [27] recently proposed a text representation method with machine learning models to detect fake scientific abstracts generated by a GPT-3 based model. Similarly, [28] studies the performance of using plagiarism detectors and blinded human reviewers on differencing original abstracts vs. fake abstracts generated by ChatGPT.",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 16,
                        "text": "[26]",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 167,
                        "end": 171,
                        "text": "[29]",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 254,
                        "end": 258,
                        "text": "[27]",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 416,
                        "end": 420,
                        "text": "[28]",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C. AI-Generated Text Detection on Specific Domains",
                "sec_num": null
            },
            {
                "text": "Fake news/reviews or misinformation: [32] - [34] studied the impacts of using AI-generated texts that can cause misinformation issues in the media. [36] conducted another study on the linguistic features and patterns of AI-generated COVID-19 misinformation. On the other hand, [35] proposed an AI model to detect AI-generated fake restaurant reviews on social media. These studies have raised the alarm about the severity of abused AI tools in media communications and information diffusion.",
                "cite_spans": [
                    {
                        "start": 37,
                        "end": 41,
                        "text": "[32]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 44,
                        "end": 48,
                        "text": "[34]",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 148,
                        "end": 152,
                        "text": "[36]",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 277,
                        "end": 281,
                        "text": "[35]",
                        "ref_id": "BIBREF33"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C. AI-Generated Text Detection on Specific Domains",
                "sec_num": null
            },
            {
                "text": "In this section, we discuss the proposed research methodology 1. The methodology comprises two main sections: data collection and experiments. We provide a description of each section here.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "III. PROPOSED METHODOLOGY",
                "sec_num": null
            },
            {
                "text": "To carry out our research, we require both synthetic and manually written text data for comparison purposes. As per our knowledge, there is no pre-existing database that provides this type of dataset. Therefore, our first step is to construct the dataset. In the second phase of our work, we will use NLP-based feature extraction to compare the features of the synthetic and human-generated data, train our model, and provide explanations.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Data Collection",
                "sec_num": null
            },
            {
                "text": "We have generated two sets of data. The first one includes Wikipedia data and its corresponding artificially created data. The second one consists of news articles regarding the 2024 US election and their corresponding artificially created data.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Data Collection",
                "sec_num": null
            },
            {
                "text": "1) Dataset Based on Wikipedia: We collected Wikipedia data using the method outlined in [46] . However, instead of only generating page summaries with ChatGPT, we expanded on this idea and created all sections of a Wikipedia page, which we labeled as synthetically generated data.",
                "cite_spans": [
                    {
                        "start": 88,
                        "end": 92,
                        "text": "[46]",
                        "ref_id": "BIBREF44"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Data Collection",
                "sec_num": null
            },
            {
                "text": "The process for generating synthetic text for a Wikipedia page about dogs is shown in Figure 2 . The page contains different sections, including Taxonomy, Evolution, Biology, and more. The text from these sections is extracted and labeled as human-written text (HWT). To generate text related to Taxonomy, we prompted ChatGPT with \"Describe Dog Taxonomy.\" The text generated by ChatGPT is labeled as synthetically generated text (SGT). The same process was used for all other sections of the Wikipedia page to generate both HWT and SGT.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 93,
                        "end": 94,
                        "text": "2",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A. Data Collection",
                "sec_num": null
            },
            {
                "text": "2) Dataset Based on News Articles: We gathered a dataset on the upcoming US Election in 2024 by collecting news articles and generating synthetic data 3. Initially, over 500 URLs of news articles related to the election were collected.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Data Collection",
                "sec_num": null
            },
            {
                "text": "Next, text data is extracted from each article and identified important keywords. Using these keywords, ChatGPT generated five questions for each article and prompted ChatGPT to ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Data Collection",
                "sec_num": null
            },
            {
                "text": "When working with datasets for NLP tasks, it is crucial to carry out feature engineering. This involves using similar techniques for both datasets to gain a better understanding of their patterns and characteristics. These features will be useful in classifying the datasets later in this research. We have hand-crafted basic NLP-related features, TD-IDF [47] related features, N-gram features [48] , topic modeling features, readability score, named entity recognition (NER) [49] count, and text error length features. To find an optimal set of features, principal component analysis (PCA) [50] is used.",
                "cite_spans": [
                    {
                        "start": 355,
                        "end": 359,
                        "text": "[47]",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 394,
                        "end": 398,
                        "text": "[48]",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 476,
                        "end": 480,
                        "text": "[49]",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 591,
                        "end": 595,
                        "text": "[50]",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B. Feature Engineering",
                "sec_num": null
            },
            {
                "text": "In order to differentiate between HWT and SGT, we utilized a dataset sourced from Wikipedia. We employed three different classifiers -Random Forest (RF) [51] , Support Vector Machine (SVM) [52] , and XGBoost (XGB) [53] -to perform the classification. Our models were trained, fine-tuned, and tested using this dataset. We used Precision, Recall, and F1 scores as metrics to evaluate and compare the performance of each model.",
                "cite_spans": [
                    {
                        "start": 153,
                        "end": 157,
                        "text": "[51]",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 189,
                        "end": 193,
                        "text": "[52]",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 214,
                        "end": 218,
                        "text": "[53]",
                        "ref_id": "BIBREF51"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C. Classification",
                "sec_num": null
            },
            {
                "text": "In order to assess the correlation between the HWT and SGT datasets pertaining to the 2024 US presidential election, the cosine similarity metric [54] has been employed to gauge their degree of similarity or dissimilarity. An overview of the similarity measure is offered for all the documents in the dataset.",
                "cite_spans": [
                    {
                        "start": 146,
                        "end": 150,
                        "text": "[54]",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "D. Similarity Calculation",
                "sec_num": null
            },
            {
                "text": "The classification results come with a detailed explanation, which helps to identify the essential hand-crafted features. After evaluating the classifier models' performance, we compared and identified these features as common important ones for both model training and individual prediction.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "E. Explanations",
                "sec_num": null
            },
            {
                "text": "In this section, data preprocessing and feature engineering will be described first. Then, the experiment setup, hyperparameter tuning, and performance metrics are explained. Finally, the experiment results and discussion are presented. In order to conduct the experiments, we need to preprocess the above two datasets and extract useful features from them, which will be described in the following two subsections.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "IV. RESULT AND DISCUSSION",
                "sec_num": null
            },
            {
                "text": "2) Data preprocessing: For each text data in the above two datasets (either human-written or ChatGPT-generated ones), we conducted standard NLP preprocessing steps and implemented using NLTK [55] , AutoCorrect [56] , and BeatifulSoup libraries [57] .",
                "cite_spans": [
                    {
                        "start": 191,
                        "end": 195,
                        "text": "[55]",
                        "ref_id": "BIBREF53"
                    },
                    {
                        "start": 210,
                        "end": 214,
                        "text": "[56]",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 244,
                        "end": 248,
                        "text": "[57]",
                        "ref_id": "BIBREF55"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "3) Feature Engineering: In order to perform NLP tasks with datasets, it's important to conduct feature engineering. This means using comparable methods for both datasets to gain a better understanding of their patterns and traits. These features will be helpful in later classifying the datasets during the research. We have manually created basic NLP-related features, TD-IDF-related features, N-gram features, topic modeling features, readability scores, named entity recognition (NER) counts, and text error length features. To determine the best set of features, we use principal component analysis (PCA) [50] .",
                "cite_spans": [
                    {
                        "start": 609,
                        "end": 613,
                        "text": "[50]",
                        "ref_id": "BIBREF48"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "Basic NLP features:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "\u2022 Word count: The number of words in the given text Topic Modeling features: For each text data in the dataset, we analyzed the topic modeling features using the Neural LDA model [58] with the number of topics = 20.",
                "cite_spans": [
                    {
                        "start": 179,
                        "end": 183,
                        "text": "[58]",
                        "ref_id": "BIBREF56"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "Other features: Besides the above features, we analyzed the following additional important text features:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "\u2022 readbility score: 8 readability score of each text data (Flesch-Kincaid score [59] , Flesch score [60] , Gunning fog score [61] , Coleman liau score [62] , Dale Chall score [63] , Ari score [64] , Linsear write score [65] , and Spache score [66] ). \u2022 Named Entity Recognition (NER) count: Analyzing and counting the number of NER tokens in the given text.",
                "cite_spans": [
                    {
                        "start": 80,
                        "end": 84,
                        "text": "[59]",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 100,
                        "end": 104,
                        "text": "[60]",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 125,
                        "end": 129,
                        "text": "[61]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 151,
                        "end": 155,
                        "text": "[62]",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 175,
                        "end": 179,
                        "text": "[63]",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 192,
                        "end": 196,
                        "text": "[64]",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 219,
                        "end": 223,
                        "text": "[65]",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 243,
                        "end": 247,
                        "text": "[66]",
                        "ref_id": "BIBREF64"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "\u2022 text error length: The number of grammar errors in the text. We collected a total of 50,783 features for each text data in our raw dataset. Most of the features are in the group of Term Frequencies and Ngram. Table I displayed the detailed list of feature descriptions and sizes of each component.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "Table II displayed the size of our two datasets after preprocessing and feature extraction. We try to balance datasets with approximately equal numbers of data rows of both classes (human-written and ChatGPT generated texts).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 8,
                        "text": "II",
                        "ref_id": "TABREF3"
                    }
                ],
                "eq_spans": [],
                "section": "A. Dataset Preprocessing and",
                "sec_num": null
            },
            {
                "text": "In this paper, we conducted three different kinds of experiments to demonstrate our proposed methodology in section III. In the first experiment, traditional machine learning algorithms were used to train classification models to identify either human-written or ChatGPT generated text based on the collected handcrafted features. In this case, RF, SVM, and XGBoost were used to train our models. In the second experiment, we calculated the cosine similarity between the extracted features of human-written text vs. ChatGPT generated text and analyzed the results. Because a lot of term frequencies and ngram features are sparse, we calculated the cosine similarity using three approaches: a) without term frequencies and ngram features (WoTFNG), b) all features (AllFT), c) extracted PCA features from all features (PCA-FT). Finally, in our last experiment, we conducted the feature importance analysis to explain our models.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "B. Experimental Setup",
                "sec_num": null
            },
            {
                "text": "In the first experiment, we used pandas for loading the extracted features and the scikit-learn [67] library to train the models and evaluate the performance. Grid searches were conducted to find the optimal parameters for the three machine learning models: RF, SVM, and XGBoost. The search ranges for the important hyperparameters are listed in Table III . For the RF model, the following other hyperparameters were used: min weight fraction leaf = 0.0, min impurity decrease = 0.0, ccp alpha = 0.0, and max samples = None. For the SVM model, we used the following fixed hyperparameters: coef0 = 0.0, tol = 0.001, cache size = 200, max iter = -1, decision function shape = 'ovr'. In the second experiment, we used only one parameter for the PCA: n components = 1024, which is the number of components that can be reduced from our feature set but can keep the maximum information.",
                "cite_spans": [
                    {
                        "start": 96,
                        "end": 100,
                        "text": "[67]",
                        "ref_id": "BIBREF65"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 352,
                        "end": 355,
                        "text": "III",
                        "ref_id": "TABREF4"
                    }
                ],
                "eq_spans": [],
                "section": "C. Hyperparameter Tuning and Performance Metrics",
                "sec_num": null
            },
            {
                "text": "To measure the performance of our classification models, we compared the following four important metrics: accuracy, precision, recall, and F1 score.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "C. Hyperparameter Tuning and Performance Metrics",
                "sec_num": null
            },
            {
                "text": "1) Classification Results: Table IV displayed the overall performance of the machine learning models in our first experiment. While SVM could not perform well because of the complexity of the problem, both RF and XGBoost performed extremely well with an F1-score of 0.9993. This result showed that our proposed model with the suggested handcrafted features can help identify either human-written or ChatGPT-generated text. This supports our assumptions that there are differences in writing styles, the vocabulary used, text lengths, errors, and others when comparing them. 2) Document Similarity: We utilized the Cosine similarity score to measure the similarity between HWT and SGT. The Cosine similarity score ranges from -1 to 1, where a score of 1 indicates that two documents are identical. If the score is less than 1, it means that the documents are not identical. As the score approaches -1, the degree of dissimilarity between the compared documents increases. In this research, we used the cosine similarity score to find the document similarity between HWT and SGT for the US Election related news articles based dataset. As mentioned in the III section, feature extraction is a prerequisite to calculating cosine similarity. Hence, we have calculated all features (as mentioned in Table I ) before calculating the cosine similarity. Out of 50,783 features, most of them (35, 742) belong to term frequencies, and they are sparse matrices. To observe the effect of the term frequencies, we have calculated two cosine similarities i) without term frequencies (15,041), and ii) with all features (50,783). In the first scenario, we are missing some information, and in the second scenario, we are using some features which are not needed. To solve both problems, we used PCA to identify the most useful features. Therefore, we used this new subset of features to find the cosine similarity score.",
                "cite_spans": [
                    {
                        "start": 1383,
                        "end": 1387,
                        "text": "(35,",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 1388,
                        "end": 1392,
                        "text": "742)",
                        "ref_id": null
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1300,
                        "end": 1301,
                        "text": "I",
                        "ref_id": "TABREF2"
                    }
                ],
                "eq_spans": [],
                "section": "D. Results and Discussions",
                "sec_num": null
            },
            {
                "text": "Table V shows the cosine similarity scores using different feature sets. For all three cases, 3643 HWT and corresponding SGT are used. The entire cosine similarity range is divided into four bins (-1 to -0.6, -0.6 to -0.2, -0.2 to 0.6, and 0.6 to 1), and a count of documents for each bin has been provided. Using the third feature subset, almost all document comparisons generate cosine scores between -0.6 to -0.2, which suggests these comparing documents are dissimilar. Using the first and 3) Feature Importance Analysis: For classification, the RF and XGB models performed similarly. To understand the behavior of the training features, we have calculated the feature importance of both models using scikit-learn library [68] . Figure 4 shows the relative training feature importance graph. We can see that for the RF classifier, Coleman liau score, word density, text error length, title word count, and punctuation count are the top five important training features; similarly, word density, title word count, word count, Coleman liau score, text error length are the top five important features for XGB classifier. Out of the top five features, four features are common for both models. This gives a fair idea about the set of important features that can be used as the distinguishing characteristics between HWT and SGT.",
                "cite_spans": [
                    {
                        "start": 726,
                        "end": 730,
                        "text": "[68]",
                        "ref_id": "BIBREF66"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 6,
                        "end": 7,
                        "text": "V",
                        "ref_id": "TABREF6"
                    },
                    {
                        "start": 740,
                        "end": 741,
                        "text": "4",
                        "ref_id": "FIGREF3"
                    }
                ],
                "eq_spans": [],
                "section": "D. Results and Discussions",
                "sec_num": null
            },
            {
                "text": "The Scikit-learn library has been used to calculate the importance of features in the entire training dataset. Additionally, the SHAP library [69] was utilized to explain the features for individual predictions. In figures 5a and 5b, the SHAP library has been used to display the individual predictions (a) SHAP Explanation using Random Forest classifier (b) SHAP Explanation using XGB classifier Fig. 5 : Feature explanation using SHAP for single text data point of the RF and XGB models in waterfall plots. These plots offer explanations for every prediction, utilizing the 10 most important features. Each row in the plot shows the positive (red) or negative (blue) contribution of each feature, moving from the expected model output over the background dataset to the prediction for that sample. The expected value of the model output is found at the bottom of the plot. SHAP explanations are given regarding models' margin output before the logistic link function. The x-axis units are log-odds units, meaning that negative values imply a probability of less than 0.5 that the document is synthetically generated. The gray text before the feature names displays the value of each feature for that specific sample.",
                "cite_spans": [
                    {
                        "start": 142,
                        "end": 146,
                        "text": "[69]",
                        "ref_id": "BIBREF67"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 402,
                        "end": 403,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "D. Results and Discussions",
                "sec_num": null
            },
            {
                "text": "In Figure 5 , the SHAP waterflow plot is displayed for both the RF and XGB models using the same text data. For the RF model shown in 5a, four of the top five SHAP features are also in the top five scikit-learn feature sets. It is worth noting that all four features have a positive impact on the decision-making process of the classifier. Similarly, for the XGB model, three out of the top five SHAP features are present in the top five scikit-learn feature sets, and all three features contribute to the decision-making process of the classifier. These findings suggest that handcrafted features are effectively contributing to the classification of the data points.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 10,
                        "end": 11,
                        "text": "5",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "D. Results and Discussions",
                "sec_num": null
            },
            {
                "text": "In this paper, we proposed two approaches to detect AIgenerated text: a) machine learning based and b) text similarity based methods. In order to test our proposed methodologies, we collected two different kinds of datasets: Wikipedia-based articles and US Election 2024 News articles. After that, we proposed four different groups of handcrafted features to be extracted from the raw text: Basic NLP features, Term Frequencies and Ngram features, Topic Modeling Features, and Other features (readability score, grammar error, and NER count). Then, we performed three experiments to demonstrate the effectiveness of our proposed methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "V. CONCLUSION",
                "sec_num": null
            },
            {
                "text": "In the first experiment, after extracting the handcrafted features, we trained three machine learning models using RF, SVM, and XGBoost on the classification of human-written or ChatGPT-generated text. Both RF and XGBoost models perform well, with an F1 score of 0.9993. This approach proves to be effective in any domain with high accuracy in detection. To use this approach, we need to collect the ground truth texts (human-written) and ask AI tools such as ChatGPT to generate the artificial text based on the forming related questions from the ground truth texts. Our designed handcrafted features can help develop effective machine-learning models for the detection of AI-generated texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "V. CONCLUSION",
                "sec_num": null
            },
            {
                "text": "The second experiment was designed to demonstrate our second approach to detecting AI-generated text. This approach does not require collecting ground truth texts. Given any text that we want to know, either human-written or AIgenerated, based on topic modeling and keyword extraction, we formed related questions and asked AI tools to generate texts (ChatGPT in this project). Then, we applied a similar feature extraction in experiment 1. Finally, text similarity based on cosine will tell us the result. The results of our second experiment show that there is a clear classification boundary between human-written text and AI-generated text. This approach has proved effective and works in general situations, as no ground truth data needs to be collected.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "V. CONCLUSION",
                "sec_num": null
            },
            {
                "text": "To understand the roles and effectiveness of our designed handcrafted features, we conducted a feature importance analysis and built explainable models based on the SHAP library in our third experiment. According to our results, the Coleman liau score, word density, text error length, title word count, word count, and punctuation count are the top important training features to help differentiate humanwritten or ChatGPT-generated texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "V. CONCLUSION",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We thank Dr. Md Shohel Rana and Dr. Mohammad Nur Nobi for their valuable input and arguments on the project ideas and research methodologies.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACKNOWLEDGEMENT",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Large language models in machine translation",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Brants",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "C"
                        ],
                        "last": "Popat",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "J"
                        ],
                        "last": "Och",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Dean",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean, \"Large language models in machine translation,\" 2007.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Emergent abilities of large language models",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Tay",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Bommasani",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Raffel",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Zoph",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Borgeaud",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Yogatama",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Bosma",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Metzler",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2206.07682"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler et al., \"Emergent abilities of large language models,\" arXiv preprint arXiv:2206.07682, 2022.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "A survey of large language models",
                "authors": [
                    {
                        "first": "W",
                        "middle": [
                            "X"
                        ],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Hou",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Min",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.18223"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong et al., \"A survey of large language models,\" arXiv preprint arXiv:2303.18223, 2023.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Handbook of Natural Language Processing",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Indurkhya",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "J"
                        ],
                        "last": "Damerau",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. Indurkhya and F. J. Damerau, Handbook of Natural Language Processing, 2nd ed. Chapman & Hall/CRC, 2010.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Building Natural Language Generation Systems, ser. Natural Language Processing",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Reiter",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Dale",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "http://prp.contentdirections.com/mr/cupress.jsp/doi=10.2277/052102451X"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "E. Reiter and R. Dale, Building Natural Language Generation Systems, ser. Natural Language Processing. Cambridge University Press, 2000. [Online]. Available: http://prp.contentdirections.com/mr/cupress. jsp/doi=10.2277/052102451X",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bird",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Loper",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Bird, E. Klein, and E. Loper, Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. Beijing: O'Reilly, 2009. [Online]. Available: http://www.nltk.org/book",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Language models are few-shot learners",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Ryder",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Subbiah",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "D"
                        ],
                        "last": "Kaplan",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Dhariwal",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Neelakantan",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Shyam",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Sastry",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Advances in neural information processing systems",
                "volume": "33",
                "issue": "",
                "pages": "1877--1901",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., \"Language mod- els are few-shot learners,\" Advances in neural information processing systems, vol. 33, pp. 1877-1901, 2020.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Llama: Open and efficient foundation language models",
                "authors": [
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Touvron",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Lavril",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Izacard",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Martinet",
                        "suffix": ""
                    },
                    {
                        "first": "M.-A",
                        "middle": [],
                        "last": "Lachaux",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Lacroix",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Rozi\u00e8re",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Hambro",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Azhar",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2302.13971"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro, F. Azhar et al., \"Llama: Open and efficient foundation language models,\" arXiv preprint arXiv:2302.13971, 2023.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Devlin",
                        "suffix": ""
                    },
                    {
                        "first": "M.-W",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Toutanova",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.04805"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" arXiv preprint arXiv:1810.04805, 2018.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \"Roberta: A robustly optimized bert pretraining approach,\" 2019, cite arxiv:1907.11692. [Online]. Available: http://arxiv.org/abs/1907.11692",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter",
                "authors": [
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Sanh",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Debut",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Chaumond",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Wolf",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1910.01108"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "V. Sanh, L. Debut, J. Chaumond, and T. Wolf, \"Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,\" arXiv preprint arXiv:1910.01108, 2019.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Spanbert: Improving pre-training by representing and predicting spans",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [
                            "S"
                        ],
                        "last": "Weld",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Transactions of the association for computational linguistics",
                "volume": "8",
                "issue": "",
                "pages": "64--77",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Joshi, D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy, \"Spanbert: Improving pre-training by representing and predicting spans,\" Transactions of the association for computational linguistics, vol. 8, pp. 64-77, 2020.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Machine-generated text: A comprehensive survey of threat models and detection methods",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Crothers",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Japkowicz",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [
                            "L"
                        ],
                        "last": "Viktor",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "IEEE Access",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Crothers, N. Japkowicz, and H. L. Viktor, \"Machine-generated text: A comprehensive survey of threat models and detection methods,\" IEEE Access, 2023.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Human heuristics for aigenerated language are flawed",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Jakesch",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "T"
                        ],
                        "last": "Hancock",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Naaman",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the National Academy of Sciences",
                "volume": "120",
                "issue": "11",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Jakesch, J. T. Hancock, and M. Naaman, \"Human heuristics for ai- generated language are flawed,\" Proceedings of the National Academy of Sciences, vol. 120, no. 11, p. e2208839120, 2023.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Artificial intelligence versus maya angelou: Experimental evidence that people cannot differentiate aigenerated from human-written poetry",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "K\u00f6bis",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "D"
                        ],
                        "last": "Mossink",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Computers in human behavior",
                "volume": "114",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "N. K\u00f6bis and L. D. Mossink, \"Artificial intelligence versus maya angelou: Experimental evidence that people cannot differentiate ai- generated from human-written poetry,\" Computers in human behavior, vol. 114, p. 106553, 2021.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Will a Chatbot Write the Next 'Succession'? -nytimes.com",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "\"Will a Chatbot Write the Next 'Succession'? -ny- times.com,\" https://www.nytimes.com/2023/04/29/business/media/ writers-guild-hollywood-ai-chatgpt.html, [Accessed 16-08-2023].",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Council Post: 10 Ways Cybercriminals Can Abuse Large Language Models forbes.com",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Drolet",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "9304",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Drolet, \"Council Post: 10 Ways Cybercriminals Can Abuse Large Language Models - forbes.com,\" https://www.forbes.com/sites/forbestechcouncil/2023/06/30/ 10-ways-cybercriminals-can-abuse-large-language-models/?sh= 74175f9304c0, [Accessed 16-08-2023].",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "Deepfake text detection: Limitations and opportunities",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Pu",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Sarwar",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "M"
                        ],
                        "last": "Abdullah",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Rehman",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Bhattacharya",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Javed",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Viswanath",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "2023 IEEE Symposium on Security and Privacy (SP)",
                "volume": "",
                "issue": "",
                "pages": "1613--1630",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Pu, Z. Sarwar, S. M. Abdullah, A. Rehman, Y. Kim, P. Bhattacharya, M. Javed, and B. Viswanath, \"Deepfake text detection: Limitations and opportunities,\" in 2023 IEEE Symposium on Security and Privacy (SP). IEEE, 2023, pp. 1613-1630.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Automatic detection of generated text is easiest when humans are fooled",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Duckworth",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Ippolito, D. Duckworth, C. Callison-Burch, and D. Eck, \"Automatic detection of generated text is easiest when humans are fooled,\" 2020.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Detecting llm-generated text in computing education: A comparative study for chatgpt cases",
                "authors": [
                    {
                        "first": "M",
                        "middle": [
                            "S"
                        ],
                        "last": "Orenstrakh",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Karnalim",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "A"
                        ],
                        "last": "Suarez",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Liut",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. S. Orenstrakh, O. Karnalim, C. A. Suarez, and M. Liut, \"Detecting llm-generated text in computing education: A comparative study for chatgpt cases,\" 2023.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Testing of detection tools for ai-generated text",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Weber-Wulff",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Anohina-Naumeca",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bjelobaba",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Folt\u00fdnek",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Guerrero-Dib",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Popoola",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "\u0160igut",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Waddington",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Weber-Wulff, A. Anohina-Naumeca, S. Bjelobaba, T. Folt\u00fdnek, J. Guerrero-Dib, O. Popoola, P. \u0160igut, and L. Waddington, \"Testing of detection tools for ai-generated text,\" 2023.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "From human writing to artificial intelligence generated text: examining the prospects and potential threats of chatgpt in academic writing",
                "authors": [
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Dergaa",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Chamari",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Zmijewski",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [
                            "B"
                        ],
                        "last": "Saad",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Biology of Sport",
                "volume": "40",
                "issue": "2",
                "pages": "615--622",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "I. Dergaa, K. Chamari, P. Zmijewski, and H. B. Saad, \"From human writing to artificial intelligence generated text: examining the prospects and potential threats of chatgpt in academic writing,\" Biology of Sport, vol. 40, no. 2, pp. 615-622, 2023.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "Detection of ai-generated essays in writing assessment",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Fauss",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Hao",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Psychological Testing and Assessment Modeling",
                "volume": "65",
                "issue": "2",
                "pages": "125--144",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. Yan, M. Fauss, J. Hao, and W. Cui, \"Detection of ai-generated essays in writing assessment,\" Psychological Testing and Assessment Modeling, vol. 65, no. 2, pp. 125-144, 2023.",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Intrinsic dimension estimation for robust detection of ai-generated texts",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Tulchinskii",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Kuznetsov",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Kushnareva",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Cherniavskii",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Barannikov",
                        "suffix": ""
                    },
                    {
                        "first": "I",
                        "middle": [],
                        "last": "Piontkovskaya",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Nikolenko",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Burnaev",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2306.04723"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "E. Tulchinskii, K. Kuznetsov, L. Kushnareva, D. Cherniavskii, S. Baran- nikov, I. Piontkovskaya, S. Nikolenko, and E. Burnaev, \"Intrinsic dimen- sion estimation for robust detection of ai-generated texts,\" arXiv preprint arXiv:2306.04723, 2023.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Is this abstract generated by ai? a research for the gap between ai-generated scientific text and human-written scientific text",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Yi",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2301.10416"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Y. Ma, J. Liu, and F. Yi, \"Is this abstract generated by ai? a research for the gap between ai-generated scientific text and human-written scientific text,\" arXiv preprint arXiv:2301.10416, 2023.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Detection of fake generated scientific abstracts",
                "authors": [
                    {
                        "first": "P",
                        "middle": [
                            "C"
                        ],
                        "last": "Theocharopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Anagnostou",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Tsoukala",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "V"
                        ],
                        "last": "Georgakopoulos",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "K"
                        ],
                        "last": "Tasoulis",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [
                            "P"
                        ],
                        "last": "Plagianakos",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2304.06148"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "P. C. Theocharopoulos, P. Anagnostou, A. Tsoukala, S. V. Georgakopou- los, S. K. Tasoulis, and V. P. Plagianakos, \"Detection of fake generated scientific abstracts,\" arXiv preprint arXiv:2304.06148, 2023.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Comparing scientific abstracts generated by chatgpt to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers",
                "authors": [
                    {
                        "first": "C",
                        "middle": [
                            "A"
                        ],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "M"
                        ],
                        "last": "Howard",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "S"
                        ],
                        "last": "Markov",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "C"
                        ],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Ramesh",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "T"
                        ],
                        "last": "Pearson",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "BioRxiv",
                "volume": "",
                "issue": "",
                "pages": "2022--2034",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. A. Gao, F. M. Howard, N. S. Markov, E. C. Dyer, S. Ramesh, Y. Luo, and A. T. Pearson, \"Comparing scientific abstracts generated by chatgpt to original abstracts using an artificial intelligence output detector, plagiarism detector, and blinded human reviewers,\" BioRxiv, pp. 2022-12, 2022.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Challenges for enforcing editorial policies on ai-generated papers",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Accountability in Research",
                "volume": "",
                "issue": "",
                "pages": "1--3",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Hu, \"Challenges for enforcing editorial policies on ai-generated papers,\" Accountability in Research, pp. 1-3, 2023.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Game of tones: Faculty detection of gpt-4 generated content in university assessments",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Perkins",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Roe",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Postma",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Mcgaughran",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Hickerson",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Perkins, J. Roe, D. Postma, J. McGaughran, and D. Hickerson, \"Game of tones: Faculty detection of gpt-4 generated content in uni- versity assessments,\" 2023.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Argugpt: evaluating, understanding and identifying argumentative essays generated by gpt models",
                "authors": [
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Z",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Yue",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Zhao",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Y. Liu, Z. Zhang, W. Zhang, S. Yue, X. Zhao, X. Cheng, Y. Zhang, and H. Hu, \"Argugpt: evaluating, understanding and identifying argumenta- tive essays generated by gpt models,\" 2023.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kreps",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "M"
                        ],
                        "last": "Mccain",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Brundage",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Journal of experimental political science",
                "volume": "9",
                "issue": "1",
                "pages": "104--117",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Kreps, R. M. McCain, and M. Brundage, \"All the news that's fit to fabricate: Ai-generated text as a tool of media misinformation,\" Journal of experimental political science, vol. 9, no. 1, pp. 104-117, 2022.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Towards detection of ai-generated texts and misinformation",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Najee-Ullah",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Landeros",
                        "suffix": ""
                    },
                    {
                        "first": "S.-Y",
                        "middle": [],
                        "last": "Balytskyi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Workshop on Socio-Technical Aspects in Security",
                "volume": "",
                "issue": "",
                "pages": "194--205",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Najee-Ullah, L. Landeros, Balytskyi, and S.-Y. Chang, \"Towards detection of ai-generated texts and misinformation,\" in International Workshop on Socio-Technical Aspects in Security. Springer, 2021, pp. 194-205.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Ai or human: the socio-ethical implications of ai-generated media content",
                "authors": [
                    {
                        "first": "R",
                        "middle": [
                            "A"
                        ],
                        "last": "Partadiredja",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "E"
                        ],
                        "last": "Serrano",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Ljubenkov",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "2020 13th CMI Conference on Cybersecurity and Privacy (CMI)-Digital Transformation-Potentials and Challenges",
                "volume": "",
                "issue": "",
                "pages": "1--6",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. A. Partadiredja, C. E. Serrano, and D. Ljubenkov, \"Ai or human: the socio-ethical implications of ai-generated media content,\" in 2020 13th CMI Conference on Cybersecurity and Privacy (CMI)-Digital Transformation-Potentials and Challenges (51275). IEEE, 2020, pp. 1-6.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Combat ai with ai: Counteract machinegenerated fake restaurant reviews on social media",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Gambetti",
                        "suffix": ""
                    },
                    {
                        "first": "Q",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2302.07731"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "A. Gambetti and Q. Han, \"Combat ai with ai: Counteract machine- generated fake restaurant reviews on social media,\" arXiv preprint arXiv:2302.07731, 2023.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Q",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "G"
                        ],
                        "last": "Parker",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "De"
                        ],
                        "last": "Choudhury",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, ser. CHI '23",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1145/3544548.3581318"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "J. Zhou, Y. Zhang, Q. Luo, A. G. Parker, and M. De Choudhury, \"Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic and human solutions,\" in Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, ser. CHI '23. New York, NY, USA: Association for Computing Machinery, 2023. [Online]. Available: https://doi.org/10.1145/3544548.3581318",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "The science of detecting llmgenerated texts",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Y.-N",
                        "middle": [],
                        "last": "Chuang",
                        "suffix": ""
                    },
                    {
                        "first": "X",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Tang, Y.-N. Chuang, and X. Hu, \"The science of detecting llm- generated texts,\" 2023.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Automatic detection of machine generated text: A critical survey",
                "authors": [
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Jawahar",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Abdul-Mageed",
                        "suffix": ""
                    },
                    {
                        "first": "L",
                        "middle": [
                            "V S"
                        ],
                        "last": "Lakshmanan",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. Jawahar, M. Abdul-Mageed, and L. V. S. Lakshmanan, \"Automatic detection of machine generated text: A critical survey,\" 2020.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "On the possibilities of ai-generated text detection",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Chakraborty",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "S"
                        ],
                        "last": "Bedi",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "An",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Manocha",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2304.04736"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "S. Chakraborty, A. S. Bedi, S. Zhu, B. An, D. Manocha, and F. Huang, \"On the possibilities of ai-generated text detection,\" arXiv preprint arXiv:2304.04736, 2023.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Can ai-generated text be reliably detected?",
                "authors": [
                    {
                        "first": "V",
                        "middle": [
                            "S"
                        ],
                        "last": "Sadasivan",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Balasubramanian",
                        "suffix": ""
                    },
                    {
                        "first": "W",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Feizi",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.11156"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "V. S. Sadasivan, A. Kumar, S. Balasubramanian, W. Wang, and S. Feizi, \"Can ai-generated text be reliably detected?\" arXiv preprint arXiv:2303.11156, 2023.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Chatgpt or human? detect and explain. explaining decisions of machine learning model for detecting short chatgpt-generated text",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Mitrovi\u0107",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Andreoletti",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Ayoub",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2301.13852"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "S. Mitrovi\u0107, D. Andreoletti, and O. Ayoub, \"Chatgpt or human? detect and explain. explaining decisions of machine learning model for de- tecting short chatgpt-generated text,\" arXiv preprint arXiv:2301.13852, 2023.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Detectgpt: Zero-shot machine-generated text detection using probability curvature",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Y",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Khazatsky",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Finn",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2301.11305"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "E. Mitchell, Y. Lee, A. Khazatsky, C. D. Manning, and C. Finn, \"Detectgpt: Zero-shot machine-generated text detection using probability curvature,\" arXiv preprint arXiv:2301.11305, 2023.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Distinguishing human generated text from chatgpt generated text using machine learning",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Islam",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Sutradhar",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Noor",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "T"
                        ],
                        "last": "Raya",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "T"
                        ],
                        "last": "Maisha",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [
                            "M"
                        ],
                        "last": "Farid",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2306.01761"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "N. Islam, D. Sutradhar, H. Noor, J. T. Raya, M. T. Maisha, and D. M. Farid, \"Distinguishing human generated text from chatgpt generated text using machine learning,\" arXiv preprint arXiv:2306.01761, 2023.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Large language models can be guided to evade ai-generated text detection",
                "authors": [
                    {
                        "first": "N",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2305.10847"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "N. Lu, S. Liu, R. He, and K. Tang, \"Large language models can be guided to evade ai-generated text detection,\" arXiv preprint arXiv:2305.10847, 2023.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "The imitation game: Detecting human and ai-generated texts in the era of large language models",
                "authors": [
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Hayawi",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Shahriar",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [
                            "S"
                        ],
                        "last": "Mathew",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2307.12166"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "K. Hayawi, S. Shahriar, and S. S. Mathew, \"The imitation game: Detecting human and ai-generated texts in the era of large language models,\" arXiv preprint arXiv:2307.12166, 2023.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Introducing a Dataset to Detect GPT-Generated Text",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Bhat",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Bhat, \"Introducing a Dataset to Detect GPT- Generated Text,\" https://towardsdatascience.com/ introducing-a-dataset-to-detect-gpt-generated-text-96bb76dd2ed2, [Accessed 21-08-2023].",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Using tf-idf to determine word relevance in document queries",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Ramos",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. Ramos, \"Using tf-idf to determine word relevance in document queries,\" 1999.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Experience with a stack decoder-based hmm csr and back-off n-gram language models",
                "authors": [
                    {
                        "first": "D",
                        "middle": [
                            "B"
                        ],
                        "last": "Paul",
                        "suffix": ""
                    }
                ],
                "year": 1991,
                "venue": "Speech and Natural Language: Proceedings of a Workshop",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "D. B. Paul, \"Experience with a stack decoder-based hmm csr and back-off n-gram language models,\" in Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, February 19-22, 1991, 1991.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Named entity recognition without gazetteers",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mikheev",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Moens",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Grover",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Ninth Conference of the European Chapter",
                "volume": "",
                "issue": "",
                "pages": "1--8",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Mikheev, M. Moens, and C. Grover, \"Named entity recognition without gazetteers,\" in Ninth Conference of the European Chapter of the Association for Computational Linguistics, 1999, pp. 1-8.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Liii. on lines and planes of closest fit to systems of points in space",
                "authors": [
                    {
                        "first": "K",
                        "middle": [
                            "P F R S"
                        ],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1901,
                "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science",
                "volume": "2",
                "issue": "11",
                "pages": "559--572",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "K. P. F.R.S., \"Liii. on lines and planes of closest fit to systems of points in space,\" The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 2, no. 11, pp. 559-572, 1901.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "Random decision forests",
                "authors": [
                    {
                        "first": "T",
                        "middle": [
                            "K"
                        ],
                        "last": "Ho",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Proceedings of 3rd international conference on document analysis and recognition",
                "volume": "1",
                "issue": "",
                "pages": "278--282",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "T. K. Ho, \"Random decision forests,\" in Proceedings of 3rd international conference on document analysis and recognition, vol. 1. IEEE, 1995, pp. 278-282.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Support-vector networks",
                "authors": [
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Cortes",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Vapnik",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Machine learning",
                "volume": "20",
                "issue": "3",
                "pages": "273--297",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "C. Cortes and V. Vapnik, \"Support-vector networks,\" Machine learning, vol. 20, no. 3, pp. 273-297, 1995.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "XGBoost: A scalable tree boosting system",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Guestrin",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16",
                "volume": "",
                "issue": "",
                "pages": "785--794",
                "other_ids": {
                    "DOI": [
                        "10.1145/2939672.2939785"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "T. Chen and C. Guestrin, \"XGBoost: A scalable tree boosting system,\" in Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD '16. New York, NY, USA: ACM, 2016, pp. 785-794. [Online]. Available: http://doi.acm.org/10.1145/2939672.2939785",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "Cosine similarity to determine similarity measure: Study case in online essay assessment",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "R"
                        ],
                        "last": "Lahitani",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [
                            "E"
                        ],
                        "last": "Permanasari",
                        "suffix": ""
                    },
                    {
                        "first": "N",
                        "middle": [
                            "A"
                        ],
                        "last": "Setiawan",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "2016 4th International Conference on Cyber and IT Service Management",
                "volume": "",
                "issue": "",
                "pages": "1--6",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. R. Lahitani, A. E. Permanasari, and N. A. Setiawan, \"Cosine similarity to determine similarity measure: Study case in online essay assessment,\" in 2016 4th International Conference on Cyber and IT Service Management. IEEE, 2016, pp. 1-6.",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Natural language processing with Python: analyzing text with the natural language toolkit",
                "authors": [
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bird",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Klein",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Loper",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. Bird, E. Klein, and E. Loper, Natural language processing with Python: analyzing text with the natural language toolkit. \" O'Reilly Media, Inc.\", 2009.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Autocorrect",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Sondej",
                        "suffix": ""
                    }
                ],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Sondej, \"Autocorrect,\" https://github.com/filyp/autocorrect, [Accessed 21-8-2023].",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Beautiful soup documentation",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Richardson",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. Richardson, \"Beautiful soup documentation,\" April, 2007, [Accessed 21-08-2023].",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Autoencoding variational inference for topic models",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Srivastava",
                        "suffix": ""
                    },
                    {
                        "first": "C",
                        "middle": [],
                        "last": "Sutton",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Srivastava and C. Sutton, \"Autoencoding variational inference for topic models,\" 2017.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel",
                "authors": [
                    {
                        "first": "J",
                        "middle": [
                            "P"
                        ],
                        "last": "Kincaid",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "P"
                        ],
                        "last": "Fishburne",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [
                            "L"
                        ],
                        "last": "Rogers",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [
                            "S"
                        ],
                        "last": "Chissom",
                        "suffix": ""
                    }
                ],
                "year": 1975,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "J. P. Kincaid, R. P. Fishburne Jr, R. L. Rogers, and B. S. Chissom, \"Derivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel,\" 1975.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "A new readability yardstick",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Flesch",
                        "suffix": ""
                    }
                ],
                "year": 1948,
                "venue": "Journal of applied psychology",
                "volume": "32",
                "issue": "3",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Flesch, \"A new readability yardstick.\" Journal of applied psychology, vol. 32, no. 3, p. 221, 1948.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Judges scold lawyers for bad writing",
                "authors": [
                    {
                        "first": "W",
                        "middle": [
                            "H"
                        ],
                        "last": "Dubay",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Plain Language At Work Newsletter (Impact Information)",
                "volume": "",
                "issue": "8",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W. H. DuBay, \"Judges scold lawyers for bad writing,\" Plain Language At Work Newsletter (Impact Information)(8), 2004.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "A computer readability formula designed for machine scoring",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Coleman",
                        "suffix": ""
                    },
                    {
                        "first": "T",
                        "middle": [
                            "L"
                        ],
                        "last": "Liau",
                        "suffix": ""
                    }
                ],
                "year": 1975,
                "venue": "Journal of Applied Psychology",
                "volume": "60",
                "issue": "2",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "M. Coleman and T. L. Liau, \"A computer readability formula designed for machine scoring.\" Journal of Applied Psychology, vol. 60, no. 2, p. 283, 1975.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "A formula for predicting readability: Instructions",
                "authors": [
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Dale",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [
                            "S"
                        ],
                        "last": "Chall",
                        "suffix": ""
                    }
                ],
                "year": 1948,
                "venue": "Educational research bulletin",
                "volume": "",
                "issue": "",
                "pages": "37--54",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "E. Dale and J. S. Chall, \"A formula for predicting readability: Instruc- tions,\" Educational research bulletin, pp. 37-54, 1948.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Automated readability index",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Senter",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 1967,
                "venue": "DTIC document, Tech. Rep",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "R. Senter and E. A. Smith, \"Automated readability index,\" Technical report, DTIC document, Tech. Rep., 1967.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Readability helps the level",
                "authors": [
                    {
                        "first": "G",
                        "middle": [
                            "J"
                        ],
                        "last": "Christensen",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "G. J. Christensen, \"Readability helps the level,\" http://www.csun.edu/ \u223c vcecn006/read1.html, 2006, [Accessed 21-8-2023].",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "A comparison of the fry, spache, and harris-jacobson readability formulas for primary grades",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "J"
                        ],
                        "last": "Harris",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [
                            "D"
                        ],
                        "last": "Jacobson",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "The Reading Teacher",
                "volume": "33",
                "issue": "8",
                "pages": "920--924",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. J. Harris and M. D. Jacobson, \"A comparison of the fry, spache, and harris-jacobson readability formulas for primary grades,\" The Reading Teacher, vol. 33, no. 8, pp. 920-924, 1980.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Scikit-learn: Machine learning in Python",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pedregosa",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Varoquaux",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Gramfort",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Michel",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Thirion",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Grisel",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Blondel",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Prettenhofer",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Weiss",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Dubourg",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Vanderplas",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Passos",
                        "suffix": ""
                    },
                    {
                        "first": "D",
                        "middle": [],
                        "last": "Cournapeau",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Brucher",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Perrot",
                        "suffix": ""
                    },
                    {
                        "first": "E",
                        "middle": [],
                        "last": "Duchesnay",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Journal of Machine Learning Research",
                "volume": "12",
                "issue": "",
                "pages": "2825--2830",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander- plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch- esnay, \"Scikit-learn: Machine learning in Python,\" Journal of Machine Learning Research, vol. 12, pp. 2825-2830, 2011.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "API design for machine learning software: experiences from the scikit-learn project",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Buitinck",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Louppe",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Blondel",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Pedregosa",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Mueller",
                        "suffix": ""
                    },
                    {
                        "first": "O",
                        "middle": [],
                        "last": "Grisel",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Niculae",
                        "suffix": ""
                    },
                    {
                        "first": "P",
                        "middle": [],
                        "last": "Prettenhofer",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Gramfort",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Grobler",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Layton",
                        "suffix": ""
                    },
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Van-Derplas",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Joly",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Holt",
                        "suffix": ""
                    },
                    {
                        "first": "G",
                        "middle": [],
                        "last": "Varoquaux",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "ECML PKDD Workshop: Languages for Data Mining and Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "108--122",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "L. Buitinck, G. Louppe, M. Blondel, F. Pedregosa, A. Mueller, O. Grisel, V. Niculae, P. Prettenhofer, A. Gramfort, J. Grobler, R. Layton, J. Van- derPlas, A. Joly, B. Holt, and G. Varoquaux, \"API design for machine learning software: experiences from the scikit-learn project,\" in ECML PKDD Workshop: Languages for Data Mining and Machine Learning, 2013, pp. 108-122.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "A unified approach to interpreting model predictions",
                "authors": [
                    {
                        "first": "S",
                        "middle": [
                            "M"
                        ],
                        "last": "Lundberg",
                        "suffix": ""
                    },
                    {
                        "first": "S.-I",
                        "middle": [],
                        "last": "Lee ; I. Guyon",
                        "suffix": ""
                    },
                    {
                        "first": "U",
                        "middle": [
                            "V"
                        ],
                        "last": "Luxburg",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Bengio",
                        "suffix": ""
                    },
                    {
                        "first": "H",
                        "middle": [],
                        "last": "Wallach",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Fergus",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Vishwanathan",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Garnett",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "30",
                "issue": "",
                "pages": "4765--4774",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "S. M. Lundberg and S.-I. Lee, \"A unified approach to interpreting model predictions,\" in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 4765-4774. [Online]. Available: http://papers.nips.cc/ paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": "1",
                "text": "Fig. 1: Proposed Methodology",
                "type_str": "figure",
                "num": null
            },
            "FIGREF1": {
                "uris": null,
                "fig_num": "1",
                "text": "Data collection: As explained in section III-A, we collected two different kinds of raw datasets: -Wikipedia based dataset: For each section in the original Wikipedia articles, we have the human-written text as the ground truth (extracted from the original article) and the corresponding ChatGPTgenerated text. -US election 2024 related news article dataset: For each article related to the US election 2024 event, we have the human-written text (extracted from the article) and a maximum of 10 related ChatGPT generated text (in the form of question answering based on the extracted keywords from the article).",
                "type_str": "figure",
                "num": null
            },
            "FIGREF2": {
                "uris": null,
                "fig_num": "3",
                "text": "Fig. 3: US Election 2024 News Article based dataset generation process",
                "type_str": "figure",
                "num": null
            },
            "FIGREF3": {
                "uris": null,
                "fig_num": "4",
                "text": "Fig. 4: Feature Importance Classifiers",
                "type_str": "figure",
                "num": null
            },
            "TABREF1": {
                "text": "The number of pronoun lexicons. Term Frequencies and Ngram features: The occurrences of words in the vocabulary.",
                "content": "<table><tr><td>\u2022 Bigram words: The TFIDF features analyzed with the</td></tr><tr><td>bigram model at the word level limiting to a maximum</td></tr><tr><td>of 5000 features.</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF2": {
                "text": "Handcrafted feature descriptions and dimension size",
                "content": "<table><tr><td>Feature Group</td><td>Descriptions</td><td>Feature Count</td></tr><tr><td/><td>char count, word count, word density,</td><td/></tr><tr><td>Basic NLP</td><td>punctuation count, title word count, upper-case count, noun count, adv count,</td><td>11</td></tr><tr><td/><td>verb count, adj count, pro count</td><td/></tr><tr><td/><td>Count vect</td><td>35742</td></tr><tr><td>Term Frequencies</td><td>Bigram words</td><td>5000</td></tr><tr><td>and Ngram</td><td>Trigram words</td><td>5000</td></tr><tr><td/><td>BiTrigram chars</td><td>5000</td></tr><tr><td>Topic modeling</td><td>NeuralLDA [58]</td><td>20</td></tr><tr><td>Others</td><td>readability score, NER count, text error length</td><td>10</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF3": {
                "text": "Dataset size",
                "content": "<table><tr><td/><td colspan=\"2\">Wikipedia Dataset US Election Dataset</td></tr><tr><td>HWT (0)</td><td>3974</td><td>829</td></tr><tr><td>SGT (1)</td><td>4557</td><td>664</td></tr><tr><td>Total</td><td>8530</td><td>1493</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF4": {
                "text": "Hyperparameter settings and optimal values for the classification models",
                "content": "<table><tr><td>ML models</td><td>Hyperparameter search range</td><td>Optimal</td></tr><tr><td/><td>n estimators = {500, 1000, 1500, 2000}</td><td>500</td></tr><tr><td/><td>criterion = {gini, entropy}</td><td>gini</td></tr><tr><td>RF</td><td>min samples split = {2,4,6,8,10}</td><td>10</td></tr><tr><td/><td>min samples leaf = {2,4,6,8,10}</td><td>10</td></tr><tr><td/><td>max features = {auto, sqrt, log2}</td><td>sqrt</td></tr><tr><td/><td>C = {1,2,3,4}</td><td>4</td></tr><tr><td>SVM</td><td>kernel = {linear, poly, rbf} degree = {3,5,7}</td><td>rbf 7</td></tr><tr><td/><td>gamma = {scale, auto}</td><td>scale</td></tr><tr><td>XGBoost</td><td>n estimators = {500, 1000, 1500, 2000} learning rate = {0.005, 0.01, 0.15}</td><td>1000 0.01</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF5": {
                "text": "Classifier Results for Wikipedia Dataset",
                "content": "<table><tr><td/><td colspan=\"2\">Accuracy Precision</td><td>Recall</td><td>F1-score</td></tr><tr><td>RF</td><td>0.9993</td><td>0.9992</td><td>0.9993</td><td>0.9993</td></tr><tr><td>SVM</td><td>0.7421</td><td>0.7422</td><td>0.7389</td><td>0.765</td></tr><tr><td colspan=\"2\">XGBoost 0.9993</td><td>0.9993</td><td>0.9993</td><td>0.9993</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF6": {
                "text": "Cosine Similarity of US Election Dataset",
                "content": "<table><tr><td>(a) Feature Importance for Random Forest Classifier</td><td/><td/></tr><tr><td>(b) Feature Importance for XGB Classifier</td><td/><td/></tr><tr><td/><td colspan=\"2\">Features without TF-IFD</td></tr><tr><td colspan=\"2\">Cosine score Document count</td><td>Document %</td></tr><tr><td>-1 to -0.6</td><td>2</td><td>0.05%</td></tr><tr><td>-0.6 to -0.2</td><td>2787</td><td>76.50%</td></tr><tr><td>-0.2 to 0.6</td><td>817</td><td>22.43%</td></tr><tr><td>0.6 to 1</td><td>37</td><td>1.02%</td></tr><tr><td/><td colspan=\"2\">All Features</td></tr><tr><td colspan=\"2\">Cosine score Document count</td><td>Document %</td></tr><tr><td>-1 to -0.6</td><td>0</td><td>0.00%</td></tr><tr><td>-0.6 to -0.2</td><td>3537</td><td>97.09%</td></tr><tr><td>-0.2 to 0.6</td><td>106</td><td>2.91%</td></tr><tr><td>0.6 to 1</td><td>0</td><td>0.00%</td></tr><tr><td/><td colspan=\"2\">Features with PCA</td></tr><tr><td colspan=\"2\">Cosine score Document count</td><td>Document %</td></tr><tr><td>-1 to -0.6</td><td>0</td><td>0.00%</td></tr><tr><td>-0.6 to -0.2</td><td>3642</td><td>99.97%</td></tr><tr><td>-0.2 to 0.6</td><td>1</td><td>0.03%</td></tr><tr><td>0.6 to 1</td><td>0</td><td>0.00%</td></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            }
        }
    }
}