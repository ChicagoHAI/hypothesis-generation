{
    "paper_id": "1811",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-07-18T18:04:02.226473Z"
    },
    "title": "On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection",
    "authors": [
        {
            "first": "Vivian",
            "middle": [],
            "last": "Lai",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Colorado Boulder",
                "location": {}
            },
            "email": "vivian.lai@colorado.edu"
        },
        {
            "first": "Chenhao",
            "middle": [],
            "last": "Tan",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "University of Colorado Boulder",
                "location": {}
            },
            "email": "chenhao.tan@colorado.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency.\nIn this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.\n\u2022 Applied computing \u2192 Law, social and behavioral sciences.",
    "pdf_parse": {
        "paper_id": "1811",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            },
            {
                "text": "\u2022 Applied computing \u2192 Law, social and behavioral sciences.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Machine learning has achieved impressive success in a wide variety of tasks. For instance, neural networks have surpassed humanlevel performance in ImageNet classification (95.06% vs. 94.9%) [29] ; Kleinberg et al. [36] demonstrate that in bail decisions, machine predictions of recidivism can reduce jail rates by 41.9% with no increase in crime rates, compared to human judges; Ott et al. [60] show that linear classifiers can achieve \u223c90% accuracy in detecting deceptive reviews while humans perform no better than chance. As a result of these achievements, machine learning holds promise for addressing important societal challenges.",
                "cite_spans": [
                    {
                        "start": 191,
                        "end": 195,
                        "text": "[29]",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 215,
                        "end": 219,
                        "text": "[36]",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 391,
                        "end": 395,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "However, it is important to recognize different roles that machine learning can play in different tasks in the context of human decision making. In tasks such as object recognition, human performance can be considered as the upper bound, and machine learning models are designed to emulate the human ability to recognize objects in an image. A high accuracy in such tasks presents great opportunities for large-scale automation and consequently improving our society's efficiency. In contrast, efficiency is a lesser concern in tasks such as bail decisions. In fact, full automation is often not desired in these tasks due to ethical and legal concerns. These tasks are challenging for humans and for machines, but with vast amounts of data, machines can sometimes identify patterns that are unsalient, unknown, or counterintuitive to humans. If the patterns embedded in the machine learning models can be elucidated for humans, they can provide valuable support when humans make decisions.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "The goal of our work is to investigate best practices for integrating machine learning into human decision making. We propose a spectrum between full human agency, where humans make decisions entirely on their own, and full automation, where machines make decisions without human intervention (see Figure 1 for an illustration). We then develop varying levels of machine assistance along the spectrum using explanations and predictions of machine learning models. We build on recent developments in interpretable machine learning that provide useful frameworks for generating explanations of machine predictions [34, 35, 45, 50, 64, 65] . Instead of using these explanations to help users debug machine learning models, we incorporate the explanations as assistance for humans to improve human performance while retaining human agency in the decision making process. Accordingly, we directly evaluate human performance in the end task through user studies.",
                "cite_spans": [
                    {
                        "start": 612,
                        "end": 616,
                        "text": "[34,",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 617,
                        "end": 620,
                        "text": "35,",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 621,
                        "end": 624,
                        "text": "45,",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 625,
                        "end": 628,
                        "text": "50,",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 629,
                        "end": 632,
                        "text": "64,",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 633,
                        "end": 636,
                        "text": "65]",
                        "ref_id": "BIBREF64"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 305,
                        "end": 306,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "In this work, we focus on a constrained form of decision making where humans make individual predictions. Specifically, we ask humans to decide whether a hotel review is genuine or deceptive based on the text. This prediction problem allows us to focus on the integration of machine learning into human predictions. In comparison, prior work in decision theory and decision support systems focuses on modeling preferences and utilities as well as building Figure 1 : A spectrum between full human agency and full automation illustrating how machine learning can be integrated in human decision making. The detailed explanation of each method is in Section 3.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 463,
                        "end": 464,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "knowledge databases and representations to reason about complex decisions [5, 31, 33, 55, 67] . Moreover, since many policy decisions can be formulated as prediction problems [37] , understanding human predictions with assistance from machine learning models constitutes an important step towards empowering humans with machine learning in critical challenging tasks. Deception detection as a testbed. In this work, we use deception detection as our testbed for three reasons. First, deceptive information is prevalent on the Internet. For instance, Ott et al. [58] find that deceptive reviews are a growing problem on multiple platforms such as TripAdvisor and Yelp. Fake news has also received significant attention recently [43, 74] and might have influenced the outcome of the U.S. presidential election in 2016 [3] . Enhancing humans' ability in detecting deception can potentially alleviate these issues.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 77,
                        "text": "[5,",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 78,
                        "end": 81,
                        "text": "31,",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 82,
                        "end": 85,
                        "text": "33,",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 86,
                        "end": 89,
                        "text": "55,",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 90,
                        "end": 93,
                        "text": "67]",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 175,
                        "end": 179,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 561,
                        "end": 565,
                        "text": "[58]",
                        "ref_id": "BIBREF57"
                    },
                    {
                        "start": 727,
                        "end": 731,
                        "text": "[43,",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 732,
                        "end": 735,
                        "text": "74]",
                        "ref_id": "BIBREF73"
                    },
                    {
                        "start": 816,
                        "end": 819,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Second, deception detection is a challenging task for humans and has been extensively studied [1, 2, 22, 24, 60] . It is promising that machines show preliminary success in prior work. For example, machines are able to achieve an accuracy of \u223c90% in distinguishing genuine reviews from deceptive ones, while human performance is no better than chance [60] . Machines can identify unsalient and counterintuitive signals, e.g., deceptive reviews are less specific about spatial configurations and tend to include less sensorial and concrete language. It is worth noting that we should take the high machine accuracy with a grain of salt in the general domain because deception detection is a complex problem. 1 The task introduced by Ott et al. [60] nevertheless provides an ideal sandbox to understand human predictions with assistance from machine learning models.",
                "cite_spans": [
                    {
                        "start": 94,
                        "end": 97,
                        "text": "[1,",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 98,
                        "end": 100,
                        "text": "2,",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 101,
                        "end": 104,
                        "text": "22,",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 105,
                        "end": 108,
                        "text": "24,",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 109,
                        "end": 112,
                        "text": "60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 351,
                        "end": 355,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 743,
                        "end": 747,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Third, full automation is not desired in critical tasks such as deception detection because of ethical and legal concerns. The government should not have the authority to automatically block information from individuals, e.g., in the context of \"fake news\". Furthermore, full automation may not comply with legal requirements. For instance, in the case of recidivism prediction, the Wisconsin Supreme Court ruled that \"judges be made aware of the limitations of risk assessment tools\" and \"a COMPAS risk assessment should not be used to determine the severity of a sentence or whether an offender is incarcerated\" [47, 71] . Similarly, the trial judge is required to act as a gatekeeper regarding the evidence from a polygraph (lie detector) [70] . Therefore, it is crucial to retain human agency and understand human predictions with assistance from machine learning models.",
                "cite_spans": [
                    {
                        "start": 614,
                        "end": 618,
                        "text": "[47,",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 619,
                        "end": 622,
                        "text": "71]",
                        "ref_id": "BIBREF70"
                    },
                    {
                        "start": 742,
                        "end": 746,
                        "text": "[70]",
                        "ref_id": "BIBREF69"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Organization and Highlights. We start by reviewing related work to provide the necessary background for our study (Section 2). Our focus in this work is on investigating human predictions with assistance from machine learning models in the context of deceptive review detection. To explore the spectrum between full human agency and full automation in Figure 1 , we develop varying levels of assistance from machine learning models (Section 3). For example, the following three levels of machine assistance gradually increase the influence of machine predictions: 1) showing only explanations of machine predictions without revealing predicted labels; 2) showing predicted labels without revealing high machine accuracy; 3) showing predicted labels with an explicit statement of strong machine accuracy.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 359,
                        "end": 360,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "In Section 4, we investigate human performance under different experimental setups along the spectrum. We show that explanations alone slightly improve human performance, while showing predicted labels achieves great improvement (\u223c21% relative improvement in human accuracy). However, this improvement is still moderate compared to \"full\" priming with an explicit statement of machine accuracy (\u223c46% relative improvement in human accuracy). Our findings suggest that there exists a tradeoff between human performance and human agency. Interestingly, when predicted labels are shown, explanations of machine predictions can achieve a similar effect as an explicit statement of machine accuracy. We also find that humans tend to trust correct machine predictions more than incorrect ones, indicating that they can somewhat identify when machines are correct.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "We further examine the effect of statements of machine accuracy by varying the accuracy numbers (Section 5). Surprisingly, we find that our participants are not sensitive to statements of machine accuracy and are more likely to trust machine predictions with an accuracy statement than without, even if the accuracy statement suggests poor machine performance. These observations echo with prior work on numeracy and suggest that it is difficult for humans to interpret and act on numbers [6, 62, 63, 69] . We also find that frequency explanations (e.g., 5 out of 10 for explaining 50%) can help humans calibrate the accuracy numbers. Note that we do not recommend these presentations on the spectrum because they present untruthful information.",
                "cite_spans": [
                    {
                        "start": 489,
                        "end": 492,
                        "text": "[6,",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 493,
                        "end": 496,
                        "text": "62,",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 497,
                        "end": 500,
                        "text": "63,",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 501,
                        "end": 504,
                        "text": "69]",
                        "ref_id": "BIBREF68"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "We discuss the limitations of our work and provide concluding thoughts regarding future directions of investigating best practices for integrating artificial intelligence into human decision making in Section 6.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "We summarize related work in two areas to put our work in context: interpretable machine learning and deception and misinformation. Interpretable machine learning. Machine learning models remain as black boxes despite wide adoption. Blindly following machine predictions may lead to dire repercussions, especially in scenarios such as medical diagnosis and justice systems [9, 36, 73] . Therefore, improving their transparency and interpretability has attracted broad interest [34, 35, 45, 50, 64, 65] , dating back to early work on recommendation systems [13, 30] . In the case of general automation, researchers have also studied issues of appropriate reliance and trust [8, 18, 44, 61, 76] .",
                "cite_spans": [
                    {
                        "start": 373,
                        "end": 376,
                        "text": "[9,",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 377,
                        "end": 380,
                        "text": "36,",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 381,
                        "end": 384,
                        "text": "73]",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 477,
                        "end": 481,
                        "text": "[34,",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 482,
                        "end": 485,
                        "text": "35,",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 486,
                        "end": 489,
                        "text": "45,",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 490,
                        "end": 493,
                        "text": "50,",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 494,
                        "end": 497,
                        "text": "64,",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 498,
                        "end": 501,
                        "text": "65]",
                        "ref_id": "BIBREF64"
                    },
                    {
                        "start": 556,
                        "end": 560,
                        "text": "[13,",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 561,
                        "end": 564,
                        "text": "30]",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 673,
                        "end": 676,
                        "text": "[8,",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 677,
                        "end": 680,
                        "text": "18,",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 681,
                        "end": 684,
                        "text": "44,",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 685,
                        "end": 688,
                        "text": "61,",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 689,
                        "end": 692,
                        "text": "76]",
                        "ref_id": "BIBREF75"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK",
                "sec_num": "2"
            },
            {
                "text": "There are two major approaches to providing explanations of machine learning models: example-based and feature-based. For example, an example-based explanation framework is MMD-critic proposed by Kim et al. [34] , which selects both prototypes and criticisms. Ribeiro et al. [64] propose a feature-based approach, LIME, that fits a sparse linear model to approximate non-linear models locally. Similarly, Lundberg and Lee [50] present a unified framework that assigns each feature an importance value for a particular prediction.",
                "cite_spans": [
                    {
                        "start": 207,
                        "end": 211,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 275,
                        "end": 279,
                        "text": "[64]",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 422,
                        "end": 426,
                        "text": "[50]",
                        "ref_id": "BIBREF49"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK",
                "sec_num": "2"
            },
            {
                "text": "We would like to emphasize two unique aspects of our work: task difficulty and interpretability evaluation. First, compared to categorizing text into topics and object recognition, deception detection is a challenging task for humans and it remains an open question whether humans can leverage help from machine learning models in such settings. Second, we directly measure human performance in the end task. In comparison, prior work in interpretable machine learning aims to help humans understand how machine learning models work and/or debug them, the evaluation is thus mostly based on either the understanding of the models or the improvement in machine performance. Concurrently, several recent studies have also examined how explanations relate to human performance [10, 23] . Our work also resonates with the seminal work on mixed-initiative user interfaces [31] and intelligence augmentation [4] . In addition, our work is connected to cognitive studies on understanding effective explanations beyond the context of machine learning [48, 49] . Deception and misinformation. Deception is a widely studied phenomenon in many disciplines [75] . In psychology, deception is defined as an act that is intended to foster in another person a belief or understanding which the deceiver considers false [41] . To detect deception, researchers have examined the role of behavioral, emotional, and linguistic cues [17, 19, 39, 42, 54, 75] .",
                "cite_spans": [
                    {
                        "start": 774,
                        "end": 778,
                        "text": "[10,",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 779,
                        "end": 782,
                        "text": "23]",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 867,
                        "end": 871,
                        "text": "[31]",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 902,
                        "end": 905,
                        "text": "[4]",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 1043,
                        "end": 1047,
                        "text": "[48,",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 1048,
                        "end": 1051,
                        "text": "49]",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 1145,
                        "end": 1149,
                        "text": "[75]",
                        "ref_id": "BIBREF74"
                    },
                    {
                        "start": 1304,
                        "end": 1308,
                        "text": "[41]",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 1413,
                        "end": 1417,
                        "text": "[17,",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 1418,
                        "end": 1421,
                        "text": "19,",
                        "ref_id": "BIBREF18"
                    },
                    {
                        "start": 1422,
                        "end": 1425,
                        "text": "39,",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 1426,
                        "end": 1429,
                        "text": "42,",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 1430,
                        "end": 1433,
                        "text": "54,",
                        "ref_id": null
                    },
                    {
                        "start": 1434,
                        "end": 1437,
                        "text": "75]",
                        "ref_id": "BIBREF74"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK",
                "sec_num": "2"
            },
            {
                "text": "Since people are increasingly relying on online reviews to make purchase decisions [11, 72, 78, 81] , machine learning methods have been used to detect deception in online reviews [22, 24, 32, 60, 77, 79] . An important challenge in detecting deception in online reviews is to obtain the groundtruth labels of reviews. Ott et al. [60] create the first sizable dataset in deception detection by asking Amazon Mechanical Turkers to write deceptive reviews. Deceptive reviews can also be seen as an instance of spamming and online fraud [2, 16, 27, 56] .",
                "cite_spans": [
                    {
                        "start": 83,
                        "end": 87,
                        "text": "[11,",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 88,
                        "end": 91,
                        "text": "72,",
                        "ref_id": "BIBREF71"
                    },
                    {
                        "start": 92,
                        "end": 95,
                        "text": "78,",
                        "ref_id": "BIBREF77"
                    },
                    {
                        "start": 96,
                        "end": 99,
                        "text": "81]",
                        "ref_id": "BIBREF80"
                    },
                    {
                        "start": 180,
                        "end": 184,
                        "text": "[22,",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 185,
                        "end": 188,
                        "text": "24,",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 189,
                        "end": 192,
                        "text": "32,",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 193,
                        "end": 196,
                        "text": "60,",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 197,
                        "end": 200,
                        "text": "77,",
                        "ref_id": "BIBREF76"
                    },
                    {
                        "start": 201,
                        "end": 204,
                        "text": "79]",
                        "ref_id": "BIBREF78"
                    },
                    {
                        "start": 330,
                        "end": 334,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 534,
                        "end": 537,
                        "text": "[2,",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 538,
                        "end": 541,
                        "text": "16,",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 542,
                        "end": 545,
                        "text": "27,",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 546,
                        "end": 549,
                        "text": "56]",
                        "ref_id": "BIBREF55"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK",
                "sec_num": "2"
            },
            {
                "text": "More recently, the issue of misinformation and fake news has drawn much attention from both the public and the research community [21, 43] . Most relevant to our work is Zhang et al. [80] , which explores varying types of credibility annotations specifically designed for news articles. In addition, Nyhan and Reifler [57] demonstrate the \"backfire\" effect, which suggests that corrections of misperceptions may enhance people's false beliefs, and Vosoughi et al. [74] show that fake news is more innovative and spreads faster than real news.",
                "cite_spans": [
                    {
                        "start": 130,
                        "end": 134,
                        "text": "[21,",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 135,
                        "end": 138,
                        "text": "43]",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 183,
                        "end": 187,
                        "text": "[80]",
                        "ref_id": "BIBREF79"
                    },
                    {
                        "start": 318,
                        "end": 322,
                        "text": "[57]",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 464,
                        "end": 468,
                        "text": "[74]",
                        "ref_id": "BIBREF73"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK",
                "sec_num": "2"
            },
            {
                "text": "It is worth noting that deception detection is a broad and complex issue. For instance, fake news can be hard to define and may not be easily separated into two classes. Moreover, detecting fake news is different from detecting deceptive reviews as the former task requires other skills such as fact checking. It is important to note that our focus in this work is on investigating how humans interact with assistance from machine learning algorithms in decision making. We thus adopt the task of distinguishing genuine reviews from deceptive ones based on textual information in Ott et al. [60] as a sandbox. Our results on this constrained deception detection task can potentially contribute valuable insights to future solutions of the broader issue of deception detection.",
                "cite_spans": [
                    {
                        "start": 591,
                        "end": 595,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RELATED WORK",
                "sec_num": "2"
            },
            {
                "text": "Our goal is to understand whether machine predictions and their explanations can improve human performance in challenging tasks, such as deception detection, and how humans interpret assistance from machine learning models. In this section, we first present our task setup and then develop varying levels of machine assistance along the spectrum introduced in Figure 1 . We finally formulate our hypotheses and define our evaluation metrics. Experimental setup. We employ the deception detection task developed by Ott et al. [60] and evaluate human performance in this task with varying levels of machine assistance. The dataset in Ott et al. [60] includes 800 genuine and 800 deceptive hotel reviews for 20 hotels in Chicago. The genuine reviews were extracted from TripAdvisor and the deceptive ones were written by turkers. We use 80% of the reviews as training data and the remaining 20% as the heldout test set. Since the machine performance with linear SVM in Ott et al. [60] already surpasses humans (\u223c50%) by a wide margin and linear classifiers are generally considered more interpretable, we follow Ott et al. [59] and use linear SVM with bag-of-words features as our machine learning model. The linear SVM classifier achieves an accuracy of 87% on the heldout test set.",
                "cite_spans": [
                    {
                        "start": 525,
                        "end": 529,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 643,
                        "end": 647,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 977,
                        "end": 981,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 1120,
                        "end": 1124,
                        "text": "[59]",
                        "ref_id": "BIBREF58"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 367,
                        "end": 368,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETUP AND HYPOTHESES",
                "sec_num": "3"
            },
            {
                "text": "Our main task in this paper is to evaluate human performance with assistance from machine learning models. To do that, we conduct a user study on Amazon Mechanical Turk. Turkers are recruited to determine whether a review in the heldout test set is genuine or deceptive. In other words, humans are asked to perform the same task as the machine on the test set. We follow a between-subject design: each turker is assigned a level of machine assistance along the spectrum (Figure 1 ) and labels 20 reviews after going through three training examples and correctly answering an attention-check question. To incentivize turkers to perform at their best, we provide 40% bonus for each correct prediction in addition to the 5 cent base rate for a review. We also solicit our participants' estimation of their own performance and basic demographic information such as gender and education background through an exit survey. We only allow a turker to participate in the study once to guarantee sample independence across experimental 2b shows both the predicted label and an explicit statement about machine accuracy (87%). Figure 2c shows the predicted label with heatmap, but does not present machine accuracy. We crop the \"Genuine\" and \"Deceptive\" buttons in Figure 2b and 2c to save space. setups. Given that there are 320 test reviews and that we collect five turker predictions for each review, each experimental setup has a total of 80 turkers. Refer to the appendix for more details regarding our user study and survey questions. Varying levels of machine assistance. Humans are the main agents in our experiments and make final decisions; machines only provide assistance, which can be ignored if humans deem it useless. An ideal outcome is that human performance can be improved with minimal information from machine learning models so that humans retain their agency in the decision making process. To examine how humans perform under different levels of influence from machine learning models, we consider the following presentations along the spectrum in Figure 1 (we only show three interfaces in Figure 2 for space reasons; see the appendix for more).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 478,
                        "end": 479,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 1026,
                        "end": 1028,
                        "text": "2b",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1123,
                        "end": 1125,
                        "text": "2c",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1261,
                        "end": 1263,
                        "text": "2b",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 2067,
                        "end": 2068,
                        "text": "1",
                        "ref_id": null
                    },
                    {
                        "start": 2110,
                        "end": 2111,
                        "text": "2",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETUP AND HYPOTHESES",
                "sec_num": "3"
            },
            {
                "text": "\u2022 Control. Humans are only presented a review. This setup contains no information from machine learning models and humans have full agency.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETUP AND HYPOTHESES",
                "sec_num": "3"
            },
            {
                "text": "\u2022 Feature-based explanations. Since our machine learning model is linear, we present two versions of feature-based explanations by highlighting words based on absolute values of weight coefficients. First, we highlight the top 10 words in each review with the same color (highlight). Second, we use heatmap to show gradual changes in weight coefficients among the top 10 words. The most heavily-weighted words are highlighted in the darkest shade of blue. Soft-highlighting (heatmap) has been shown to improve visual search on targeted areas for humans [40] . Note that we do not indicate the sign of features to avoid revealing predicted labels. Humans may pay extra attention to the highlighted words and accordingly make decisions on their own. Figure 2a shows an example interface for heatmap. (b) Human accuracy with predicted labels (and other information).",
                "cite_spans": [
                    {
                        "start": 553,
                        "end": 557,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 755,
                        "end": 757,
                        "text": "2a",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETUP AND HYPOTHESES",
                "sec_num": "3"
            },
            {
                "text": "Figure 3 : Human accuracy with varying levels of assistance. In Figure 3a , control provides no assistance; examples, highlight, and heatmap present explanations of machine predictions alone; predicted label w/o accuracy shows predicted labels; predicted label w/ accuracy shows predicted labels and reports machine accuracy that suggests strong machine performance. It is clear that showing predicted labels is crucial for improving human accuracy. Adding an explicit statement of machine accuracy further improves human accuracy. Figure 3b further investigates the combinations of predicted labels and their explanations, and presents machine performance as a benchmark. Intriguingly, we find that adding explanations achieves a similar effect as adding an explicit statement of machine accuracy. All p-values are computed by conducting t-test between the corresponding setup and the first experimental setup in the figure (\"control\" in Figure 3a and \"predicted label w/o accuracy\" in Figure 3b ).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 7,
                        "end": 8,
                        "text": "3",
                        "ref_id": null
                    },
                    {
                        "start": 71,
                        "end": 73,
                        "text": "3a",
                        "ref_id": null
                    },
                    {
                        "start": 539,
                        "end": 541,
                        "text": "3b",
                        "ref_id": null
                    },
                    {
                        "start": 946,
                        "end": 948,
                        "text": "3a",
                        "ref_id": null
                    },
                    {
                        "start": 994,
                        "end": 996,
                        "text": "3b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETUP AND HYPOTHESES",
                "sec_num": "3"
            },
            {
                "text": "neighbor classifiers. Humans can potentially make better decisions in this setup than in control by comparing the similarity between reviews. \u2022 Predicted label without accuracy. The above two approaches only show explanations of machine predictions, but do not reveal any information about predicted labels. The next level of priming presents the predicted label. If humans fully follow machine predictions, they will perform much better than chance and likely lead to an upper bound in this deception detection task for humans. However, humans may not trust the machine due to algorithm aversion [15] . \u2022 Predicted label with accuracy. We may further influence human decisions by explicitly suggesting that machines perform well in this task with 87% accuracy. Figure 2b shows an example for predicted label with accuracy. Note that such strong recommendations may not be desired due to ethical and legal concerns (see our discussion in the introduction). \u2022 Combinations. Finally, we combine feature (example)-based explanations and predicted labels. Note that we do not show machine performance to avoid strong priming. Figure 2c shows an example of predicted label + heatmap without information about machine performance. Hypotheses. We formulate the following hypotheses regarding how well humans can perform with machine assistance and how often humans trust machine predictions when predicted labels are available.",
                "cite_spans": [
                    {
                        "start": 597,
                        "end": 601,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 769,
                        "end": 771,
                        "text": "2b",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 1129,
                        "end": 1131,
                        "text": "2c",
                        "ref_id": "FIGREF1"
                    }
                ],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETUP AND HYPOTHESES",
                "sec_num": "3"
            },
            {
                "text": "\u2022 Hypothesis 1a. Feature-based explanations and example-based explanations improve human performance over control. \u2022 Hypothesis 1b. Heatmap is more effective than highlight as gradual changes in weight coefficients can be useful, as shown in Kneusel and Mozer [40] for visual search. Feature-based explanations are more effective than example-based explanations since the latter requires a greater cognitive load, i.e., reading two more reviews. \u2022 Hypothesis 2. Showing predicted labels significantly improves human performance compared to feature (example)-based explanations alone. Assuming that humans trust the machine and follow its prediction, showing predicted labels can likely improve human performance because the machine accuracy is 87%. However, showing predicted labels reduces human agency, so it is important to understand the size of the performance gap and make informed design choices. \u2022 Hypothesis 3. By combining predicted labels and feature (example)based explanations, the trust that humans place on machine predictions increases, as it has been shown that concrete details can influence the level of trust in general automation [44] . We evaluate the above hypotheses using two metrics, accuracy and trust. Accuracy is defined as the percentage of correctly predicted instances by humans; trust is defined as the percentage of instances for which humans follow the machine prediction. Note that we can only compute trust when predicted labels are available.",
                "cite_spans": [
                    {
                        "start": 260,
                        "end": 264,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 1151,
                        "end": 1155,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "EXPERIMENTAL SETUP AND HYPOTHESES",
                "sec_num": "3"
            },
            {
                "text": "In this section, we investigate how varying levels of assistance from machine learning models along the spectrum in Figure 1 affect human predictions. We first discuss aggregate human performance using human accuracy and trust. Our results show that in this challenging task, explanations alone slightly improve human performance, while showing predicted labels can significantly improve human performance. When predicted labels are shown, we examine the level of trust that humans place on machine predictions. Our results suggest that humans can somewhat differentiate correct machine predictions from incorrect ones. Finally, we present individual differences among our participants based on information collected in the exit survey. Our dataset and demonstration are available at https://deception.machineintheloop.com/.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 123,
                        "end": 124,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "RESULTS",
                "sec_num": "4"
            },
            {
                "text": "We first present human accuracy measured by the percentage of correctly predicted instances by humans. Our results suggest that Figure 4 : The trust that humans place on machine predictions. Figure 4a shows that adding feature-based explanations (heatmap) can effectively increase the trust level compared to predicted label w/o accuracy. p-value in Figure 4a is computed by conducting t-test between the corresponding setup and predicted label w/o accuracy. Figure 4b breaks down the trust based on whether machine predictions are correct or incorrect and shows that humans trust correct machine predictions more than the incorrect ones in all the five experimental setups, although the differences are only statistically significant in two setups.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 135,
                        "end": 136,
                        "text": "4",
                        "ref_id": null
                    },
                    {
                        "start": 198,
                        "end": 200,
                        "text": "4a",
                        "ref_id": null
                    },
                    {
                        "start": 357,
                        "end": 359,
                        "text": "4a",
                        "ref_id": null
                    },
                    {
                        "start": 466,
                        "end": 468,
                        "text": "4b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "showing predicted labels is crucial for improving human performance. Featured-based explanations coupled with predicted labels are able to induce similar human performance as an explicit statement of strong machine accuracy. As such, adding feature-based explanations to predicted labels may be more ideal than suggesting strong machine performance as the priming is weaker and may facilitate a higher level of human agency in decision making.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "Explanations alone slightly improve human performance (Figure 3a ). As Figure 3a shows, human performance in control is no better than chance (51.1%). This finding is consistent with Ott et al. [60] and decades of research on deception detection [7] .",
                "cite_spans": [
                    {
                        "start": 194,
                        "end": 198,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 246,
                        "end": 249,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 62,
                        "end": 64,
                        "text": "3a",
                        "ref_id": null
                    },
                    {
                        "start": 78,
                        "end": 80,
                        "text": "3a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "Explanations alone slightly improve human performance over control, and the differences are statistically significant for highlight and heatmap, not for examples. However, the best explanations, heatmap, is not statistically significantly different from highlight (p = 0.335) or examples (p = 0.069). As a result, our findings partially supports Hypothesis 1a and rejects Hypothesis 1b.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "These findings suggest that it is difficult for humans to understand explanations on their own. This is plausible for example-based explanations since it requires extra cognitive burden and estimating text similarity is a nontrivial task for humans. For feature-based explanations, it seems that the improvement is driven by the small number of training reviews that we provide to explain the task. First-person singular pronouns provide a good example: one of the training reviews is deceptive and highlight many occurrences of the word, \"my\". A participant said, \"I tried to match the pattern from the example. In the example. the review with the most \"My's\" and \"I's\" were deceptive\". In other words, the improvement in heatmap and highlight may not happen at all without the training reviews, which indicates the difficulty of interpreting these feature-based explanations and the importance of explaining the explanations. One possible direction is to develop automatic tutorials to teach the intuitions behind important features, which is related to machine teaching [51, 68, 82] . Showing predicted labels significantly improves human performance (Figure 3a and 3b ). As Figure 3a shows, showing predicted labels drastically improves human performance (61.9% for predicted label w/o accuracy, a 21% relative improvement over control; the difference with heatmap is statistically significant (p <0.001)).",
                "cite_spans": [
                    {
                        "start": 1073,
                        "end": 1077,
                        "text": "[51,",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 1078,
                        "end": 1081,
                        "text": "68,",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 1082,
                        "end": 1085,
                        "text": "82]",
                        "ref_id": "BIBREF81"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1162,
                        "end": 1164,
                        "text": "3a",
                        "ref_id": null
                    },
                    {
                        "start": 1169,
                        "end": 1171,
                        "text": "3b",
                        "ref_id": null
                    },
                    {
                        "start": 1185,
                        "end": 1187,
                        "text": "3a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "By presenting machine accuracy as shown in Figure 2b , the performance is further improved to 74.6% (predicted label w/ accuracy in Figure 3a , a 46% relative improvement over control).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 50,
                        "end": 52,
                        "text": "2b",
                        "ref_id": "FIGREF1"
                    },
                    {
                        "start": 139,
                        "end": 141,
                        "text": "3a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "These results are consistent with Hypothesis 2. The big performance gap between showing predicted labels and showing feature (example)-based explanations alone suggests that when humans interact with machine learning models, it makes a significant difference whether predicted labels are shown. However, this observation also echoes with concerns about humans overly relying on machines [44] .",
                "cite_spans": [
                    {
                        "start": 387,
                        "end": 391,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "To further understand human performance with predicted labels, we examine all experimental setups with predicted labels in Figure 3b . Although showing predicted labels seems necessary for achieving sizable human performance improvement, the effect of presenting machine accuracy can be moderated by showing feature (example)-based explanations. We find that predicted label + examples and predicted label + heatmap outperform predicted label w/o accuracy (69.7% and 72.5% vs. 61.9%), without presenting the machine accuracy. In this case, we observe that heatmap is more effective than examples, and leads to comparable human performance with predicted label w/ accuracy. There is still a gap between the best human performance (predicted label w/ accuracy) and machine performance (74.6% vs. 87.0%). These observations suggest that humans do not necessarily trust machine predictions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 130,
                        "end": 132,
                        "text": "3b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Human Accuracy",
                "sec_num": "4.1"
            },
            {
                "text": "We further examine the levels of trust that humans place on machine predictions when predicted labels are available. Since machine performance surpasses human performance in control by a wide margin in this task, higher levels of trust are correlated with higher levels of accuracy in our experiments. However, these two metrics capture different dimensions of human predictions because trust is tied to machine predictions. This becomes clear when we break down human trust by whether machine predictions are correct or not. We find that humans tend to trust correct machine predictions more than incorrect ones, which suggests that humans can somewhat effectively identify cases where machines are wrong. It is important to emphasize that our focus is on understanding how 5a shows performance estimation by participants in three different experimental setups. Figure 5b presents the performance of participants in predicted label + heatmap group by two variables, hint usefulness and gender.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 775,
                        "end": 777,
                        "text": "5a",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 870,
                        "end": 872,
                        "text": "5b",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Trust",
                "sec_num": "4.2"
            },
            {
                "text": "human trust varies along the spectrum rather than manipulating the trust of humans in machines. Feature (example)-based explanations increase the trust that humans place on machine predictions (Figure 4a ). We further introduce random heatmap by randomly highlighting an equal number of words as in heatmap to examine whether humans are influenced by any explanations including random ones.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 201,
                        "end": 203,
                        "text": "4a",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Trust",
                "sec_num": "4.2"
            },
            {
                "text": "Our results are consistent with Hypothesis 3: both feature-based and example-based explanations increase the trust of humans in machine predictions. In fact, predicted label + heatmap leads to a similar level of trust as predicted label w/ accuracy, although the latter explicitly tells humans that the machine learning model \"has an accuracy of approximately 87%\". In other words, when predicted labels are shown, heatmap can nudge humans in decision making without making strong statements of machine accuracy. Interestingly, random heatmap also increases the trust level significantly, suggesting that even irrelevant details can increase the trust of humans in machine predictions. The fact that heatmap is significantly more effective than random heatmap (78.7% vs. 73.4%, p < 0.001) indicates that humans can interpret valuable information in weight coefficients beyond \"the placebo effect\". Humans tend to trust machine predictions more when machine predictions are correct. (Figure 4b ). We next examine whether humans trust machine predictions more when machine predictions are correct than when they are incorrect. Figure 4b shows that in all the five experimental setups with predicted labels, our participants trust correct machine predictions more than incorrect ones. However, the difference is statistically significant only in predicted label w/ accuracy (p < 0.001) and predicted label w/ heatmap (random) (p = 0.015). These results suggest that humans can somewhat differentiate correct machine predictions from incorrect ones. Further evidence is required to fully understanding the reasons why humans (don't) trust (in)correct machine predictions. Such understandings can improve both machine learning models and their presentations to support human decision making.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 990,
                        "end": 992,
                        "text": "4b",
                        "ref_id": null
                    },
                    {
                        "start": 1132,
                        "end": 1134,
                        "text": "4b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Trust",
                "sec_num": "4.2"
            },
            {
                "text": "We finally discuss the heterogeneity between participants in our study. Here we focus on the participants' estimation of their own performance and gender differences. Refer to the appendix for additional comparisons.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Heterogeneity in Human Perception and Performance",
                "sec_num": "4.3"
            },
            {
                "text": "Human estimation of their own performance (Figure 5a ). We ask participants to estimate their own performance in our exit survey. Our results are not exactly aligned with the previous finding that humans tend to overestimate their capacity of detecting lying [20] . In fact, \u223c42% of the participants correctly predicted their performance. Among the remaining, \u223c18% overestimated their performance, while \u223c40% underestimated their performance. Figure 5a shows the breakdown for three experimental setups. In general, it seems difficult for humans to estimate their performance. One participant who overestimated his performance (estimated 11-15 but got 10 correct) said, \"I enjoyed this hit. When I was a young man, I was a manager in the hotel business and got to read a lot of comment cards from guests. I hope that I was pretty accurate in my answers\". Another participant who underestimated his performance (estimated 6-10 but got 15 correct) said, \"It was difficult to determine if they were genuine or deceptive. I don't feel certain on any of my choices\". Heterogeneity in performance across individuals (Figure 5b ).",
                "cite_spans": [
                    {
                        "start": 259,
                        "end": 263,
                        "text": "[20]",
                        "ref_id": "BIBREF19"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 50,
                        "end": 52,
                        "text": "5a",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 450,
                        "end": 452,
                        "text": "5a",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 1118,
                        "end": 1120,
                        "text": "5b",
                        "ref_id": "FIGREF5"
                    }
                ],
                "eq_spans": [],
                "section": "Heterogeneity in Human Perception and Performance",
                "sec_num": "4.3"
            },
            {
                "text": "We have so far focused on average human performance comparisons between different experimental setups. It is important to recognize that the performance of individuals can vary. Exit survey responses allow us to study such heterogeneity. We focus on two properties in the interest of space. Refer to the appendix for a complete discussion of heterogeneity between individuals. First, individuals who find the hints useful outperform those who find the hints not useful. The difference between these two groups in Figure 5b (predicted label + heatmap) is statistically significant. This observation resonates with our analysis regarding the trust of humans in machine predictions and holds in 5 out of 8 experimental setups (this question was not asked in control), although the differences are only statistically significant in three setups. 2 Second, we find that females generally outperform males. This observation holds in 8 out of 9 experimental setups, but none of the differences is statistically significant. Our results contribute to mixed observations regarding gender differences in deception detection [14, 46, 52, 53] . Figure 6 : Human accuracy and trust given varying statements of machine accuracy. Figure 6a and Figure 6b show that human accuracy and trust generally decline with statements of decreasing machine accuracy despite the fact that machine predictions remain unchanged. Note that the decline of human trust with statements of decreasing accuracy is small. Only by adding frequency explanations, human accuracy and trust become closer to not showing any indication of machine accuracy, i.e., predicted label w/o accuracy.",
                "cite_spans": [
                    {
                        "start": 1114,
                        "end": 1118,
                        "text": "[14,",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 1119,
                        "end": 1122,
                        "text": "46,",
                        "ref_id": "BIBREF45"
                    },
                    {
                        "start": 1123,
                        "end": 1126,
                        "text": "52,",
                        "ref_id": "BIBREF51"
                    },
                    {
                        "start": 1127,
                        "end": 1130,
                        "text": "53]",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 520,
                        "end": 522,
                        "text": "5b",
                        "ref_id": "FIGREF5"
                    },
                    {
                        "start": 1140,
                        "end": 1141,
                        "text": "6",
                        "ref_id": null
                    },
                    {
                        "start": 1222,
                        "end": 1224,
                        "text": "6a",
                        "ref_id": null
                    },
                    {
                        "start": 1236,
                        "end": 1238,
                        "text": "6b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Heterogeneity in Human Perception and Performance",
                "sec_num": "4.3"
            },
            {
                "text": "Given the strong influence of predicted labels and machine accuracy, a natural question to ask is how human judgment changes if we vary the statement of machine accuracy. For example, instead of the true accuracy of 87%, we could claim that the machine has an accuracy of 60%. It is important to emphasize that since these statements of accuracy are not true, we do not recommend this approach as part of our spectrum in Figure 1 and thus put these results in a separate section. However, we think that it is valuable to understand how varying statements of accuracy might influence human predictions.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 428,
                        "end": 429,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "VARYING STATEMENTS OF MACHINE ACCURACY",
                "sec_num": "5"
            },
            {
                "text": "Although human accuracy and trust generally decline with statements that suggest lower accuracy, statements of machine accuracy improve human trust in machine predictions even when the claimed accuracy is only 50%. To understand human accuracy with varying statements of machine accuracy, we use predicted label w/o accuracy and predicted label w/ accuracy (87%) as benchmarks. In Figure 6a and Figure 6b , human accuracy and trust with varying statements of machine accuracy all fall between these two benchmarks as expected. Here we focus on the blue bars filled with forward slashes that correspond to simple statements of machine accuracy, \"The machine predicts that the below review is deceptive. It has an accuracy of approximately x%\" (x = 70, 60, 50).",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 388,
                        "end": 390,
                        "text": "6a",
                        "ref_id": null
                    },
                    {
                        "start": 402,
                        "end": 404,
                        "text": "6b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "VARYING STATEMENTS OF MACHINE ACCURACY",
                "sec_num": "5"
            },
            {
                "text": "As the claimed accuracy declines from 87% to 50%, human accuracy and trust decrease, with the exception of human accuracy from 70% to 60%. However, the decline in human trust and accuracy is fairly small. For instance, predicted label w/ accuracy (50%) still outperforms predicted label w/o accuracy significantly. The results are surprising and counterintuitive since one should put less trust in a machine that has only an accuracy of 50% as compared to a machine that boasts 87%. Our findings suggest that any indication of machine accuracy, be it high or low, improves human trust in the machine. This observation echoes prior work on numeracy that suggests that average humans and even doctors struggle with interpreting and acting on numbers [6, 62, 63, 69] . Therefore, it is crucial that we develop a better empirical understanding of how humans interact with explanations and predictions of machine learning models in decision making before using these machine learning models in the loop of human decision making.",
                "cite_spans": [
                    {
                        "start": 748,
                        "end": 751,
                        "text": "[6,",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 752,
                        "end": 755,
                        "text": "62,",
                        "ref_id": "BIBREF61"
                    },
                    {
                        "start": 756,
                        "end": 759,
                        "text": "63,",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 760,
                        "end": 763,
                        "text": "69]",
                        "ref_id": "BIBREF68"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VARYING STATEMENTS OF MACHINE ACCURACY",
                "sec_num": "5"
            },
            {
                "text": "Frequency explanations can help humans interpret and act on statements of machine accuracy. To further investigate human interaction with varying statements of machine accuracy, we add frequency explanations to the statement with accuracy 50% and 60%. Specifically, we show participants \"The machine predicts that the below review is deceptive. It has an accuracy of approximately 50%, which means that it is correct 5 out of 10 times.\" instead of \"The machine predicts that the below review is deceptive. It has an accuracy of approximately 50%.\" The results are shown with the red bars filled with stars in Figure 6a and Figure 6b . We find that frequency explanations reduce the trust that humans place on machine predictions. For instance, human accuracy in predicted label w/ accuracy (50%) + frequency explanation is \u223c7% lower (p=0.003) than in predicted label w/ accuracy (50%). Similarly, human trust in predicted label w/ accuracy (50%) + frequency explanation is \u223c10% lower (p<0.001) than in predicted label w/ accuracy (50%). Furthermore, the differences in human accuracy and trust are not statistically significant between predicted label w/ accuracy (50%) + frequency explanation and predicted label w/o accuracy. These observations suggest that frequency explanations can help humans interpret statements of machine accuracy, in which case a statement of 50% accuracy with frequency explanation is almost the same as not showing machine accuracy. Our frequency explanations are also known as frequent format and have been shown to be more effective for conveying uncertainty than stating the probability [25, 26, 66] .",
                "cite_spans": [
                    {
                        "start": 1619,
                        "end": 1623,
                        "text": "[25,",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 1624,
                        "end": 1627,
                        "text": "26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1628,
                        "end": 1631,
                        "text": "66]",
                        "ref_id": "BIBREF65"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 616,
                        "end": 618,
                        "text": "6a",
                        "ref_id": null
                    },
                    {
                        "start": 630,
                        "end": 632,
                        "text": "6b",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "VARYING STATEMENTS OF MACHINE ACCURACY",
                "sec_num": "5"
            },
            {
                "text": "In this paper, we conduct the first empirical study to investigate whether machine predictions and their explanations can improve human performance in challenging tasks such as deception detection. We propose a spectrum between full human agency and full automation, and design machine assistance with varying levels of priming along the spectrum. We find that explanations alone slighlty improve human performance, while showing predicted labels significantly improves human performance. Adding an explicit statement of strong machine performance can further improve human performance. Our results demonstrate a tradeoff between human performance and human agency, and explaining machine predictions may moderate this tradeoff. We find interesting results regarding the trust that humans place on machine predictions. On the one hand, humans tend to trust correct machine predictions more than incorrect ones, which indicates that it is possible to improve human decision making while retaining human agency. On the other hand, we show that human trust can be easily enhanced by adding random heatmap as explanations or statements of low accuracies that do not justify trusting machine predictions. In other words, additional details including irrelevant ones can improve the trust that humans place on machine predictions. These findings highlight the importance of taking caution in using machine learning for supporting decision making and developing methods to improve the transparency of machine learning models and its associated human interpretation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUDING DISCUSSION",
                "sec_num": "6"
            },
            {
                "text": "As machine learning gets employed to support decision making in our society, it is crucial that the machine learning community not only advances machine learning models, but also develops a better understanding of how these machine learning models are used and how humans interact with these models in the process of decision making. Our study takes an initial step towards understanding human predictions with assistance from machine learning models in challenging tasks. Implications and future directions. Our results show that explanations alone slightly improve human performance. One reason for the limited improvement with explanations alone is that although we provide explanations during the decision making process, we provide limited resources to \"teach\" these explanations. A possible future direction is to develop tutorials for machine learning models and their explanations to relieve some cognitive burden from humans, e.g., summarizing the model as a list of rules, adding heatmap in examples or providing a sequence of training examples with explanations and sufficient coverage. This direction also connects to the area of machine teaching [51, 68, 82] .",
                "cite_spans": [
                    {
                        "start": 1159,
                        "end": 1163,
                        "text": "[51,",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 1164,
                        "end": 1167,
                        "text": "68,",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 1168,
                        "end": 1171,
                        "text": "82]",
                        "ref_id": "BIBREF81"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUDING DISCUSSION",
                "sec_num": "6"
            },
            {
                "text": "Another possible direction to improve the effectiveness of explanations is to provide narratives. Our results suggest that featurebased and example-based explanations provide useful details for machine predictions to improve the trust of humans in machine predictions. It can be useful if we can similarly provide rationales behind feature-based and example-based explanations in the form of narratives. A qualitative understanding of how turkers interpret hints from machine learning models may shed light on the requirements of effective narratives.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUDING DISCUSSION",
                "sec_num": "6"
            },
            {
                "text": "Last but not least, it is important to study the ethical concerns of providing assistance from machine learning models in human decision making. Our results demonstrate a clear tradeoff in this space: it is difficult to improve human performance without showing predicted labels, but showing predicted labels, especially alongside machine performance, runs the risk of removing human agency. Human decision makers with assistance from machines further complicate the current discussions on the issue of fairness in algorithmic decision making [12, 28, 38] . As the adoption of machine learning approaches can have broad impacts on our society, such questions require inputs from machine learning researchers, legal scholars, and the entire society.",
                "cite_spans": [
                    {
                        "start": 543,
                        "end": 547,
                        "text": "[12,",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 548,
                        "end": 551,
                        "text": "28,",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 552,
                        "end": 555,
                        "text": "38]",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUDING DISCUSSION",
                "sec_num": "6"
            },
            {
                "text": "Limitations. We use Amazon Mechanical Turk to recruit participants, but this may not be a representative sample of the population. However, we would like to emphasize that turkers are likely to provide a better proxy than machine learning experts for understanding how humans interact with assistance from machine learning models in critical challenging tasks. Also, our explanations are derived from a linear SVM classifier and nearest neighbors. It may be even more challenging for humans to interpret explanations of non-linear classifiers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUDING DISCUSSION",
                "sec_num": "6"
            },
            {
                "text": "Another important challenge in understanding how humans interact with machine learning models lies in the difficulty to assess the generalizability of our results. Our formulation of deception detection represents a scenario where machines outperform humans by a wide margin and humans may have developed false beliefs about this task, as most humans have read reviews online. In order to consider a wide range of tasks, e.g., bail decisions and medical diagnosis, we need a framework to compare different tasks. Machine performance and humans' prior intuition are probably important factors that can influence human interpretation of the explanations. However, it remains an open question whether there exists a principled framework to reason about these tasks. At the very least, it is important for our community to go beyond simple visual tasks such as OCR and object recognition, especially for the purpose of improving human performance in decision making. ",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUDING DISCUSSION",
                "sec_num": "6"
            },
            {
                "text": "Here we present further results on heterogeneous performance among individuals. We present figures for four experimental setups that are representative of different levels of priming: heatmap, examples, predicted label w/o accuracy, and predicted label + heatmap. Hint usefulness (Figure 9 ). As discussed in the main paper, human performance is better for participants who find hints useful than those who do not find hints useful in 5 out of 8 experimental setups. Highlight, heatmap and predicted label w/o accuracy are the exceptions. The difference in three setups (predicted label + heatmap, predicted label + heatmap (random), predicted label w/ accuracy) is statistically significant. Gender differences (Figure 10 ). Females generally outperform males, in 8 out of 9 experimental setups. None of the differences is statistically significant. Review sentiments (Figure 11 ). One possible hypothesis is that humans perform differently depending on the sentiment of reviews. Indeed, we observe that humans consistently perform better for positive reviews (8 out 9 experimental setups). However, the difference is only statistically significant for predicted label w/o accuracy. Education background (Figure 12 ). There is no clear trend regarding education background, which suggests that education levels do not correlate with the ability to detect deception. For instance, high school graduates perform the best in predicted label w/o accurcay, but the worst in examples. Since there are five groups, each group is relatively sparse. We thus did not conduct statistical testing for these observations. Age group (Figure 13 ). There is no clear trend regarding age groups either. For instance, participants that are 61 & above perform the best in predicted label w/o accuracy, but worst in predicted label + heatmap. Similarly, since there are five groups and that each group is also relatively sparse, we did not conduct statistical testing for these observations. Review experience (Figure 14 ). There is no clear trend regarding experience of writing reviews. With the exception of control and predicted label + heatmap (random), the group that reports the best performance is either users who write reviews weekly or users who write reviews frequently. Again, we did not conduct statistical testing for review experience. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 288,
                        "end": 289,
                        "text": "9",
                        "ref_id": "FIGREF9"
                    },
                    {
                        "start": 720,
                        "end": 722,
                        "text": "10",
                        "ref_id": "FIGREF10"
                    },
                    {
                        "start": 877,
                        "end": 879,
                        "text": "11",
                        "ref_id": "FIGREF11"
                    },
                    {
                        "start": 1213,
                        "end": 1215,
                        "text": "12",
                        "ref_id": "FIGREF13"
                    },
                    {
                        "start": 1628,
                        "end": 1630,
                        "text": "13",
                        "ref_id": "FIGREF14"
                    },
                    {
                        "start": 1999,
                        "end": 2001,
                        "text": "14",
                        "ref_id": "FIGREF16"
                    }
                ],
                "eq_spans": [],
                "section": "A.3 Individual Differences",
                "sec_num": null
            },
            {
                "text": "For instance, one can argue that it is impossible to fully address the issue of deception in online reviews only based on textual information as an adversarial user can copy another user's review, which becomes a deceptive review but with exactly the same text as a genuine one.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            },
            {
                "text": "The low number of statistically significant differences is expected, because human performance is low unless we show predicted labels.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "",
                "sec_num": null
            }
        ],
        "back_matter": [
            {
                "text": "We would like to thank Elizabeth Bradley, Michael Mozer, Sendhil Mullainathan, Amit Sharma, Adith Swaminathan, and anonymous reviewers for helpful discussions and feedback. This material is based upon work supported by the National Science Foundation under Grant No. 1837986. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "ACKNOWLEDGMENTS",
                "sec_num": null
            },
            {
                "text": "To ensure quality results, we include several criteria for turkers: 1) the turker is based in the United States so that we assume English fluency; 2) the turker has completed at least 50 HITs (human intelligence tasks); 3) the turker has an approval rate of at least 99%.Before working on the main task, turkers need to go through a short training session, in which we show three reviews from the training data. We present the correct answer after turkers make their prediction. The interface during training is exactly the same as in the actual experiment. After making predictions for 20 reviews, turkers are required to fill out an exit survey that solicits their estimation of their own performance in this task and basic demographic information including age, gender, education background, and experience with online reviews (screenshots in Figure 15 and Figure 16 ). If the HIT is approved, the turker is compensated a dollar and bonuses depending on the number of reviews he correctly predicted. For example, if a turker makes 11 correct predictions, he is compensated $0.22 in addition to a dollar. The average duration for finishing our HIT is about 11 minutes (Figure 7 shows the CDF of the duration). Turkers spend the shortest amount of time on average (8.3 minutes) in predicted labels w/ accuracy and the longest amount of time on average (14.4 minutes) in examples, which is consistent with our expectation about extra cognitive burden from reading two more reviews. To sanity check that participants pay similar attention throughout the study, Figure 8 shows the average accuracy with respect to the order in which reviews show up 3 : there does not exist a downward trend. All results are based on the 9 experimental setups in Section 4 of the main paper and results with varying statements of accuracy are not included.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 853,
                        "end": 855,
                        "text": "15",
                        "ref_id": null
                    },
                    {
                        "start": 867,
                        "end": 869,
                        "text": "16",
                        "ref_id": null
                    },
                    {
                        "start": 1178,
                        "end": 1179,
                        "text": "7",
                        "ref_id": null
                    },
                    {
                        "start": 1567,
                        "end": 1568,
                        "text": "8",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A APPENDIX A.1 Amazon Mechanical Turk Setup",
                "sec_num": null
            },
            {
                "text": "This section shows example interfaces for the other five experimental setups that are not shown in the main paper (predicted label + heatmap (random) has the same interface as predicted label + heatmap except that words are highlighted randomly).\u2022 Control (Figure 17a ).\u2022 Highlight (Figure 17b ).\u2022 Examples (Figure 18a ).\u2022 Predicted label w/o accuracy (Figure 18b ).\u2022 Predicted label + examples (Figure 19 ). 3 Thanks to suggestions from anonymous reviewers. ",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 264,
                        "end": 267,
                        "text": "17a",
                        "ref_id": null
                    },
                    {
                        "start": 290,
                        "end": 293,
                        "text": "17b",
                        "ref_id": null
                    },
                    {
                        "start": 315,
                        "end": 318,
                        "text": "18a",
                        "ref_id": null
                    },
                    {
                        "start": 360,
                        "end": 363,
                        "text": "18b",
                        "ref_id": null
                    },
                    {
                        "start": 403,
                        "end": 405,
                        "text": "19",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "A.2 Experiment Interfaces",
                "sec_num": null
            }
        ],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Deception detection using a multimodal approach",
                "authors": [
                    {
                        "first": "Mohamed",
                        "middle": [],
                        "last": "Abouelenien",
                        "suffix": ""
                    },
                    {
                        "first": "Veronica",
                        "middle": [],
                        "last": "P\u00e9rez-Rosas",
                        "suffix": ""
                    },
                    {
                        "first": "Rada",
                        "middle": [],
                        "last": "Mihalcea",
                        "suffix": ""
                    },
                    {
                        "first": "Mihai",
                        "middle": [],
                        "last": "Burzo",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of ICMI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mohamed Abouelenien, Veronica P\u00e9rez-Rosas, Rada Mihalcea, and Mihai Burzo. 2014. Deception detection using a multimodal approach. In Proceedings of ICMI.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Opinion Fraud Detection in Online Reviews by Network Effects",
                "authors": [
                    {
                        "first": "Leman",
                        "middle": [],
                        "last": "Akoglu",
                        "suffix": ""
                    },
                    {
                        "first": "Rishi",
                        "middle": [],
                        "last": "Chandy",
                        "suffix": ""
                    },
                    {
                        "first": "Christos",
                        "middle": [],
                        "last": "Faloutsos",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of ICWSM",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Leman Akoglu, Rishi Chandy, and Christos Faloutsos. 2013. Opinion Fraud Detection in Online Reviews by Network Effects.. In Proceedings of ICWSM.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Social media and fake news in the 2016 election",
                "authors": [
                    {
                        "first": "Hunt",
                        "middle": [],
                        "last": "Allcott",
                        "suffix": ""
                    },
                    {
                        "first": "Matthew",
                        "middle": [],
                        "last": "Gentzkow",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Journal of Economic Perspectives",
                "volume": "31",
                "issue": "",
                "pages": "211--236",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hunt Allcott and Matthew Gentzkow. 2017. Social media and fake news in the 2016 election. Journal of Economic Perspectives 31, 2 (2017), 211-236.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "An introduction to cybernetics",
                "authors": [
                    {
                        "first": "Ashby",
                        "middle": [],
                        "last": "Ross",
                        "suffix": ""
                    }
                ],
                "year": 1957,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "W Ross Ashby. 1957. An introduction to cybernetics. (1957).",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Statistical decision theory and Bayesian analysis",
                "authors": [
                    {
                        "first": "O",
                        "middle": [],
                        "last": "James",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Berger",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James O Berger. 2013. Statistical decision theory and Bayesian analysis. Springer Science & Business Media.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "When doctors meet numbers",
                "authors": [
                    {
                        "first": "Harvey",
                        "middle": [
                            "V"
                        ],
                        "last": "Donald M Berwick",
                        "suffix": ""
                    },
                    {
                        "first": "Milton",
                        "middle": [
                            "C"
                        ],
                        "last": "Fineberg",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Weinstein",
                        "suffix": ""
                    }
                ],
                "year": 1981,
                "venue": "The American journal of medicine",
                "volume": "71",
                "issue": "",
                "pages": "991--998",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Donald M Berwick, Harvey V Fineberg, and Milton C Weinstein. 1981. When doctors meet numbers. The American journal of medicine 71, 6 (1981), 991-998.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Accuracy of deception judgments",
                "authors": [
                    {
                        "first": "Charles F Bond",
                        "middle": [],
                        "last": "Jr",
                        "suffix": ""
                    },
                    {
                        "first": "Bella",
                        "middle": [
                            "M"
                        ],
                        "last": "Depaulo",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Personality and social psychology Review",
                "volume": "10",
                "issue": "3",
                "pages": "214--234",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Charles F Bond Jr and Bella M DePaulo. 2006. Accuracy of deception judgments. Personality and social psychology Review 10, 3 (2006), 214-234.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "The role of explanations on trust and reliance in clinical decision support systems",
                "authors": [
                    {
                        "first": "Adrian",
                        "middle": [],
                        "last": "Bussone",
                        "suffix": ""
                    },
                    {
                        "first": "Simone",
                        "middle": [],
                        "last": "Stumpf",
                        "suffix": ""
                    },
                    {
                        "first": "O'",
                        "middle": [],
                        "last": "Dympna",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Sullivan",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Healthcare Informatics (ICHI), 2015 International Conference on",
                "volume": "",
                "issue": "",
                "pages": "160--169",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adrian Bussone, Simone Stumpf, and Dympna O'Sullivan. 2015. The role of expla- nations on trust and reliance in clinical decision support systems. In Healthcare Informatics (ICHI), 2015 International Conference on. IEEE, 160-169.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
                "authors": [
                    {
                        "first": "Rich",
                        "middle": [],
                        "last": "Caruana",
                        "suffix": ""
                    },
                    {
                        "first": "Yin",
                        "middle": [],
                        "last": "Lou",
                        "suffix": ""
                    },
                    {
                        "first": "Johannes",
                        "middle": [],
                        "last": "Gehrke",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Koch",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Sturm",
                        "suffix": ""
                    },
                    {
                        "first": "Noemie",
                        "middle": [],
                        "last": "Elhadad",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of KDD",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rich Caruana, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015. Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission. In Proceedings of KDD.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Do explanations make VQA models more predictable to a human",
                "authors": [
                    {
                        "first": "Arjun",
                        "middle": [],
                        "last": "Chandrasekaran",
                        "suffix": ""
                    },
                    {
                        "first": "Viraj",
                        "middle": [],
                        "last": "Prabhu",
                        "suffix": ""
                    },
                    {
                        "first": "Deshraj",
                        "middle": [],
                        "last": "Yadav",
                        "suffix": ""
                    },
                    {
                        "first": "Prithvijit",
                        "middle": [],
                        "last": "Chattopadhyay",
                        "suffix": ""
                    },
                    {
                        "first": "Devi",
                        "middle": [],
                        "last": "Parikh",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. 2018. Do explanations make VQA models more predictable to a human?. In Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "The effect of word of mouth on sales: Online book reviews",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Judith",
                        "suffix": ""
                    },
                    {
                        "first": "Dina",
                        "middle": [],
                        "last": "Chevalier",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mayzlin",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Journal of marketing research",
                "volume": "43",
                "issue": "3",
                "pages": "345--354",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Judith A Chevalier and Dina Mayzlin. 2006. The effect of word of mouth on sales: Online book reviews. Journal of marketing research 43, 3 (2006), 345-354.",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "Algorithmic decision making and the cost of fairness",
                "authors": [
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Corbett-Davies",
                        "suffix": ""
                    },
                    {
                        "first": "Emma",
                        "middle": [],
                        "last": "Pierson",
                        "suffix": ""
                    },
                    {
                        "first": "Avi",
                        "middle": [],
                        "last": "Feller",
                        "suffix": ""
                    },
                    {
                        "first": "Sharad",
                        "middle": [],
                        "last": "Goel",
                        "suffix": ""
                    },
                    {
                        "first": "Aziz",
                        "middle": [],
                        "last": "Huq",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of KDD",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. 2017. Algorithmic decision making and the cost of fairness. In Proceedings of KDD.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Is seeing believing?: how recommender system interfaces affect users' opinions",
                "authors": [
                    {
                        "first": "Dan",
                        "middle": [],
                        "last": "Cosley",
                        "suffix": ""
                    },
                    {
                        "first": "K",
                        "middle": [],
                        "last": "Shyong",
                        "suffix": ""
                    },
                    {
                        "first": "Istvan",
                        "middle": [],
                        "last": "Lam",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [
                            "A"
                        ],
                        "last": "Albert",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Konstan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Riedl",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Proceedings of CHI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Dan Cosley, Shyong K Lam, Istvan Albert, Joseph A Konstan, and John Riedl. 2003. Is seeing believing?: how recommender system interfaces affect users' opinions. In Proceedings of CHI.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Sex differences in lying: How women and men deal with the dilemma of deceit",
                "authors": [
                    {
                        "first": "Jennifer",
                        "middle": [
                            "A"
                        ],
                        "last": "Bella M Depaulo",
                        "suffix": ""
                    },
                    {
                        "first": "Melissa",
                        "middle": [
                            "M"
                        ],
                        "last": "Epstein",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wyer",
                        "suffix": ""
                    }
                ],
                "year": 1993,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Bella M DePaulo, Jennifer A Epstein, and Melissa M Wyer. 1993. Sex differences in lying: How women and men deal with the dilemma of deceit. (1993).",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Algorithm aversion: People erroneously avoid algorithms after seeing them err",
                "authors": [
                    {
                        "first": "J",
                        "middle": [],
                        "last": "Berkeley",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [
                            "P"
                        ],
                        "last": "Dietvorst",
                        "suffix": ""
                    },
                    {
                        "first": "Cade",
                        "middle": [],
                        "last": "Simmons",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Massey",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Journal of Experimental Psychology: General",
                "volume": "144",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Berkeley J Dietvorst, Joseph P Simmons, and Cade Massey. 2015. Algorithm aversion: People erroneously avoid algorithms after seeing them err. Journal of Experimental Psychology: General 144, 1 (2015), 114.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Support vector machines for spam categorization",
                "authors": [
                    {
                        "first": "Harris",
                        "middle": [],
                        "last": "Drucker",
                        "suffix": ""
                    },
                    {
                        "first": "Donghui",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Vladimir",
                        "middle": [
                            "N"
                        ],
                        "last": "Vapnik",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "IEEE Transactions on Neural networks",
                "volume": "10",
                "issue": "",
                "pages": "1048--1054",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Harris Drucker, Donghui Wu, and Vladimir N Vapnik. 1999. Support vector machines for spam categorization. IEEE Transactions on Neural networks 10, 5 (1999), 1048-1054.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "Changes in language behavior as a function of veracity",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Earl",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dulaney",
                        "suffix": ""
                    }
                ],
                "year": 1982,
                "venue": "Human Communication Research",
                "volume": "9",
                "issue": "1",
                "pages": "75--82",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Earl F Dulaney. 1982. Changes in language behavior as a function of veracity. Human Communication Research 9, 1 (1982), 75-82.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "The role of trust in automation reliance",
                "authors": [
                    {
                        "first": "Mary",
                        "middle": [
                            "T"
                        ],
                        "last": "Dzindolet",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [
                            "A"
                        ],
                        "last": "Peterson",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [
                            "A"
                        ],
                        "last": "Pomranky",
                        "suffix": ""
                    },
                    {
                        "first": "Linda",
                        "middle": [
                            "G"
                        ],
                        "last": "Pierce",
                        "suffix": ""
                    },
                    {
                        "first": "Hall",
                        "middle": [
                            "P"
                        ],
                        "last": "Beck",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "International journal of human-computer studies",
                "volume": "58",
                "issue": "",
                "pages": "697--718",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mary T Dzindolet, Scott A Peterson, Regina A Pomranky, Linda G Pierce, and Hall P Beck. 2003. The role of trust in automation reliance. International journal of human-computer studies 58, 6 (2003), 697-718.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Facial signs of emotional experience",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Ekman",
                        "suffix": ""
                    },
                    {
                        "first": "Sonia",
                        "middle": [],
                        "last": "Wallace V Freisen",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ancoli",
                        "suffix": ""
                    }
                ],
                "year": 1980,
                "venue": "Journal of personality and social psychology",
                "volume": "39",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Ekman, Wallace V Freisen, and Sonia Ancoli. 1980. Facial signs of emotional experience. Journal of personality and social psychology 39, 6 (1980), 1125.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Effects of feedback on the overestimated capacity to detect lies and the underestimated ability to tell lies",
                "authors": [
                    {
                        "first": "Eitan",
                        "middle": [],
                        "last": "Elaad",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Applied Cognitive Psychology",
                "volume": "17",
                "issue": "",
                "pages": "349--363",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eitan Elaad. 2003. Effects of feedback on the overestimated capacity to detect lies and the underestimated ability to tell lies. Applied Cognitive Psychology 17, 3 (2003), 349-363.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Fake TV news: Widespread and undisclosed",
                "authors": [
                    {
                        "first": "Diane",
                        "middle": [],
                        "last": "Farsetta",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Price",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Center for Media and Democracy",
                "volume": "6",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Diane Farsetta and Daniel Price. 2006. Fake TV news: Widespread and undisclosed. Center for Media and Democracy 6 (2006).",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Syntactic stylometry for deception detection",
                "authors": [
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Ritwik",
                        "middle": [],
                        "last": "Banerjee",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Song Feng, Ritwik Banerjee, and Yejin Choi. 2012. Syntactic stylometry for deception detection. In Proceedings of ACL (short papers).",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play",
                "authors": [
                    {
                        "first": "Shi",
                        "middle": [],
                        "last": "Feng",
                        "suffix": ""
                    },
                    {
                        "first": "Jordan",
                        "middle": [],
                        "last": "Boyd-Graber",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1810.09648"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Shi Feng and Jordan Boyd-Graber. 2018. What can AI do for me: Evaluat- ing Machine Learning Interpretations in Cooperative Play. arXiv preprint arXiv:1810.09648 (2018).",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Detecting deceptive opinions with profile compatibility",
                "authors": [
                    {
                        "first": "Vanessa",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Feng",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Graeme",
                        "middle": [],
                        "last": "Hirst",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of IJCNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vanessa Wei Feng and Graeme Hirst. 2013. Detecting deceptive opinions with profile compatibility. In Proceedings of IJCNLP.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "The psychology of good judgment: frequency formats and simple algorithms",
                "authors": [
                    {
                        "first": "Gerd",
                        "middle": [],
                        "last": "Gigerenzer",
                        "suffix": ""
                    }
                ],
                "year": 1996,
                "venue": "Medical decision making",
                "volume": "16",
                "issue": "",
                "pages": "273--280",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gerd Gigerenzer. 1996. The psychology of good judgment: frequency formats and simple algorithms. Medical decision making 16, 3 (1996), 273-280.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "How to improve Bayesian reasoning without instruction: frequency formats",
                "authors": [
                    {
                        "first": "Gerd",
                        "middle": [],
                        "last": "Gigerenzer",
                        "suffix": ""
                    },
                    {
                        "first": "Ulrich",
                        "middle": [],
                        "last": "Hoffrage",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "Psychological review",
                "volume": "102",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Gerd Gigerenzer and Ulrich Hoffrage. 1995. How to improve Bayesian reasoning without instruction: frequency formats. Psychological review 102, 4 (1995), 684.",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Combating web spam with trustrank",
                "authors": [
                    {
                        "first": "Zolt\u00e1n",
                        "middle": [],
                        "last": "Gy\u00f6ngyi",
                        "suffix": ""
                    },
                    {
                        "first": "Hector",
                        "middle": [],
                        "last": "Garcia-Molina",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Pedersen",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Proceedings of VLDB",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zolt\u00e1n Gy\u00f6ngyi, Hector Garcia-Molina, and Jan Pedersen. 2004. Combating web spam with trustrank. In Proceedings of VLDB.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Equality of opportunity in supervised learning",
                "authors": [
                    {
                        "first": "Moritz",
                        "middle": [],
                        "last": "Hardt",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Price",
                        "suffix": ""
                    },
                    {
                        "first": "Nati",
                        "middle": [],
                        "last": "Srebro",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Moritz Hardt, Eric Price, and Nati Srebro. 2016. Equality of opportunity in supervised learning. In Proceedings of NIPS.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
                "authors": [
                    {
                        "first": "Kaiming",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Xiangyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of ICCV",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of ICCV.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Explaining collaborative filtering recommendations",
                "authors": [
                    {
                        "first": "Jonathan",
                        "middle": [
                            "L"
                        ],
                        "last": "Herlocker",
                        "suffix": ""
                    },
                    {
                        "first": "Joseph",
                        "middle": [
                            "A"
                        ],
                        "last": "Konstan",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Riedl",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "Proceedings of CSCW",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jonathan L Herlocker, Joseph A Konstan, and John Riedl. 2000. Explaining collaborative filtering recommendations. In Proceedings of CSCW.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "Principles of mixed-initiative user interfaces",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Horvitz",
                        "suffix": ""
                    }
                ],
                "year": 1999,
                "venue": "Proceedings of CHI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eric Horvitz. 1999. Principles of mixed-initiative user interfaces. In Proceedings of CHI.",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Opinion spam and analysis",
                "authors": [
                    {
                        "first": "Nitin",
                        "middle": [],
                        "last": "Jindal",
                        "suffix": ""
                    },
                    {
                        "first": "Bing",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Proceedings of WSDM",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Nitin Jindal and Bing Liu. 2008. Opinion spam and analysis. In Proceedings of WSDM.",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Decision support systems; an organizational perspective",
                "authors": [
                    {
                        "first": "G",
                        "middle": [
                            "W"
                        ],
                        "last": "Peter",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Keen",
                        "suffix": ""
                    }
                ],
                "year": 1978,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter GW Keen. 1978. Decision support systems; an organizational perspective. Technical Report.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Examples are not enough, learn to criticize! criticism for interpretability",
                "authors": [
                    {
                        "first": "Been",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Rajiv",
                        "middle": [],
                        "last": "Khanna",
                        "suffix": ""
                    },
                    {
                        "first": "Oluwasanmi",
                        "middle": [
                            "O"
                        ],
                        "last": "Koyejo",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Been Kim, Rajiv Khanna, and Oluwasanmi O Koyejo. 2016. Examples are not enough, learn to criticize! criticism for interpretability. In Proceedings of NIPS.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification",
                "authors": [
                    {
                        "first": "Been",
                        "middle": [],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Cynthia",
                        "middle": [],
                        "last": "Rudin",
                        "suffix": ""
                    },
                    {
                        "first": "Julie",
                        "middle": [
                            "A"
                        ],
                        "last": "Shah",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Been Kim, Cynthia Rudin, and Julie A Shah. 2014. The bayesian case model: A generative approach for case-based reasoning and prototype classification. In Proceedings of NIPS.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Human decisions and machine predictions",
                "authors": [
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Kleinberg",
                        "suffix": ""
                    },
                    {
                        "first": "Himabindu",
                        "middle": [],
                        "last": "Lakkaraju",
                        "suffix": ""
                    },
                    {
                        "first": "Jure",
                        "middle": [],
                        "last": "Leskovec",
                        "suffix": ""
                    },
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Ludwig",
                        "suffix": ""
                    },
                    {
                        "first": "Sendhil",
                        "middle": [],
                        "last": "Mullainathan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "The Quarterly Journal of Economics",
                "volume": "133",
                "issue": "",
                "pages": "237--293",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, and Sendhil Mullainathan. 2017. Human decisions and machine predictions. The Quarterly Journal of Economics 133, 1 (2017), 237-293.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Prediction policy problems",
                "authors": [
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Kleinberg",
                        "suffix": ""
                    },
                    {
                        "first": "Jens",
                        "middle": [],
                        "last": "Ludwig",
                        "suffix": ""
                    },
                    {
                        "first": "Sendhil",
                        "middle": [],
                        "last": "Mullainathan",
                        "suffix": ""
                    },
                    {
                        "first": "Ziad",
                        "middle": [],
                        "last": "Obermeyer",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "American Economic Review",
                "volume": "105",
                "issue": "",
                "pages": "491--495",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Ziad Obermeyer. 2015. Prediction policy problems. American Economic Review 105, 5 (2015), 491-95.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Inherent trade-offs in the fair determination of risk scores",
                "authors": [
                    {
                        "first": "Jon",
                        "middle": [],
                        "last": "Kleinberg",
                        "suffix": ""
                    },
                    {
                        "first": "Sendhil",
                        "middle": [],
                        "last": "Mullainathan",
                        "suffix": ""
                    },
                    {
                        "first": "Manish",
                        "middle": [],
                        "last": "Raghavan",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jon Kleinberg, Sendhil Mullainathan, and Manish Raghavan. 2017. Inherent trade-offs in the fair determination of risk scores. Proceedings of ITCS.",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "An exploration of deception as a communication construct",
                "authors": [
                    {
                        "first": "Roderick",
                        "middle": [
                            "P"
                        ],
                        "last": "Mark L Knapp",
                        "suffix": ""
                    },
                    {
                        "first": "Harry",
                        "middle": [
                            "S"
                        ],
                        "last": "Hart",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dennis",
                        "suffix": ""
                    }
                ],
                "year": 1974,
                "venue": "Human communication research",
                "volume": "1",
                "issue": "1",
                "pages": "15--29",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark L Knapp, Roderick P Hart, and Harry S Dennis. 1974. An exploration of deception as a communication construct. Human communication research 1, 1 (1974), 15-29.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Improving Human-Machine Cooperative Visual Search With Soft Highlighting",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Ronald",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [
                            "C"
                        ],
                        "last": "Kneusel",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mozer",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "ACM Transactions on Applied Perception (TAP)",
                "volume": "15",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ronald T Kneusel and Michael C Mozer. 2017. Improving Human-Machine Cooperative Visual Search With Soft Highlighting. ACM Transactions on Applied Perception (TAP) 15, 1 (2017), 3.",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Modalities and cues in the detection of deception",
                "authors": [
                    {
                        "first": "Valerie",
                        "middle": [],
                        "last": "Robert M Krauss",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Geller",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Olson",
                        "suffix": ""
                    }
                ],
                "year": 1976,
                "venue": "Meeting of the American Psychological Association",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Robert M Krauss, Valerie Geller, and Christopher Olson. 1976. Modalities and cues in the detection of deception. In Meeting of the American Psychological Association, Washington, DC.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Telling it like it isn't: A review of theory and research on deceptive communications",
                "authors": [
                    {
                        "first": "L",
                        "middle": [],
                        "last": "Mark",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [
                            "E"
                        ],
                        "last": "Knapp",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Comaden",
                        "suffix": ""
                    }
                ],
                "year": 1979,
                "venue": "Human Communication Research",
                "volume": "5",
                "issue": "",
                "pages": "270--285",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mark L Knapp and Mark E Comaden. 1979. Telling it like it isn't: A review of theory and research on deceptive communications. Human Communication Research 5, 3 (1979), 270-285.",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "The science of fake news",
                "authors": [
                    {
                        "first": "Matthew",
                        "middle": [
                            "A"
                        ],
                        "last": "David Mj Lazer",
                        "suffix": ""
                    },
                    {
                        "first": "Yochai",
                        "middle": [],
                        "last": "Baum",
                        "suffix": ""
                    },
                    {
                        "first": "Adam",
                        "middle": [
                            "J"
                        ],
                        "last": "Benkler",
                        "suffix": ""
                    },
                    {
                        "first": "Kelly",
                        "middle": [
                            "M"
                        ],
                        "last": "Berinsky",
                        "suffix": ""
                    },
                    {
                        "first": "Filippo",
                        "middle": [],
                        "last": "Greenhill",
                        "suffix": ""
                    },
                    {
                        "first": "Miriam",
                        "middle": [
                            "J"
                        ],
                        "last": "Menczer",
                        "suffix": ""
                    },
                    {
                        "first": "Brendan",
                        "middle": [],
                        "last": "Metzger",
                        "suffix": ""
                    },
                    {
                        "first": "Gordon",
                        "middle": [],
                        "last": "Nyhan",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Pennycook",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Rothschild",
                        "suffix": ""
                    },
                    {
                        "first": "Steven",
                        "middle": [
                            "A"
                        ],
                        "last": "Schudson",
                        "suffix": ""
                    },
                    {
                        "first": "Cass",
                        "middle": [
                            "R"
                        ],
                        "last": "Sloman",
                        "suffix": ""
                    },
                    {
                        "first": "Emily",
                        "middle": [
                            "A"
                        ],
                        "last": "Sunstein",
                        "suffix": ""
                    },
                    {
                        "first": "Duncan",
                        "middle": [
                            "J"
                        ],
                        "last": "Thorson",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [
                            "L"
                        ],
                        "last": "Watts",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zittrain",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Science",
                "volume": "359",
                "issue": "",
                "pages": "1094--1096",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "David MJ Lazer, Matthew A Baum, Yochai Benkler, Adam J Berinsky, Kelly M Greenhill, Filippo Menczer, Miriam J Metzger, Brendan Nyhan, Gordon Penny- cook, David Rothschild, Michael Schudson, Steven A. Sloman, Cass R. Sunstein, Emily A. Thorson, Duncan J. Watts, and Jonathan L. Zittrain. 2018. The science of fake news. Science 359, 6380 (2018), 1094-1096.",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Trust in automation: Designing for appropriate reliance",
                "authors": [
                    {
                        "first": "D",
                        "middle": [],
                        "last": "John",
                        "suffix": ""
                    },
                    {
                        "first": "Katrina",
                        "middle": [
                            "A"
                        ],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "See",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Human factors",
                "volume": "46",
                "issue": "",
                "pages": "50--80",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John D Lee and Katrina A See. 2004. Trust in automation: Designing for appro- priate reliance. Human factors 46, 1 (2004), 50-80.",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Rationalizing neural predictions",
                "authors": [
                    {
                        "first": "Tao",
                        "middle": [],
                        "last": "Lei",
                        "suffix": ""
                    },
                    {
                        "first": "Regina",
                        "middle": [],
                        "last": "Barzilay",
                        "suffix": ""
                    },
                    {
                        "first": "Tommi",
                        "middle": [],
                        "last": "Jaakkola",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of EMNLP",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tao Lei, Regina Barzilay, and Tommi Jaakkola. 2016. Rationalizing neural predic- tions. Proceedings of EMNLP.",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Sex differences in deception detection",
                "authors": [
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Li Li. 2011. Sex differences in deception detection.",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Sent to Prison by a Software Program's Secret Algorithms",
                "authors": [
                    {
                        "first": "Adam",
                        "middle": [],
                        "last": "Liptak",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adam Liptak. 2017. Sent to Prison by a Software Program's Secret Algorithms.",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "The structure and function of explanations",
                "authors": [
                    {
                        "first": "Tania",
                        "middle": [],
                        "last": "Lombrozo",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Trends in cognitive sciences",
                "volume": "10",
                "issue": "10",
                "pages": "464--470",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tania Lombrozo. 2006. The structure and function of explanations. Trends in cognitive sciences 10, 10 (2006), 464-470.",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Simplicity and probability in causal explanation",
                "authors": [
                    {
                        "first": "Tania",
                        "middle": [],
                        "last": "Lombrozo",
                        "suffix": ""
                    }
                ],
                "year": 2007,
                "venue": "Cognitive psychology",
                "volume": "55",
                "issue": "",
                "pages": "232--257",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tania Lombrozo. 2007. Simplicity and probability in causal explanation. Cognitive psychology 55, 3 (2007), 232-257.",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "A unified approach to interpreting model predictions",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Scott",
                        "suffix": ""
                    },
                    {
                        "first": "Su-In",
                        "middle": [],
                        "last": "Lundberg",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Proceedings of NIPS",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Scott M Lundberg and Su-In Lee. 2017. A unified approach to interpreting model predictions. In Proceedings of NIPS.",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "Teaching categories to human learners with visual explanations",
                "authors": [
                    {
                        "first": "Oisin",
                        "middle": [],
                        "last": "Mac Aodha",
                        "suffix": ""
                    },
                    {
                        "first": "Shihan",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxin",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Pietro",
                        "middle": [],
                        "last": "Perona",
                        "suffix": ""
                    },
                    {
                        "first": "Yisong",
                        "middle": [],
                        "last": "Yue",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of CVPR",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Oisin Mac Aodha, Shihan Su, Yuxin Chen, Pietro Perona, and Yisong Yue. 2018. Teaching categories to human learners with visual explanations. In Proceedings of CVPR.",
                "links": null
            },
            "BIBREF51": {
                "ref_id": "b51",
                "title": "Detecting true lies: police officers' ability to detect suspects' lies",
                "authors": [
                    {
                        "first": "Samantha",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Aldert",
                        "middle": [],
                        "last": "Vrij",
                        "suffix": ""
                    },
                    {
                        "first": "Ray",
                        "middle": [],
                        "last": "Bull",
                        "suffix": ""
                    }
                ],
                "year": 2004,
                "venue": "Journal of applied psychology",
                "volume": "89",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Samantha Mann, Aldert Vrij, and Ray Bull. 2004. Detecting true lies: police officers' ability to detect suspects' lies. Journal of applied psychology 89, 1 (2004), 137.",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "What women know that men don't: Sex differences in determining the truth behind deceptive messages",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Steven",
                        "suffix": ""
                    },
                    {
                        "first": "Malcolm",
                        "middle": [
                            "R"
                        ],
                        "last": "Mccornack",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Parks",
                        "suffix": ""
                    }
                ],
                "year": 1990,
                "venue": "Journal of Social and Personal Relationships",
                "volume": "7",
                "issue": "",
                "pages": "107--118",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steven A McCornack and Malcolm R Parks. 1990. What women know that men don't: Sex differences in determining the truth behind deceptive messages. Journal of Social and Personal Relationships 7, 1 (1990), 107-118.",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Human problem solving",
                "authors": [
                    {
                        "first": "Allen",
                        "middle": [],
                        "last": "Newell",
                        "suffix": ""
                    },
                    {
                        "first": "Herbert",
                        "middle": [],
                        "last": "Alexander",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    }
                ],
                "year": 1972,
                "venue": "",
                "volume": "104",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Allen Newell and Herbert Alexander Simon. 1972. Human problem solving. Vol. 104. Prentice-Hall Englewood Cliffs, NJ.",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Detecting spam web pages through content analysis",
                "authors": [
                    {
                        "first": "Alexandros",
                        "middle": [],
                        "last": "Ntoulas",
                        "suffix": ""
                    },
                    {
                        "first": "Marc",
                        "middle": [],
                        "last": "Najork",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Manasse",
                        "suffix": ""
                    },
                    {
                        "first": "Dennis",
                        "middle": [],
                        "last": "Fetterly",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of WWW",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alexandros Ntoulas, Marc Najork, Mark Manasse, and Dennis Fetterly. 2006. Detecting spam web pages through content analysis. In Proceedings of WWW.",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "When corrections fail: The persistence of political misperceptions",
                "authors": [
                    {
                        "first": "Brendan",
                        "middle": [],
                        "last": "Nyhan",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Reifler",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Political Behavior",
                "volume": "32",
                "issue": "",
                "pages": "303--330",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Brendan Nyhan and Jason Reifler. 2010. When corrections fail: The persistence of political misperceptions. Political Behavior 32, 2 (2010), 303-330.",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Estimating the prevalence of deception in online review communities",
                "authors": [
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Hancock",
                        "suffix": ""
                    }
                ],
                "year": 2012,
                "venue": "Proceedings of WWW",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Myle Ott, Claire Cardie, and Jeff Hancock. 2012. Estimating the prevalence of deception in online review communities. In Proceedings of WWW.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Negative deceptive opinion spam",
                "authors": [
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "T"
                        ],
                        "last": "Hancock",
                        "suffix": ""
                    }
                ],
                "year": 2013,
                "venue": "Proceedings of NAACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Myle Ott, Claire Cardie, and Jeffrey T Hancock. 2013. Negative deceptive opinion spam. In Proceedings of NAACL.",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Finding deceptive opinion spam by any stretch of the imagination",
                "authors": [
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "Claire",
                        "middle": [],
                        "last": "Cardie",
                        "suffix": ""
                    },
                    {
                        "first": "Jeffrey",
                        "middle": [
                            "T"
                        ],
                        "last": "Hancock",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Proceedings of ACL",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T Hancock. 2011. Finding decep- tive opinion spam by any stretch of the imagination. In Proceedings of ACL.",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Humans and automation: Use, misuse, disuse, abuse",
                "authors": [
                    {
                        "first": "Raja",
                        "middle": [],
                        "last": "Parasuraman",
                        "suffix": ""
                    },
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Riley",
                        "suffix": ""
                    }
                ],
                "year": 1997,
                "venue": "Human factors",
                "volume": "39",
                "issue": "",
                "pages": "230--253",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Raja Parasuraman and Victor Riley. 1997. Humans and automation: Use, misuse, disuse, abuse. Human factors 39, 2 (1997), 230-253.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Numeracy and decision making",
                "authors": [
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "V\u00e4stfj\u00e4ll",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Slovic",
                        "suffix": ""
                    },
                    {
                        "first": "Ketti",
                        "middle": [],
                        "last": "Mertz",
                        "suffix": ""
                    },
                    {
                        "first": "Stephan",
                        "middle": [],
                        "last": "Mazzocco",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Dickert",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Psychological science",
                "volume": "17",
                "issue": "",
                "pages": "407--413",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ellen Peters, Daniel V\u00e4stfj\u00e4ll, Paul Slovic, CK Mertz, Ketti Mazzocco, and Stephan Dickert. 2006. Numeracy and decision making. Psychological science 17, 5 (2006), 407-413.",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "Numeracy, ratio bias, and denominator neglect in judgments of risk and probability",
                "authors": [
                    {
                        "first": "F",
                        "middle": [],
                        "last": "Valerie",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [
                            "J"
                        ],
                        "last": "Reyna",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Brainerd",
                        "suffix": ""
                    }
                ],
                "year": 2008,
                "venue": "Learning and individual differences",
                "volume": "18",
                "issue": "",
                "pages": "89--107",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Valerie F Reyna and Charles J Brainerd. 2008. Numeracy, ratio bias, and denom- inator neglect in judgments of risk and probability. Learning and individual differences 18, 1 (2008), 89-107.",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Why should i trust you?: Explaining the predictions of any classifier",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Tulio Ribeiro",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Carlos",
                        "middle": [],
                        "last": "Guestrin",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Proceedings of KDD",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of KDD.",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Anchors: High-Precision Model-Agnostic Explanations",
                "authors": [
                    {
                        "first": "Marco",
                        "middle": [],
                        "last": "Tulio Ribeiro",
                        "suffix": ""
                    },
                    {
                        "first": "Sameer",
                        "middle": [],
                        "last": "Singh",
                        "suffix": ""
                    },
                    {
                        "first": "Carlos",
                        "middle": [],
                        "last": "Guestrin",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Anchors: High- Precision Model-Agnostic Explanations. In Proceedings of AAAI.",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "Teaching Bayesian reasoning in less than two hours",
                "authors": [
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Sedlmeier",
                        "suffix": ""
                    },
                    {
                        "first": "Gerd",
                        "middle": [],
                        "last": "Gigerenzer",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Journal of Experimental Psychology: General",
                "volume": "130",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter Sedlmeier and Gerd Gigerenzer. 2001. Teaching Bayesian reasoning in less than two hours. Journal of Experimental Psychology: General 130, 3 (2001), 380.",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "Past, present, and future of decision support technology",
                "authors": [
                    {
                        "first": "Merrill",
                        "middle": [],
                        "last": "Jung P Shim",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [
                            "F"
                        ],
                        "last": "Warkentin",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [
                            "J"
                        ],
                        "last": "Courtney",
                        "suffix": ""
                    },
                    {
                        "first": "Ramesh",
                        "middle": [],
                        "last": "Power",
                        "suffix": ""
                    },
                    {
                        "first": "Christer",
                        "middle": [],
                        "last": "Sharda",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Carlsson",
                        "suffix": ""
                    }
                ],
                "year": 2002,
                "venue": "Decision support systems",
                "volume": "33",
                "issue": "",
                "pages": "111--126",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jung P Shim, Merrill Warkentin, James F Courtney, Daniel J Power, Ramesh Sharda, and Christer Carlsson. 2002. Past, present, and future of decision support technology. Decision support systems 33, 2 (2002), 111-126.",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "Near-Optimally Teaching the Crowd to Classify",
                "authors": [
                    {
                        "first": "Adish",
                        "middle": [],
                        "last": "Singla",
                        "suffix": ""
                    },
                    {
                        "first": "Ilija",
                        "middle": [],
                        "last": "Bogunovic",
                        "suffix": ""
                    },
                    {
                        "first": "G\u00e1bor",
                        "middle": [],
                        "last": "Bart\u00f3k",
                        "suffix": ""
                    },
                    {
                        "first": "Amin",
                        "middle": [],
                        "last": "Karbasi",
                        "suffix": ""
                    },
                    {
                        "first": "Andreas",
                        "middle": [],
                        "last": "Krause",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Proceedings of ICML",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adish Singla, Ilija Bogunovic, G\u00e1bor Bart\u00f3k, Amin Karbasi, and Andreas Krause. 2014. Near-Optimally Teaching the Crowd to Classify.. In Proceedings of ICML.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "Risk perception and affect",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Slovic",
                        "suffix": ""
                    },
                    {
                        "first": "Ellen",
                        "middle": [],
                        "last": "Peters",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Current directions in psychological science",
                "volume": "15",
                "issue": "6",
                "pages": "322--325",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul Slovic and Ellen Peters. 2006. Risk perception and affect. Current directions in psychological science 15, 6 (2006), 322-325.",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "Supreme Court of the United States. 1993. Daubert v. Merrell Dow Pharmaceuticals",
                "authors": [],
                "year": null,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Supreme Court of the United States. 1993. Daubert v. Merrell Dow Pharmaceuti- cals, Inc. 509 U.S. 579.",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "State of Wisconsin, Plaintiff-Respondent",
                "authors": [],
                "year": 2016,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Supreme Court of Wisconsin. 2016. State of Wisconsin, Plaintiff-Respondent, v. Eric L. Loomis, Defendant-Appellant.",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "Effects of word-ofmouth versus traditional marketing: findings from an internet social networking site",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Trusov",
                        "suffix": ""
                    },
                    {
                        "first": "Randolph",
                        "middle": [
                            "E"
                        ],
                        "last": "Bucklin",
                        "suffix": ""
                    },
                    {
                        "first": "Koen",
                        "middle": [],
                        "last": "Pauwels",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "Journal of marketing",
                "volume": "73",
                "issue": "",
                "pages": "90--102",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Trusov, Randolph E Bucklin, and Koen Pauwels. 2009. Effects of word-of- mouth versus traditional marketing: findings from an internet social networking site. Journal of marketing 73, 5 (2009), 90-102.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Engineering safety in machine learning",
                "authors": [
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Kush",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Varshney",
                        "suffix": ""
                    }
                ],
                "year": 2016,
                "venue": "Information Theory and Applications Workshop (ITA)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kush R Varshney. 2016. Engineering safety in machine learning. In Information Theory and Applications Workshop (ITA), 2016.",
                "links": null
            },
            "BIBREF73": {
                "ref_id": "b73",
                "title": "The spread of true and false news online",
                "authors": [
                    {
                        "first": "Soroush",
                        "middle": [],
                        "last": "Vosoughi",
                        "suffix": ""
                    },
                    {
                        "first": "Deb",
                        "middle": [],
                        "last": "Roy",
                        "suffix": ""
                    },
                    {
                        "first": "Sinan",
                        "middle": [],
                        "last": "Aral",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Science",
                "volume": "359",
                "issue": "",
                "pages": "1146--1151",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Soroush Vosoughi, Deb Roy, and Sinan Aral. 2018. The spread of true and false news online. Science 359, 6380 (2018), 1146-1151.",
                "links": null
            },
            "BIBREF74": {
                "ref_id": "b74",
                "title": "Detecting lies and deceit: The psychology of lying and implications for professional practice",
                "authors": [
                    {
                        "first": "Aldert",
                        "middle": [],
                        "last": "Vrij",
                        "suffix": ""
                    }
                ],
                "year": 2000,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aldert Vrij. 2000. Detecting lies and deceit: The psychology of lying and implications for professional practice. Wiley.",
                "links": null
            },
            "BIBREF75": {
                "ref_id": "b75",
                "title": "Engineering psychology & human performance",
                "authors": [
                    {
                        "first": "Justin",
                        "middle": [
                            "G"
                        ],
                        "last": "Christopher D Wickens",
                        "suffix": ""
                    },
                    {
                        "first": "Simon",
                        "middle": [],
                        "last": "Hollands",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Banbury",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christopher D Wickens, Justin G Hollands, Simon Banbury, and Raja Parasura- man. 2015. Engineering psychology & human performance. Psychology Press.",
                "links": null
            },
            "BIBREF76": {
                "ref_id": "b76",
                "title": "Distortion as a validation criterion in the identification of suspicious reviews",
                "authors": [
                    {
                        "first": "Guangyu",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Derek",
                        "middle": [],
                        "last": "Greene",
                        "suffix": ""
                    },
                    {
                        "first": "Barry",
                        "middle": [],
                        "last": "Smyth",
                        "suffix": ""
                    },
                    {
                        "first": "P\u00e1draig",
                        "middle": [],
                        "last": "Cunningham",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "Proceedings of the First Workshop on Social Media Analytics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Guangyu Wu, Derek Greene, Barry Smyth, and P\u00e1draig Cunningham. 2010. Distortion as a validation criterion in the identification of suspicious reviews. In Proceedings of the First Workshop on Social Media Analytics.",
                "links": null
            },
            "BIBREF77": {
                "ref_id": "b77",
                "title": "The influence of user-generated content on traveler behavior: An empirical investigation on the effects of e-wordof-mouth to hotel online bookings",
                "authors": [
                    {
                        "first": "Qiang",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Law",
                        "suffix": ""
                    },
                    {
                        "first": "Bin",
                        "middle": [],
                        "last": "Gu",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2011,
                "venue": "Computers in Human behavior",
                "volume": "27",
                "issue": "",
                "pages": "634--639",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Qiang Ye, Rob Law, Bin Gu, and Wei Chen. 2011. The influence of user-generated content on traveler behavior: An empirical investigation on the effects of e-word- of-mouth to hotel online bookings. Computers in Human behavior 27, 2 (2011), 634-639.",
                "links": null
            },
            "BIBREF78": {
                "ref_id": "b78",
                "title": "Comparison of deceptive and truthful travel reviews. Information and communication technologies in tourism",
                "authors": [
                    {
                        "first": "Kyung-Hyan",
                        "middle": [],
                        "last": "Yoo",
                        "suffix": ""
                    },
                    {
                        "first": "Ulrike",
                        "middle": [],
                        "last": "Gretzel",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "37--47",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kyung-Hyan Yoo and Ulrike Gretzel. 2009. Comparison of deceptive and truthful travel reviews. Information and communication technologies in tourism 2009 (2009), 37-47.",
                "links": null
            },
            "BIBREF79": {
                "ref_id": "b79",
                "title": "A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles",
                "authors": [
                    {
                        "first": "Amy",
                        "middle": [
                            "X"
                        ],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Aditya",
                        "middle": [],
                        "last": "Ranganathan",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [
                            "Emlen"
                        ],
                        "last": "Metz",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Appling",
                        "suffix": ""
                    },
                    {
                        "first": "Connie",
                        "middle": [
                            "Moon"
                        ],
                        "last": "Sehat",
                        "suffix": ""
                    },
                    {
                        "first": "Norman",
                        "middle": [],
                        "last": "Gilmore",
                        "suffix": ""
                    },
                    {
                        "first": "Nick",
                        "middle": [
                            "B"
                        ],
                        "last": "Adams",
                        "suffix": ""
                    },
                    {
                        "first": "Emmanuel",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Robbins",
                        "suffix": ""
                    },
                    {
                        "first": "Ed",
                        "middle": [],
                        "last": "Bice",
                        "suffix": ""
                    },
                    {
                        "first": "Sandro",
                        "middle": [],
                        "last": "Hawke",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Karger",
                        "suffix": ""
                    },
                    {
                        "first": "An",
                        "middle": [
                            "Xiao"
                        ],
                        "last": "Mina",
                        "suffix": ""
                    }
                ],
                "year": 2018,
                "venue": "Proceedings of WWW (Companion)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Amy X Zhang, Aditya Ranganathan, Sarah Emlen Metz, Scott Appling, Con- nie Moon Sehat, Norman Gilmore, Nick B Adams, Emmanuel Vincent, Martin Robbins, Ed Bice, Sandro Hawke, David Karger, and An Xiao Mina. 2018. A Structured Response to Misinformation: Defining and Annotating Credibility Indicators in News Articles. In Proceedings of WWW (Companion).",
                "links": null
            },
            "BIBREF80": {
                "ref_id": "b80",
                "title": "The impact of e-wordof-mouth on the online popularity of restaurants: A comparison of consumer reviews and editor reviews",
                "authors": [
                    {
                        "first": "Ziqiong",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Qiang",
                        "middle": [],
                        "last": "Ye",
                        "suffix": ""
                    },
                    {
                        "first": "Rob",
                        "middle": [],
                        "last": "Law",
                        "suffix": ""
                    },
                    {
                        "first": "Yijun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    }
                ],
                "year": 2010,
                "venue": "International Journal of Hospitality Management",
                "volume": "29",
                "issue": "",
                "pages": "694--700",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ziqiong Zhang, Qiang Ye, Rob Law, and Yijun Li. 2010. The impact of e-word- of-mouth on the online popularity of restaurants: A comparison of consumer reviews and editor reviews. International Journal of Hospitality Management 29, 4 (2010), 694-700.",
                "links": null
            },
            "BIBREF81": {
                "ref_id": "b81",
                "title": "Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education",
                "authors": [
                    {
                        "first": "Xiaojin",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of AAAI",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiaojin Zhu. 2015. Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education. In Proceedings of AAAI.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "text": "(a) Heatmap (without showing predicted labels), an instance of feature-based explanations. (b) Predicted label with accuracy. (c) Predicted label + heatmap (without accuracy).",
                "type_str": "figure",
                "uris": null,
                "fig_num": null,
                "num": null
            },
            "FIGREF1": {
                "text": "Figure2: Example interfaces with varying levels of machine assistance. Figure2aonly presents feature-based explanations of machine predictions in the form of heatmap. Figure2bshows both the predicted label and an explicit statement about machine accuracy (87%). Figure2cshows the predicted label with heatmap, but does not present machine accuracy. We crop the \"Genuine\" and \"Deceptive\" buttons in Figure2band 2c to save space.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "2",
                "num": null
            },
            "FIGREF2": {
                "text": "Figure2: Example interfaces with varying levels of machine assistance. Figure2aonly presents feature-based explanations of machine predictions in the form of heatmap. Figure2bshows both the predicted label and an explicit statement about machine accuracy (87%). Figure2cshows the predicted label with heatmap, but does not present machine accuracy. We crop the \"Genuine\" and \"Deceptive\" buttons in Figure2band 2c to save space.",
                "type_str": "figure",
                "uris": null,
                "fig_num": null,
                "num": null
            },
            "FIGREF3": {
                "text": "Trust in correct and incorrect machine predictions.",
                "type_str": "figure",
                "uris": null,
                "fig_num": null,
                "num": null
            },
            "FIGREF4": {
                "text": "Gender and hint usefulness in predicted label + heatmap.",
                "type_str": "figure",
                "uris": null,
                "fig_num": null,
                "num": null
            },
            "FIGREF5": {
                "text": "Figure5: Heterogeneity findings among participants in our study. Figure5ashows performance estimation by participants in three different experimental setups. Figure5bpresents the performance of participants in predicted label + heatmap group by two variables, hint usefulness and gender.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "5",
                "num": null
            },
            "FIGREF8": {
                "text": "Figure 8: Average accuracy with respect to review ordering in 9 experimental setups.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "8",
                "num": null
            },
            "FIGREF9": {
                "text": "Figure 9: Human accuracy vs. usefulness of hints.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "9",
                "num": null
            },
            "FIGREF10": {
                "text": "Figure 10: Human accuracy vs. gender.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "10",
                "num": null
            },
            "FIGREF11": {
                "text": "Figure 11: Human accuracy vs. review sentiment.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "11",
                "num": null
            },
            "FIGREF13": {
                "text": "Figure 12: Human accuracy vs. education background.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "12",
                "num": null
            },
            "FIGREF14": {
                "text": "Figure 13: Human accuracy vs. age groups.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "13",
                "num": null
            },
            "FIGREF16": {
                "text": "Figure 14: Human accuracy vs. review writing experience.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "14",
                "num": null
            },
            "FIGREF17": {
                "text": "Figure 16: Survey questions for all the other groups.",
                "type_str": "figure",
                "uris": null,
                "fig_num": "16",
                "num": null
            },
            "FIGREF18": {
                "text": "Figure 19: Example interface for predicted label + examples.",
                "type_str": "figure",
                "uris": null,
                "fig_num": null,
                "num": null
            }
        }
    }
}