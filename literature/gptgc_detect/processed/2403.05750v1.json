{
    "paper_id": "2403",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-09-19T13:38:43.292174Z"
    },
    "title": "Decoding the AI Pen: Techniques and Challenges in Detecting AI-Generated Text *",
    "authors": [
        {
            "first": "Sara",
            "middle": [],
            "last": "Abdali",
            "suffix": "",
            "affiliation": {},
            "email": "saraabdali@microsoft.com"
        },
        {
            "first": "Richard",
            "middle": [],
            "last": "Anarfi",
            "suffix": "",
            "affiliation": {},
            "email": "ranarfi@microsoft.com"
        },
        {
            "first": "C",
            "middle": [
                "J"
            ],
            "last": "Barberan",
            "suffix": "",
            "affiliation": {},
            "email": "cjbarberan@microsoft.com"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.",
    "pdf_parse": {
        "paper_id": "2403",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "Large Language Models (LLMs) have revolutionized the field of Natural Language Generation (NLG) by demonstrating an impressive ability to generate human-like text. However, their widespread usage introduces challenges that necessitate thoughtful examination, ethical scrutiny, and responsible practices. In this study, we delve into these challenges, explore existing strategies for mitigating them, with a particular emphasis on identifying AI-generated text as the ultimate solution. Additionally, we assess the feasibility of detection from a theoretical perspective and propose novel research directions to address the current limitations in this domain.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Large Language Models (LLMs) constitute a transformative advancement in the field of natural language processing (NLP). Their applications traverse a wide spectrum of domains, including question answering [5, 68] , sentiment analysis [4, 22] and specially text generation [46] . As LLMs are trained on extensive textual corpora, they demonstrate a remarkable capacity to produce human-like text.",
                "cite_spans": [
                    {
                        "start": 205,
                        "end": 208,
                        "text": "[5,",
                        "ref_id": "BIBREF4"
                    },
                    {
                        "start": 209,
                        "end": 212,
                        "text": "68]",
                        "ref_id": "BIBREF67"
                    },
                    {
                        "start": 234,
                        "end": 237,
                        "text": "[4,",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 238,
                        "end": 241,
                        "text": "22]",
                        "ref_id": "BIBREF21"
                    },
                    {
                        "start": 272,
                        "end": 276,
                        "text": "[46]",
                        "ref_id": "BIBREF45"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "However, the ubiquity of LLMs brings forth a convergence of opportunities and challenges that necessitate prudent examination. Among these challenges, we encounter the potential for LLMs to produce biased, toxic, or harmful content [13, 27, 60, 61] . Additionally, there are concerns related to intellectual property rights infringement [41, 50] and the misuse of LLMs for malicious purposes, such as disseminating misleading information and propaganda [36, 54] . These multifaceted considerations highlight the need for judicious evaluation and ethical handling of LLMs in various contexts.",
                "cite_spans": [
                    {
                        "start": 232,
                        "end": 236,
                        "text": "[13,",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 237,
                        "end": 240,
                        "text": "27,",
                        "ref_id": "BIBREF26"
                    },
                    {
                        "start": 241,
                        "end": 244,
                        "text": "60,",
                        "ref_id": "BIBREF59"
                    },
                    {
                        "start": 245,
                        "end": 248,
                        "text": "61]",
                        "ref_id": "BIBREF60"
                    },
                    {
                        "start": 337,
                        "end": 341,
                        "text": "[41,",
                        "ref_id": "BIBREF40"
                    },
                    {
                        "start": 342,
                        "end": 345,
                        "text": "50]",
                        "ref_id": "BIBREF49"
                    },
                    {
                        "start": 453,
                        "end": 457,
                        "text": "[36,",
                        "ref_id": "BIBREF35"
                    },
                    {
                        "start": 458,
                        "end": 461,
                        "text": "54]",
                        "ref_id": "BIBREF53"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "Leveraging AI-generated text could be regarded as an efficacious strategy for alleviating the aforementioned challenges. However, the convergence of AI-generated content with human-written texts has reached a point where discerning between the two has become increasingly intricate. The task of distinguishing LLM-generated text from human-written content presents a dual challenge. On one hand, identifying disparities can enhance the quality of AI-generated material. On the other hand, this endeavor also complicates the identification process. In recent years, researchers have proposed various methodologies for detecting AI-generated text . These efforts contribute to the ongoing exploration of this landscape.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "This work aims to comprehensively investigate responsible AIgenerated text. To this end, first, we highlight the risks and misuses associated with such text, while also discussing common mitigation strategies. As a significant solution, we thoroughly explore AIgenerated text, concentrating on thematic categorization and assessing their constraints and vulnerabilities. Additionally, we conduct a theoretical exploration to assess the feasibility and potential of detecting AI-generated text. By adopting a theoretical lens, we seek to determine whether such detection is achievable or if detection remains an elusive goal within the domain of generative AI.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "The rest of this paper is organized as follows: In section 2, we discuss the potential risks arising from the misuse of LLMs. Section 3 provides a comprehensive categorization of various text detection strategies, emphasizing their role as mitigation techniques. Subsequently, in Section 4, we describe the vulnerabilities inherent in these strategies. Moving forward, Section 5 engages in a theoretical exploration of the feasibility of detection, followed by section 6 where we propose new avenues of research. Finally, in section 7, we conclude. Figure 1 demonstrates an overview of the paper.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 556,
                        "end": 557,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "INTRODUCTION",
                "sec_num": "1"
            },
            {
                "text": "LLMs harbor the capacity to generate harmful content and enable malicious actions. These include spreading toxic, biased, or harmful language, misinformation propagation and even committing plagiarism. In the subsequent sections, we will explore a non-comprehensive list of potential risks associated with LLM misuse, while discussing proposed mitigation techniques.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "RISKS AND MISUSE OF AI-GENERATED TEXT",
                "sec_num": "2"
            },
            {
                "text": "LLMs, have the potential to produce text that exhibits discriminatory, offensive, or harmful characteristics towards individuals or groups. The manifestation of such undesirable language depends on factors such as the quality and diversity of the training data used, the design decisions made during model development, and the intended or unintended contexts in which the model is applied [12, 13, 15] . Thus, LLMs may pose ethical and social challenges that require careful evaluation and regulation. A recent work published by DeepMind [60] , structures the risk landscape associated with LLMs. It outlines six specific risk areas, including discrimination, exclusion and toxicity, and discusses the potential mitigation approaches and challenges. For discrimination, exclusion and toxicity, the paper suggests improving data quality and diversity, applying fairness metrics and interventions, and implementing content moderation and reporting mechanisms.",
                "cite_spans": [
                    {
                        "start": 389,
                        "end": 393,
                        "text": "[12,",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 394,
                        "end": 397,
                        "text": "13,",
                        "ref_id": "BIBREF12"
                    },
                    {
                        "start": 398,
                        "end": 401,
                        "text": "15]",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 538,
                        "end": 542,
                        "text": "[60]",
                        "ref_id": "BIBREF59"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrimination, Toxicity and Harms",
                "sec_num": "2.1"
            },
            {
                "text": "In another study by Deshpande et al., [13] it is shown that Chat-GPT, when assigned a persona, can exhibit significant toxicity. Particularly, this risk is elevated for vulnerable groups such as students, minors, and patients. This work emphasizes that toxicity is closely tied to the style of communication, with explicit negative prompts leading to increased toxicity.",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 42,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrimination, Toxicity and Harms",
                "sec_num": "2.1"
            },
            {
                "text": "Furthermore, the research reveal that specific genders and ethnicities are disproportionately susceptible to encountering toxic content. They posit that this phenomenon arises from an over-reliance on reinforcement learning with human feedback (RLHF) as a mechanism to mitigate model toxicity. However, the feedback provided to the model may carry inherent biases. For instance, feedback related to toxicity concerning different genders might be influenced by the representation of those genders.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrimination, Toxicity and Harms",
                "sec_num": "2.1"
            },
            {
                "text": "Kour et al. [25] recently introduced the AttaQ dataset, designed to provoke harmful or inappropriate responses from LLMs. They conduct evaluations on multiple LLMs using this dataset and observe that, in numerous instances, LLMs generate unsafe outputs.",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 16,
                        "text": "[25]",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrimination, Toxicity and Harms",
                "sec_num": "2.1"
            },
            {
                "text": "More importantly, LLMs have the potential to produce implicit toxic responses that elude existing classifiers. These harmful outputs, while not easily identifiable, can offend individuals or groups by insinuating negative or false statements. As a result, this undermines the safety and reliability of NLG systems and gives rise to ethical concerns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrimination, Toxicity and Harms",
                "sec_num": "2.1"
            },
            {
                "text": "A recent study by Wen et al. [61] delves into the phenomenon of LLMs producing implicit toxic content that remains elusive to conventional toxicity classifiers. The proposed approach leverages reinforcement learning to uncover and expose this implicit toxicity, emphasizing the need to fine-tune classifiers using annotated examples derived from the attack method for enhanced detection capabilities.",
                "cite_spans": [
                    {
                        "start": 29,
                        "end": 33,
                        "text": "[61]",
                        "ref_id": "BIBREF60"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrimination, Toxicity and Harms",
                "sec_num": "2.1"
            },
            {
                "text": "Given the multifaceted nature of content generation by LLMs, it is critical to explore the intricate interplay of user-driven, datadriven, and model-driven factors that contribute to the production of toxic and harmful content. Therefore, further research is warranted to comprehensively investigate the deleterious effects and toxicity associated with LLMs. Future research endeavor should focus on devising robust methodologies and mechanisms for prevention, detection, and mitigation. By doing so, we not only bolster the safety and reliability of LLMs but also make significant contributions to the advancement of related disciplines.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Discrimination, Toxicity and Harms",
                "sec_num": "2.1"
            },
            {
                "text": "Enforcing factual coherence during reasoning constitutes a key challenge for LLMs. These models often demonstrate tendencies such as overlooking conditions, misinterpreting context, and even hallucinating content in response to specific queries [28] . For example, an investigation into GPT-3 by Khatun et al. [21] revealed that while the model adeptly avoids blatant conspiracies and stereotypes, it hesitates when handling commonplace misunderstandings and debates. In particular, the model's responses exhibit variability across different questions and contextual scenarios, emphasizing the inherent unpredictability of GPT-3.",
                "cite_spans": [
                    {
                        "start": 245,
                        "end": 249,
                        "text": "[28]",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 310,
                        "end": 314,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Factual Inconsistency and Unreliability of AI Responses",
                "sec_num": "2.2"
            },
            {
                "text": "Zhou et al. [72] recently conducted a study revealing that LLMs, including ChatGPT and Claude, exhibit deficiencies in conveying uncertainties when responding to questions. Furthermore, these models occasionally display unwarranted overconfidence, even when their answers are incorrect. While LLMs can be coerced to express confidence, this process is fraught with high error rates.",
                "cite_spans": [
                    {
                        "start": 12,
                        "end": 16,
                        "text": "[72]",
                        "ref_id": "BIBREF71"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Factual Inconsistency and Unreliability of AI Responses",
                "sec_num": "2.2"
            },
            {
                "text": "Significantly, the study highlights that users encounter challenges in assessing the accuracy of LLM-generated responses. Their judgment is influenced by the tone and style of the LLMs, which may introduce bias. This issue is extremely importance, as biases against uncertain text may impact the training and evaluation of LLMs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Factual Inconsistency and Unreliability of AI Responses",
                "sec_num": "2.2"
            },
            {
                "text": "To mitigate such mistakes, various strategies have been proposed through fine-tuning [29, 43, 69] , prompt engineering techniques such as verification, scratchpads [11, 38] , Chain-of-Thought (CoT) [59] , Reversing Chain-of-Thought [63] (RoCT), RLHF [10, 73] , iterative self-reflection [34, 48] , self-consistency [56] , society of minds strategy [14] , pruning truthful datasets [9] , adjusting the system parameters to limit model creativity [37] , external knowledge retrieval [18] and training-free methods with likelihood estimation [19] .",
                "cite_spans": [
                    {
                        "start": 85,
                        "end": 89,
                        "text": "[29,",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 90,
                        "end": 93,
                        "text": "43,",
                        "ref_id": "BIBREF42"
                    },
                    {
                        "start": 94,
                        "end": 97,
                        "text": "69]",
                        "ref_id": "BIBREF68"
                    },
                    {
                        "start": 164,
                        "end": 168,
                        "text": "[11,",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 169,
                        "end": 172,
                        "text": "38]",
                        "ref_id": "BIBREF37"
                    },
                    {
                        "start": 198,
                        "end": 202,
                        "text": "[59]",
                        "ref_id": "BIBREF58"
                    },
                    {
                        "start": 232,
                        "end": 236,
                        "text": "[63]",
                        "ref_id": "BIBREF62"
                    },
                    {
                        "start": 250,
                        "end": 254,
                        "text": "[10,",
                        "ref_id": "BIBREF9"
                    },
                    {
                        "start": 255,
                        "end": 258,
                        "text": "73]",
                        "ref_id": "BIBREF72"
                    },
                    {
                        "start": 287,
                        "end": 291,
                        "text": "[34,",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 292,
                        "end": 295,
                        "text": "48]",
                        "ref_id": "BIBREF47"
                    },
                    {
                        "start": 315,
                        "end": 319,
                        "text": "[56]",
                        "ref_id": "BIBREF55"
                    },
                    {
                        "start": 348,
                        "end": 352,
                        "text": "[14]",
                        "ref_id": "BIBREF13"
                    },
                    {
                        "start": 381,
                        "end": 384,
                        "text": "[9]",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 445,
                        "end": 449,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    },
                    {
                        "start": 481,
                        "end": 485,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 539,
                        "end": 543,
                        "text": "[19]",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Factual Inconsistency and Unreliability of AI Responses",
                "sec_num": "2.2"
            },
            {
                "text": "Indeed, the existing body of research emphasize the unspeakable capabilities of LLMs, while simultaneously acknowledging their susceptibility to errors. Therefore, it is crucial to approach LLMgenerated outputs with caution.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Factual Inconsistency and Unreliability of AI Responses",
                "sec_num": "2.2"
            },
            {
                "text": "LLMs could pose a substantial risk to academic writing by elevating the likelihood of copyright violations and plagiarism. For example, writers might employ LLMs to produce articles without original composition, while students could resort to LLMs for completing their assignments. These practices erode academic integrity and subvert the objectives of assignments and examinations [20, 50] .",
                "cite_spans": [
                    {
                        "start": 382,
                        "end": 386,
                        "text": "[20,",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 387,
                        "end": 390,
                        "text": "50]",
                        "ref_id": "BIBREF49"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Copyright Infringement and Plagiarism",
                "sec_num": "2.3"
            },
            {
                "text": "In response to this challenge, researchers have devised several detectors aimed at discerning between text authored by humans and that generated by AI. These detectors fall into two main categories: black-box methods, exemplified by studies such as [32, 42, 57] , and white-box detection, as explored by [53] .",
                "cite_spans": [
                    {
                        "start": 249,
                        "end": 253,
                        "text": "[32,",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 254,
                        "end": 257,
                        "text": "42,",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 258,
                        "end": 261,
                        "text": "57]",
                        "ref_id": "BIBREF56"
                    },
                    {
                        "start": 304,
                        "end": 308,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Copyright Infringement and Plagiarism",
                "sec_num": "2.3"
            },
            {
                "text": "In the black-box detection approach, there is limited access to the LLM's output text. Specifically, we interact with the LLM via its API, which provides us with the generated text. Black-box detectors rely on collecting samples of both human-written and AIgenerated text. These samples are then used to train a classifier. The trained classifier discriminates between LLM-generated and human-written texts based on features extracted from the text samples [32, 42, 52] .",
                "cite_spans": [
                    {
                        "start": 457,
                        "end": 461,
                        "text": "[32,",
                        "ref_id": "BIBREF31"
                    },
                    {
                        "start": 462,
                        "end": 465,
                        "text": "42,",
                        "ref_id": "BIBREF41"
                    },
                    {
                        "start": 466,
                        "end": 469,
                        "text": "52]",
                        "ref_id": null
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Copyright Infringement and Plagiarism",
                "sec_num": "2.3"
            },
            {
                "text": "In contrast to the black-box approach, the white-box approach necessitates additional access to the probabilities associated with each token in the model [53] . Consequently, there are fewer whitebox detectors currently available. Thus, black-box methods that are independent of model access and can be readily adjusted to a new model seem to be more feasible and practical.",
                "cite_spans": [
                    {
                        "start": 154,
                        "end": 158,
                        "text": "[53]",
                        "ref_id": "BIBREF52"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Copyright Infringement and Plagiarism",
                "sec_num": "2.3"
            },
            {
                "text": "Another crucial aspect to consider is the generalizability of AIgenerated text detectors. These detectors should perform effectively on unseen data across diverse dimensions, including: models, languages and domains. Achieving robustness and reliability across these dimensions contributes to the overall effectiveness of machinegenerated text detectors in real-world scenarios.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Copyright Infringement and Plagiarism",
                "sec_num": "2.3"
            },
            {
                "text": "Their study involved using various generative models to create text articles and then distinguishing between AI-generated and humanwritten content. They applied traditional machine learning techniques and transformer-based models to analyze stylistic features. Interestingly, while these methods performed well within their specific domains, they struggled with out-of-domain detection tasks. Furthermore, their findings indicate that when text detectors are trained on content generated by one LLM and then tested on data produced by a different LLM, performance tends to decline and generalizability becomes an issue. However, given that this study examines only a limited number of detection methods, more comprehensive and systematic assessments are necessary to validate this aspect of LLM capabilities. In section 3, we will delve further into the topic of AI-generated text detection techniques.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Copyright Infringement and Plagiarism",
                "sec_num": "2.3"
            },
            {
                "text": "LLMs, especially when integrated into Open-Domain Question Answering (ODQA) systems, can inadvertently contribute to the creation and dissemination of misinformation [7, 39, 40 ]. An intuitive approach, as suggested by Pan et al. [40] to counteract the spread of misinformation in ODQA systems is to reduce its prevalence. In fact, the goal is to minimize the proportion of misinformation that these QA systems encounter. Achieving this involves retrieving more context paragraphs to provide a solid background for readers. However, research have shown that expanding the context size has minimal impact on mitigating the performance degradation that is caused by misinformation. Consequently, the straightforward strategy of diluting misinformation by increasing context size proves ineffective for defending against it [40] . A more straightforward approach involves prompting LLMs to issue warnings regarding misleading information. For instance, readers could be advised: \"Be cautious, as some of the text may be intentionally misleading\" .",
                "cite_spans": [
                    {
                        "start": 166,
                        "end": 169,
                        "text": "[7,",
                        "ref_id": "BIBREF6"
                    },
                    {
                        "start": 170,
                        "end": 173,
                        "text": "39,",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 174,
                        "end": 176,
                        "text": "40",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 230,
                        "end": 234,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 821,
                        "end": 825,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Misinformation Dissemination",
                "sec_num": "2.4"
            },
            {
                "text": "Furthermore, it is feasible to detect and filter out misinformation produced by LLMs using different characteristics, such as content, style, or propagation structure. Chen et al. [7] , for example, introduce four instruction-tuned strategies to enhance LLMs for misinformation detection. One of these strategies is Instruction Filtering, which aims to exclude LLM outputs that deviate from instructions or contain misleading content, Instruction Verification which verifying the outputs of the LLM against the instructions or external sources to check their validity and reliability and Instruction Combination which combines multiple instructions to generate more diverse and accurate outputs from the LLM.",
                "cite_spans": [
                    {
                        "start": 180,
                        "end": 183,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Misinformation Dissemination",
                "sec_num": "2.4"
            },
            {
                "text": "Another interesting approach introduced by Chen et al. [7] is the Reader Ensemble technique. This method leverages multiple LLMs or other models to cross-check the accuracy and coherence of information produced by a specific LLM. Chen and colleagues also suggest employing Vigilant Prompting, which provides precise prompts or instructions to LLMs. These instructions serve a dual purpose: preventing the generation of misinformation and disclosing the model's machine identity.",
                "cite_spans": [
                    {
                        "start": 55,
                        "end": 58,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Misinformation Dissemination",
                "sec_num": "2.4"
            },
            {
                "text": "As AI-generated texts increasingly blend seamlessly with humanwritten content, the demand for more effective methods to detect misleading information produced by AI grows. In the next section, we will dive deeper into detection methods.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Misinformation Dissemination",
                "sec_num": "2.4"
            },
            {
                "text": "In the previous section, we briefly explored a classification of detection techniques into two primary categories: black-box and whitebox techniques. As mentioned, in the black-box scenario, the access is limited to the output text produced by LLM given an arbitrary input. Conversely, in the white-box context, we gain additional access into the model's output probabilities for individual tokens. In this section, we discuss various detection techniques, considering both their strengths and vulnerabilities.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "AI-GENERATED TEXT DETECTION TECHNIQUES",
                "sec_num": "3"
            },
            {
                "text": "A frequently employed detection strategy is fine-tuning a language model on datasets comprising both AI-generated and human-written texts [2, 3, 30, 49, 70] . The majority of LLMs require substantial computational resources, rendering it exceedingly challenging to curate sufficiently large datasets that comprise a diverse spectrum of samples. As a result, this approach is not generally an optimal solution. Moreover, it is susceptible to adversarial attacks, including data poisoning [8, 45, 64] . For example, malicious actors have the potential to elude detection by leveraging their access to human.written texts present in the training set and detector rankings. Moreover, within a whitebox context, attackers can undermine detector training-a concerning scenario, especially considering that numerous detectors rely on commonly used datasets, making them susceptible even to basic attacks. Additionally, these techniques are prone to the paraphrasing attack, where a paraphrased layer is added to the generative text model, allowing deception of any detector, including those utilizing supervised neural networks [26, 44] .",
                "cite_spans": [
                    {
                        "start": 138,
                        "end": 141,
                        "text": "[2,",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 142,
                        "end": 144,
                        "text": "3,",
                        "ref_id": "BIBREF2"
                    },
                    {
                        "start": 145,
                        "end": 148,
                        "text": "30,",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 149,
                        "end": 152,
                        "text": "49,",
                        "ref_id": "BIBREF48"
                    },
                    {
                        "start": 153,
                        "end": 156,
                        "text": "70]",
                        "ref_id": "BIBREF69"
                    },
                    {
                        "start": 487,
                        "end": 490,
                        "text": "[8,",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 491,
                        "end": 494,
                        "text": "45,",
                        "ref_id": "BIBREF44"
                    },
                    {
                        "start": 495,
                        "end": 498,
                        "text": "64]",
                        "ref_id": "BIBREF63"
                    },
                    {
                        "start": 1121,
                        "end": 1125,
                        "text": "[26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 1126,
                        "end": 1129,
                        "text": "44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Supervised Detection",
                "sec_num": "3.1"
            },
            {
                "text": "Another line of research employ pre-trained models as zero-shot classifiers to discern text written by AI, eliminating the necessity for supplementary training or data collection [1, 16, 51, 55] . According to [35] , commonly employed techniques often set a threshold for the predicted per-token log probability to identify AI-generated texts. This approach is grounded in the observation that passages generated by AI often exhibit a negative log probability curvature. While this method mitigates the risk of data poisoning and minimizes data and resource requirements, it remains vulnerable to other attacks such as spoofing [47] and paraphrasing [26, 44] .",
                "cite_spans": [
                    {
                        "start": 179,
                        "end": 182,
                        "text": "[1,",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 183,
                        "end": 186,
                        "text": "16,",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 187,
                        "end": 190,
                        "text": "51,",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 191,
                        "end": 194,
                        "text": "55]",
                        "ref_id": "BIBREF54"
                    },
                    {
                        "start": 210,
                        "end": 214,
                        "text": "[35]",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 628,
                        "end": 632,
                        "text": "[47]",
                        "ref_id": "BIBREF46"
                    },
                    {
                        "start": 650,
                        "end": 654,
                        "text": "[26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 655,
                        "end": 658,
                        "text": "44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Zero-shot Detection",
                "sec_num": "3.2"
            },
            {
                "text": "An alternative avenue of research utilize information retrieval methods to differentiate between texts written by humans and those generated by machines. This is achieved by comparing a given text with a database of texts created by LLMs and identifying semantically similar matches [26, 44] .",
                "cite_spans": [
                    {
                        "start": 283,
                        "end": 287,
                        "text": "[26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 288,
                        "end": 291,
                        "text": "44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval-based Detection",
                "sec_num": "3.3"
            },
            {
                "text": "Nevertheless, these approaches are impractical for real-world use due to their reliance on an extensive and up-to-date database of AIgenerated texts. Such databases can be computationally costly or may not be available across all domains, tasks, or models. Additionally, like other techniques, these methods are susceptible to paraphrasing and spoofing attacks [26, 31, 44, 62] .",
                "cite_spans": [
                    {
                        "start": 361,
                        "end": 365,
                        "text": "[26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 366,
                        "end": 369,
                        "text": "31,",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 370,
                        "end": 373,
                        "text": "44,",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 374,
                        "end": 377,
                        "text": "62]",
                        "ref_id": "BIBREF61"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Retrieval-based Detection",
                "sec_num": "3.3"
            },
            {
                "text": "Another alternative avenue, referred to as watermarking techniques, employs a model signature within the generated text outputs to imprint specific patterns. For instance, Kirchenbauer et al. [23] propose a soft watermarking approach that categorizes tokens into green and red lists, facilitating the construction of these patterns. In this approach, a watermarked LLM selects a token, with a high likelihood, from the green list based on its preceding token. Remarkably, these watermarks often remain imperceptible to human observers.",
                "cite_spans": [
                    {
                        "start": 192,
                        "end": 196,
                        "text": "[23]",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "To better understand the technique proposed by Kirchenbauer et al., assume an autoregressive language model is trained on a vocabulary of size | |. Given a sequence of tokens as input at step , a language model predicts the next token in the sequence by outputting a vector of logit scores \u2208 | | with one entry for each item in the vocabulary. A random number generator is seeded with a context window of \u210e preceding tokens, based on a pseudo-random function (PRF) : \u210e \u2192 . With this random seed, a subset of tokens of size | |, where (0, 1) is green list size, are \"colored\" green and denoted . Now, the logit scores are modified such that witha hardness parameter > 0:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "= + , if \u2208 , otherwise",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "In a straightforward scenario the process involves passing scores through a softmax layer and then sampling from the resulting output distribution. This tends to introduce a bias toward tokens from the set . Once watermarked text is generated, it is possible to verify the watermark even without direct access to the LLM. This verification is achieved by recomputing the at each position and identifying the set of token positions associated with the . For tokens, the statistical significance of the watermark is assessed by z-score:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "= (| | -) (1 -)",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "When this z-score is large (and the corresponding P-value is small), one can be confident that the text is watermarked [23] .",
                "cite_spans": [
                    {
                        "start": 119,
                        "end": 123,
                        "text": "[23]",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "Despite previous indications, watermark-based techniques remain both theoretically and practically vulnerable to rewording attacks. Notably, even LLMs safeguarded by watermarking schemes are susceptible to spoofing attacks. In these attacks, human adversaries inject their own text into human-generated content, creating an illusion that the material originated from LLMs [44] . The interested reader is referred to [44] for more details.",
                "cite_spans": [
                    {
                        "start": 372,
                        "end": 376,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 416,
                        "end": 420,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "Moreover, unless all highly successful LLMs are uniformly safeguarded, watermarking remains an ineffective strategy for preventing LLM exploitation. Thus, the practical applicability of watermarking is restricted, especially in scenarios where only black-box LLMs are accessible. Due to API providers withholding probability distributions for commercial reasons, most third-party developers creating API-based applications cannot independently watermark text.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "To tackle this challenge, Yang et al. [65] have developed a watermarking framework to empower third parties with the ability to autonomously inject watermarks into black-box language model scenarios. In this approach, a binary encoding function is leveraged that generates random binary encodings corresponding to individual words. These encodings adhere to a Bernoulli distribution, where the probability of a word representing bit-1 is approximately 0.5. To embed a watermark, this distribution is modified by selectively replacing words associated with bit-0 using contextually relevant synonyms that represent bit-1. Subsequently, a statistical test is employed to detect the watermark.",
                "cite_spans": [
                    {
                        "start": 38,
                        "end": 42,
                        "text": "[65]",
                        "ref_id": "BIBREF64"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "Another study conducted by Kirchenbauer et al. [24] explores the effectiveness of watermarks in identifying AI-generated text. They assess how watermarked text withstands challenges such as human restructuring, non-watermarked LLM paraphrasing, and seamless integration into longer handwritten documents.",
                "cite_spans": [
                    {
                        "start": 47,
                        "end": 51,
                        "text": "[24]",
                        "ref_id": "BIBREF23"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "They have observed that even after undergoing automated and human paraphrasing, watermarks can still be discerned. When a sufficient number of tokens are identified, paraphrased versions tend to inadvertently leak n-grams or even larger segments of the original text. Hence, high-confidence detection remains possible despite these attacks weakening the effectiveness of the watermark. The study proposes interpreting watermarking reliability as a function of text length. Surprisingly, even when attempting to remove the Training on commonly used datasets, makes it vulnerable to most attacks including paraphrasing. [1, 16, 17, 35, 51, 55] Zero-shot detection To use a pre-trained language model in zero-shot settings.",
                "cite_spans": [
                    {
                        "start": 618,
                        "end": 621,
                        "text": "[1,",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 622,
                        "end": 625,
                        "text": "16,",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 626,
                        "end": 629,
                        "text": "17,",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 630,
                        "end": 633,
                        "text": "35,",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 634,
                        "end": 637,
                        "text": "51,",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 638,
                        "end": 641,
                        "text": "55]",
                        "ref_id": "BIBREF54"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "Reduces the risk of data poisoning attacks and eliminates data and resource over-heads, but it is still susceptible to other adversarial attacks like spoofing and paraphrasing. [26, 31, 44, 62] Retrieval-based detection Apply methods of information retrieval to match a given text with a collection of texts generated by LLMs and finding similarities in meaning.",
                "cite_spans": [
                    {
                        "start": 177,
                        "end": 181,
                        "text": "[26,",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 182,
                        "end": 185,
                        "text": "31,",
                        "ref_id": "BIBREF30"
                    },
                    {
                        "start": 186,
                        "end": 189,
                        "text": "44,",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 190,
                        "end": 193,
                        "text": "62]",
                        "ref_id": "BIBREF61"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "It is impractical because it requires a large and updated collection of texts, which is computationally expensive, or may be unavailable for all domains, tasks or models. It is also vulnerable to paraphrasing and spoofing attacks. [23, 24, 44, 65] Watermarking To use a model signature in the produced text outputs to stamp particular pattern.",
                "cite_spans": [
                    {
                        "start": 231,
                        "end": 235,
                        "text": "[23,",
                        "ref_id": "BIBREF22"
                    },
                    {
                        "start": 236,
                        "end": 239,
                        "text": "24,",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 240,
                        "end": 243,
                        "text": "44,",
                        "ref_id": "BIBREF43"
                    },
                    {
                        "start": 244,
                        "end": 247,
                        "text": "65]",
                        "ref_id": "BIBREF64"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "The most trustworthy strategy, but is shown to be fundamentally impossible for generative models. It is susceptible to attacks such as rewording and spoofing.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "[ 35, 51, 66, 67, 71] Feature-based detection To identify and classify based on extracted discriminating features. Susceptible to adversarial attacks such as paraphrasing.",
                "cite_spans": [
                    {
                        "start": 2,
                        "end": 5,
                        "text": "35,",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 6,
                        "end": 9,
                        "text": "51,",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 10,
                        "end": 13,
                        "text": "66,",
                        "ref_id": "BIBREF65"
                    },
                    {
                        "start": 14,
                        "end": 17,
                        "text": "67,",
                        "ref_id": "BIBREF66"
                    },
                    {
                        "start": 18,
                        "end": 21,
                        "text": "71]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "watermark intentionally, human writers struggle to do so if the text exceeds 1,000 words. This interpretation emerges as a significant characteristic of watermarking. Ultimately, according to this work, watermarking remains the most dependable strategy, as alternative paradigms like retrieval and loss-based detection have not demonstrated substantial improvements with increasing text length. Despite prior findings, a recent study by Zhang et al. [71] demonstrates that, under some assumptions, no robust watermarking scheme can prevent an attacker from removing the watermark without significantly degrading the output quality. We will further discuss this finding in section 5.",
                "cite_spans": [
                    {
                        "start": 450,
                        "end": 454,
                        "text": "[71]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking-based Detection",
                "sec_num": "3.4"
            },
            {
                "text": "Another stream of work is to identify and classify text based on distinguishing features. For instance, Yu et al. [67] have discovered a genetic inheritance characteristic in GPT-generated text. Leveraging this characteristic, the model's output becomes a rearrangement of content from its training corpus. In other words, when repeatedly answering questions, the model's responses are constrained by the information within its training data, resulting in limited variations. This hypothesis suggests that the output of an LLM, such as ChatGPT, is predictable. Therefore, for highly similar questions, the model tends to produce correspondingly similar responses.",
                "cite_spans": [
                    {
                        "start": 114,
                        "end": 118,
                        "text": "[67]",
                        "ref_id": "BIBREF66"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "Drawing an analogy, paternity testing utilizes DNA profiles to determine whether an individual is the biological parent of another person. This process becomes particularly crucial when legal rights and responsibilities related to parenthood are in question, and uncertainty exists regarding the child's paternity.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "In another study, Yang et al. [66] introduce a training-free detection strategy known as Divergent N-Gram Analysis (DNA-GPT). This approach assesses the dissimilarities between a given text and its remaining truncated portions using n-gram analysis in black-box scenarios or probability divergence in white-box scenarios.",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 34,
                        "text": "[66]",
                        "ref_id": "BIBREF65"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "For the black box scenario, Yang et al. define DNA-GPT BScore:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "( , \u03a9) = 1 \u03a3 =1 \u03a3 = 0 ( ) |n-grams( \u02c6 ) \u2229 n-grams( 2 )| | \u02c6 ||n-grams( 2 )|",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "where is the LLM output, 2 the human written ground truth, ( ) a weight function for different n-grams and \u03a9 = { \u02c6 1 , . . . , \u02c6 }",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "For white-box scenario, they propose calculating a DNA-GPT WScore between \u03a9 and :",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "( , \u03a9) = 1 \u03a3 =1 ( 2 | 1 ) ( \u02c6 | 1 )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "Where \u03a9 is a set of samples of an LM decoder and \u02c6 = ( 1 ) and 2 is the human written ground truth. \"In both black-box and white-box scenarios, two parameters significantly impact detection accuracy: the truncation ratio and the number of re-prompting iterations . This strategy reveals substantial discrepancies between AIgenerated and human-written texts. This approach highlights substantial disparities between AI-generated and human-written texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "Another distinguishing feature is the vulnerability of text to manipulations. Both AI-generated and human-written texts can be adversely affected by minor alterations, such as word replacements. However, recent research have revealed that machine-generated text is particularly prone to such manipulations [35, 51] . For instance, Su et al. [51] propose a metric called the Log-Likelihood Log-Rank Ratio (LRR) to quantify the sensitivity of LLMs to perturbations:",
                "cite_spans": [
                    {
                        "start": 306,
                        "end": 310,
                        "text": "[35,",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 311,
                        "end": 314,
                        "text": "51]",
                        "ref_id": "BIBREF50"
                    },
                    {
                        "start": 341,
                        "end": 345,
                        "text": "[51]",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "= - \u03a3 =1 log ( | < ) \u03a3 =1 log ( | < )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "where ( | < ) \u2265 1 is the rank of token conditioned on the previous tokens [51] . The Log-Likelihood in the numerator reflects the absolute confidence for the correct token, whereas the Log-Rank in the denominator considers relative confidence. Together, they provide complementary information about the texts.",
                "cite_spans": [
                    {
                        "start": 74,
                        "end": 78,
                        "text": "[51]",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "They also propose Normalized Log-Rank Perturbation (NPR):",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "NPR = 1 \u03a3 =1 log ( \u02dc ) log ( )",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "where small perturbations are applied on the target text to produce the perturbed text \u02dc . This work demonstrates that the LRR tends to be larger for machine-generated text, making it a useful discriminator between machine-generated and human-written content. One plausible explanation is that, in machine-generated text, the log rank stands out more prominently than the log likelihood, resulting in a distinct pattern that LRR captures. The rationale behind NPR lies in the fact that both machinegenerated and human-written texts experience adverse effects from minor alterations. Specifically, the log rank score tends to increase after perturbations. However, machine-generated text is particularly vulnerable to such alterations, resulting in a more pronounced increase in the log rank score following perturbation. As a result, this pattern suggests a higher NPR score for AI-generated texts [51] .",
                "cite_spans": [
                    {
                        "start": 898,
                        "end": 902,
                        "text": "[51]",
                        "ref_id": "BIBREF50"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection via Discriminating Features",
                "sec_num": "3.5"
            },
            {
                "text": "Despite various methods mentioned above, most categories of detection strategies are susceptible to paraphrasing or spoofing attacks.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VULNERABILITIES OF DETECTION TECHNIQUES",
                "sec_num": "4"
            },
            {
                "text": "To tackle this, retrieval-based detectors have been employed as detection strategies. As mentioned earlier, these detectors store the output of LLMs in a database and perform semantic searches to extract the best matches. This approach enhances the detector's resilience against paraphrasing attacks. However, there are privacy concerns related to storing user-LLM conversations. Also, this technique is ineffective when dealing with recursive paraphrasing [44] .",
                "cite_spans": [
                    {
                        "start": 457,
                        "end": 461,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VULNERABILITIES OF DETECTION TECHNIQUES",
                "sec_num": "4"
            },
            {
                "text": "In addition, researchers have discovered that by effectively optimizing prompts, LLMs can successfully evade many detection techniques. For example, Lu et al. [33] propose a novel method called Substitution-based In-Context example Optimization (SICO), where discriminating features are extracted from human and AI-generated texts. These features, along with a paraphrasing prompt, are combined and fed to the LLM to modify its output. The prompt is carefully optimized through word and sentence replacements, aiming to minimize the chances of detection while maximizing the similarity between human and AI-generated texts. The results strongly demonstrate the vulnerability of existing methods.",
                "cite_spans": [
                    {
                        "start": 159,
                        "end": 163,
                        "text": "[33]",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "VULNERABILITIES OF DETECTION TECHNIQUES",
                "sec_num": "4"
            },
            {
                "text": "Despite being considered an effective detection strategy, watermarking faces several challenges. Firstly, unless all LLMs are uniformly safeguarded, watermarking remains ineffective. Secondly, its practical applicability is limited, especially when dealing with black-box language models. Thirdly, API providers often withhold probability distributions, preventing third-party developers from independently watermarking text. Lastly, recent research suggest that no robust watermarking scheme can prevent attackers from removing watermarks without significantly degrading output quality. Therefore, watermarking generative models might be fundamentally unachievable, and alternative approaches are necessary to protect the intellectual property of model developers and LLM users. Table 1 illustrates an overview of detection strategies, highlighting the vulnerabilities associated with each category.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 786,
                        "end": 787,
                        "text": "1",
                        "ref_id": "TABREF0"
                    }
                ],
                "eq_spans": [],
                "section": "VULNERABILITIES OF DETECTION TECHNIQUES",
                "sec_num": "4"
            },
            {
                "text": "Given the increasing interest in detecting text generated by LLMs, researchers have recently investigated the theoretical aspects of this task. They explore the fundamental feasibility and limitations associated with identifying LLM-generated text. Sadasivan et al. for instance, present a discovery of impossibility [44] . They assert that:as language models become increasingly sophisticated and adept at emulating human text, the effectiveness of even the best-possible detectors diminishes significantly. Sadasivan et al. establish an upper-bound for the area under the Receiver Operating Characteristic (ROC) curve of any decoder D as:",
                "cite_spans": [
                    {
                        "start": 317,
                        "end": 321,
                        "text": "[44]",
                        "ref_id": "BIBREF43"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "EQUATION",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [
                    {
                        "start": 0,
                        "end": 8,
                        "text": "EQUATION",
                        "ref_id": "EQREF",
                        "raw_str": "(D) \u2264 1 2 + (M, H ) - (M, H ) 2",
                        "eq_num": "2"
                    }
                ],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "where (M, H ) is the total variation distance between machine and human-written texts. This formula indicates that when human and machine generated texts are very similar i.e., (M, H ) is very small, even the most effective detector may exhibit only marginal improvement over a random classifier.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "However, another interesting study by Chakraborty et al. [6] suggests that as long as the distributions of human-generated and AIgenerated texts are not identical (which is typically the case), it remains feasible to detect AI-generated texts. This detection becomes possible when we gather sufficient samples from each distribution. ) is an increasing sequence, it eventually converges to 1 as the number of samples for each distribution increases. If this happens, the total variation distance approaches 1 quickly, and hence increasing the AUROC.",
                "cite_spans": [
                    {
                        "start": 57,
                        "end": 60,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "In another work, Zhang et al. [71] , investigate the theoretical aspect of watermarking detection. They conceptualize watermarking as the process of embedding a statistical signal a.k.a. \"watermark\" into a model's output. This embedded watermark serves as a signal for later verification, ensuring that the output is indeed originated from the model. A robust watermarking prevents an attacker from erasing the watermark without causing significant quality degradation.",
                "cite_spans": [
                    {
                        "start": 30,
                        "end": 34,
                        "text": "[71]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "The authors introduce two key assumptions in their work. First, they propose the concept of a \"Quality Oracle\", which grants the attacker access to an oracle capable of evaluating the quality of outputs. This oracle assists the attacker in assessing the quality of modified responses. Second, they introduce the \"Perturbation Oracle\", which allows the attacker to modify an output while maintaining a nontrivial probability of preserving quality. The perturbation oracle essentially induces an efficiently mixing random walk on highquality outputs. Their investigation culminates in a compelling result: For every public or secret-key watermarking setting that satisfying these assumptions, there exists an efficient attacker. \"Given a prompt and a watermarked output , this attacker can leverage the quality and perturbation oracles to obtain an output \u2032 with a probability very close to 1. The attacker's goal is to find an output \u2032 such that (1) \u2032 is not watermarked with high probability and",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "(2) ( , \u2032 ) \u2265 ( , )\" [71] . In simpler terms, watermarking without causing significant quality degradation is impossible. Thus, the authors propose alternative approaches to safeguard the intellectual property of model developers.",
                "cite_spans": [
                    {
                        "start": 21,
                        "end": 25,
                        "text": "[71]",
                        "ref_id": "BIBREF70"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "Detecting AI-generated text is an essential and complex task and current state-of-the-art methods occasionally face limitations due to a lack of comprehensive understanding regarding the fundamental feasibility and boundaries of this task. Thus, it is crucial to continue exploring and investigating the theoretical aspects of the matter.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "POSSIBILITY OF DETECTION THROUGH A THEORETICAL LENS",
                "sec_num": "5"
            },
            {
                "text": "This paper offers a comprehensive overview of the latest advancements and recommended practices in detecting AI-generated text. However, existing methods often face limitations and are susceptible to malicious attacks. Thus, it is essential to continue exploring both theoretical and practical aspects of this realm. In light of this, we propose opportunities for advancing the field of responsible AI, vulnerability assessment, and risk mitigation studies related to LLMs. Some of these opportunities include:",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NEW OPPORTUNITIES AND FUTURE RESEARCH",
                "sec_num": "6"
            },
            {
                "text": "Enhancing the diversity and representativeness of datasets is crucial for training and evaluating AI-text detection models. The existing datasets may not comprise all the potential types and sources of AI-generated content.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NEW OPPORTUNITIES AND FUTURE RESEARCH",
                "sec_num": "6"
            },
            {
                "text": "Further investigating interpretable features to discern the subtle differences between human-written and AI-generated text considering the limited availability of such features.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NEW OPPORTUNITIES AND FUTURE RESEARCH",
                "sec_num": "6"
            },
            {
                "text": "Exploring advanced and adaptable learning techniques to effectively address the dynamic and ever-evolving nature of AI-generated text. These methods include adversarial learning, meta-learning, and self-supervised learning [58] .",
                "cite_spans": [
                    {
                        "start": 223,
                        "end": 227,
                        "text": "[58]",
                        "ref_id": "BIBREF57"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NEW OPPORTUNITIES AND FUTURE RESEARCH",
                "sec_num": "6"
            },
            {
                "text": "A comprehensive understanding of the fundamental feasibility and boundaries as there is often an absence of a thorough grasp of the fundamental feasibility and limitations within current SOTA methods. Therefore, there is a need for deeper exploration and investigation into the theoretical aspects of this task to facilitate the creation of more resilient and efficient techniques.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "NEW OPPORTUNITIES AND FUTURE RESEARCH",
                "sec_num": "6"
            },
            {
                "text": "In this paper, we delve into a thorough study of AI-generated text. Our analysis comprises not only the potential risks and misuses associated with content generated by LLMs, but also investigates widely recognized techniques for mitigating these risks. As a pivotal mitigation strategy, we conduct a comprehensive study of AIgenerated text detection techniques, categorizing them into five distinct categories and meticulously comparing their weaknesses and vulnerabilities. Our investigation approaches AI-generated text detection from both empirical and theoretical angles, shedding light on the intricacies of the field, which leads to proposing new avenues of research to bolster this critical area of study.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "CONCLUSIONS",
                "sec_num": "7"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "ZeroGPT: AI Text Detector",
                "authors": [],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "2023. ZeroGPT: AI Text Detector. https://www.zerogpt.com",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Beno\u00eet Sagot, and Djam\u00e9 Seddah. 2023. Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect?",
                "authors": [
                    {
                        "first": "Virginie",
                        "middle": [],
                        "last": "Wissam Antoun",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mouilleron",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wissam Antoun, Virginie Mouilleron, Beno\u00eet Sagot, and Djam\u00e9 Seddah. 2023. Towards a Robust Detection of Language Model Generated Text: Is ChatGPT that Easy to Detect? ArXiv abs/2306.05871 (2023).",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Real or Fake? Learning to Discriminate Machine from Human Generated Text",
                "authors": [
                    {
                        "first": "Anton",
                        "middle": [],
                        "last": "Bakhtin",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Gross",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Yuntian",
                        "middle": [],
                        "last": "Deng",
                        "suffix": ""
                    },
                    {
                        "first": "Marc'aurelio",
                        "middle": [],
                        "last": "Ranzato",
                        "suffix": ""
                    },
                    {
                        "first": "Arthur",
                        "middle": [
                            "D"
                        ],
                        "last": "Szlam",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Anton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc'Aurelio Ranzato, and Arthur D. Szlam. 2019. Real or Fake? Learning to Discriminate Machine from Human Generated Text. ArXiv abs/1906.03351 (2019).",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "BERT-Based Sentiment Analysis: A Software Engineering Perspective",
                "authors": [
                    {
                        "first": "Himanshu",
                        "middle": [],
                        "last": "Batra",
                        "suffix": ""
                    },
                    {
                        "first": "Narinder",
                        "middle": [],
                        "last": "Singh Punn",
                        "suffix": ""
                    },
                    {
                        "first": "Sanjay",
                        "middle": [],
                        "last": "Kumar Sonbhadra",
                        "suffix": ""
                    },
                    {
                        "first": "Sonali",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Conference on Database and Expert Systems Applications",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Himanshu Batra, Narinder Singh Punn, Sanjay Kumar Sonbhadra, and Sonali Agarwal. 2021. BERT-Based Sentiment Analysis: A Software Engineering Per- spective. In International Conference on Database and Expert Systems Applica- tions.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "Investigating Answerability of LLMs for Long-Form Question Answering",
                "authors": [
                    {
                        "first": "Moorthy",
                        "middle": [],
                        "last": "Meghana",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Bhat",
                        "suffix": ""
                    },
                    {
                        "first": "Ye",
                        "middle": [],
                        "last": "Meng",
                        "suffix": ""
                    },
                    {
                        "first": "Yingbo",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Semih",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Yavuz",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo Zhou, and Semih Yavuz. 2023. Investigating Answerability of LLMs for Long-Form Question Answering. ArXiv abs/2309.08210 (2023).",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "On the Possibilities of AI-Generated Text Detection",
                "authors": [
                    {
                        "first": "A",
                        "middle": [
                            "S"
                        ],
                        "last": "Souradip Chakraborty",
                        "suffix": ""
                    },
                    {
                        "first": "Sicheng",
                        "middle": [],
                        "last": "Bedi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Souradip Chakraborty, A. S. Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong Huang. 2023. On the Possibilities of AI-Generated Text Detection. ArXiv abs/2304.04736 (2023).",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Can Large Language Models Understand Content and Propagation for Misinformation Detection: An Empirical Study",
                "authors": [
                    {
                        "first": "Mengyang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Lingwei",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Cao",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Song",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mengyang Chen, Lingwei Wei, Han Cao, Wei Zhou, and Song Hu. 2023. Can Large Language Models Understand Content and Propagation for Misinforma- tion Detection: An Empirical Study. ArXiv abs/2311.12699 (2023).",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning",
                "authors": [
                    {
                        "first": "Xinyun",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Kimberly",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Dawn",
                        "middle": [
                            "Xiaodong"
                        ],
                        "last": "Song",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Xiaodong Song. 2017. Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. ArXiv abs/1712.05526 (2017).",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Deep reinforcement learning from human preferences",
                "authors": [
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Christiano",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Leike",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [
                            "B"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Miljan",
                        "middle": [],
                        "last": "Martic",
                        "suffix": ""
                    },
                    {
                        "first": "Shane",
                        "middle": [],
                        "last": "Legg",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1706.03741"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2023. Deep reinforcement learning from human preferences. arXiv:1706.03741 [stat.ML]",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "Deep Reinforcement Learning from Human Preferences",
                "authors": [
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Paul F Christiano",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Leike",
                        "suffix": ""
                    },
                    {
                        "first": "Miljan",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Shane",
                        "middle": [],
                        "last": "Martic",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Legg",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    }
                ],
                "year": 2017,
                "venue": "Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg",
                "volume": "30",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. 2017. Deep Reinforcement Learning from Human Preferences. In Ad- vances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc.",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "Training Verifiers to Solve Math Word Problems",
                "authors": [
                    {
                        "first": "Karl",
                        "middle": [],
                        "last": "Cobbe",
                        "suffix": ""
                    },
                    {
                        "first": "Vineet",
                        "middle": [],
                        "last": "Kosaraju",
                        "suffix": ""
                    },
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Bavarian",
                        "suffix": ""
                    },
                    {
                        "first": "Mark",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Heewoo",
                        "middle": [],
                        "last": "Jun",
                        "suffix": ""
                    },
                    {
                        "first": "Lukasz",
                        "middle": [],
                        "last": "Kaiser",
                        "suffix": ""
                    },
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Plappert",
                        "suffix": ""
                    },
                    {
                        "first": "Jerry",
                        "middle": [],
                        "last": "Tworek",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Hilton",
                        "suffix": ""
                    },
                    {
                        "first": "Reiichiro",
                        "middle": [],
                        "last": "Nakano",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [],
                        "last": "Hesse",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Schulman",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Rei- ichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. ArXiv abs/2110.14168 (2021). https://api.semanticscholar.org/CorpusID:239998651",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "FFT: Towards Harmlessness Evaluation and Analysis for LLMs with Factuality, Fairness, Toxicity",
                "authors": [
                    {
                        "first": "Shiyao",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenyu",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Yilong",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Wenyuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Tianyun",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Siqi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Tingwen",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. 2023. FFT: Towards Harmlessness Evaluation and Anal- ysis for LLMs with Factuality, Fairness, Toxicity. ArXiv abs/2311.18580 (2023).",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
                "authors": [
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Deshpande",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Vishvak",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Murahari",
                        "suffix": ""
                    },
                    {
                        "first": "A",
                        "middle": [],
                        "last": "Tanmay Rajpurohit",
                        "suffix": ""
                    },
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Kalyan",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Narasimhan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "A. Deshpande, Vishvak S. Murahari, Tanmay Rajpurohit, A. Kalyan, and Karthik Narasimhan. 2023. Toxicity in ChatGPT: Analyzing Persona-assigned Language Models. ArXiv abs/2304.05335 (2023).",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
                "authors": [
                    {
                        "first": "Yilun",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Shuang",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Torralba",
                        "suffix": ""
                    },
                    {
                        "first": "Joshua",
                        "middle": [],
                        "last": "Tenenbaum",
                        "suffix": ""
                    },
                    {
                        "first": "Igor",
                        "middle": [],
                        "last": "Mordatch",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yilun Du, Shuang Li, Antonio Torralba, Joshua Tenenbaum, and Igor Mordatch. 2023. Improving Factuality and Reasoning in Language Models through Multia- gent Debate.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Realtoxicityprompts: Evaluating neural toxic degeneration in language models",
                "authors": [
                    {
                        "first": "Suchin",
                        "middle": [],
                        "last": "Samuel Gehman",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Gururangan",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Sap",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Choi",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2009.11462"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in lan- guage models. arXiv preprint arXiv:2009.11462 (2020).",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "GLTR: Statistical Detection and Visualization of Generated Text",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Gehrmann",
                        "suffix": ""
                    },
                    {
                        "first": "Hendrik",
                        "middle": [],
                        "last": "Strobelt",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Gehrmann, Hendrik Strobelt, and Alexander M. Rush. 2019. GLTR: Statistical Detection and Visualization of Generated Text. In Annual Meeting of the Association for Computational Linguistics.",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising",
                "authors": [
                    {
                        "first": "Zhen",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Shangdi",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zhen Guo and Shangdi Yu. 2023. AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising. ArXiv abs/2311.07700 (2023).",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "REALM: Retrieval-Augmented Language Model Pre-Training (ICML'20)",
                "authors": [
                    {
                        "first": "Kelvin",
                        "middle": [],
                        "last": "Guu",
                        "suffix": ""
                    },
                    {
                        "first": "Kenton",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Zora",
                        "middle": [],
                        "last": "Tung",
                        "suffix": ""
                    },
                    {
                        "first": "Panupong",
                        "middle": [],
                        "last": "Pasupat",
                        "suffix": ""
                    },
                    {
                        "first": "Ming-Wei",
                        "middle": [],
                        "last": "Chang",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "368",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-Augmented Language Model Pre-Training (ICML'20). JMLR.org, Article 368, 10 pages.",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "Language Models (Mostly) Know What They Know",
                "authors": [
                    {
                        "first": "Saurav",
                        "middle": [],
                        "last": "Kadavath",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Conerly",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Henighan",
                        "suffix": ""
                    },
                    {
                        "first": "Dawn",
                        "middle": [],
                        "last": "Drain",
                        "suffix": ""
                    },
                    {
                        "first": "Ethan",
                        "middle": [],
                        "last": "Perez",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Schiefer",
                        "suffix": ""
                    },
                    {
                        "first": "Zac",
                        "middle": [],
                        "last": "Hatfield-Dodds",
                        "suffix": ""
                    },
                    {
                        "first": "Nova",
                        "middle": [],
                        "last": "Dassarma",
                        "suffix": ""
                    },
                    {
                        "first": "Eli",
                        "middle": [],
                        "last": "Tran-Johnson",
                        "suffix": ""
                    },
                    {
                        "first": "Scott",
                        "middle": [],
                        "last": "Johnston",
                        "suffix": ""
                    },
                    {
                        "first": "Sheer",
                        "middle": [],
                        "last": "El-Showk",
                        "suffix": ""
                    },
                    {
                        "first": "Andy",
                        "middle": [],
                        "last": "Jones",
                        "suffix": ""
                    },
                    {
                        "first": "Nelson",
                        "middle": [],
                        "last": "Elhage",
                        "suffix": ""
                    },
                    {
                        "first": "Tristan",
                        "middle": [],
                        "last": "Hume",
                        "suffix": ""
                    },
                    {
                        "first": "Anna",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Yuntao",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Bowman",
                        "suffix": ""
                    },
                    {
                        "first": "Stanislav",
                        "middle": [],
                        "last": "Fort",
                        "suffix": ""
                    },
                    {
                        "first": "Deep",
                        "middle": [],
                        "last": "Ganguli",
                        "suffix": ""
                    },
                    {
                        "first": "Danny",
                        "middle": [],
                        "last": "Hernandez",
                        "suffix": ""
                    },
                    {
                        "first": "Josh",
                        "middle": [],
                        "last": "Jacobson",
                        "suffix": ""
                    },
                    {
                        "first": "Jackson",
                        "middle": [],
                        "last": "Kernion",
                        "suffix": ""
                    },
                    {
                        "first": "Shauna",
                        "middle": [],
                        "last": "Kravec",
                        "suffix": ""
                    },
                    {
                        "first": "Liane",
                        "middle": [],
                        "last": "Lovitt",
                        "suffix": ""
                    },
                    {
                        "first": "Kamal",
                        "middle": [],
                        "last": "Ndousse",
                        "suffix": ""
                    },
                    {
                        "first": "Catherine",
                        "middle": [],
                        "last": "Olsson",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Ringer",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [],
                        "last": "Joseph",
                        "suffix": ""
                    },
                    {
                        "first": "Ben",
                        "middle": [],
                        "last": "Mann",
                        "suffix": ""
                    },
                    {
                        "first": "Sam",
                        "middle": [],
                        "last": "Mccandlish",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Olah",
                        "suffix": ""
                    },
                    {
                        "first": "Jared",
                        "middle": [],
                        "last": "Kaplan",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2207.05221[cs.CL]"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran- Johnson, Scott Johnston, Sheer El-Showk, Andy Jones, Nelson Elhage, Tris- tan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Gan- guli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Kaplan. 2022. Language Models (Mostly) Know What They Know. arXiv:2207.05221 [cs.CL]",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Will ChatGPT get you caught? Rethinking of Plagiarism Detection",
                "authors": [
                    {
                        "first": "Mohammad",
                        "middle": [],
                        "last": "Khalil",
                        "suffix": ""
                    },
                    {
                        "first": "Erkan",
                        "middle": [],
                        "last": "Er",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2302.04335[cs.AI]"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Mohammad Khalil and Erkan Er. 2023. Will ChatGPT get you caught? Rethink- ing of Plagiarism Detection. arXiv:2302.04335 [cs.AI]",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "Reliability Check: An Analysis of GPT-3's Response to Sensitive Topics and Prompt Wording",
                "authors": [
                    {
                        "first": "Aisha",
                        "middle": [],
                        "last": "Khatun",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Aisha Khatun and Daniel Brown. 2023. Reliability Check: An Analysis of GPT- 3's Response to Sensitive Topics and Prompt Wording. ArXiv abs/2306.06199 (2023).",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning",
                "authors": [
                    {
                        "first": "Kiana",
                        "middle": [],
                        "last": "Kheiri",
                        "suffix": ""
                    },
                    {
                        "first": "Hamid",
                        "middle": [],
                        "last": "Karimi",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kiana Kheiri and Hamid Karimi. 2023. SentimentGPT: Exploiting GPT for Ad- vanced Sentiment Analysis and its Departure from Current Machine Learning. ArXiv abs/2307.10234 (2023).",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A Watermark for Large Language Models",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Kirchenbauer",
                        "suffix": ""
                    },
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Geiping",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxin",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Katz",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Miers",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Goldstein",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "International Conference on Machine Learning",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. 2023. A Watermark for Large Lan- guage Models. In International Conference on Machine Learning. https://api.semanticscholar.org/CorpusID:256194179",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "On the Reliability of Watermarks for Large Language Models",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Kirchenbauer",
                        "suffix": ""
                    },
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Geiping",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxin",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Manli",
                        "middle": [],
                        "last": "Shu",
                        "suffix": ""
                    },
                    {
                        "first": "Khalid",
                        "middle": [],
                        "last": "Saifullah",
                        "suffix": ""
                    },
                    {
                        "first": "Kezhi",
                        "middle": [],
                        "last": "Kong",
                        "suffix": ""
                    },
                    {
                        "first": "Kasun",
                        "middle": [],
                        "last": "Fernando",
                        "suffix": ""
                    },
                    {
                        "first": "Aniruddha",
                        "middle": [],
                        "last": "Saha",
                        "suffix": ""
                    },
                    {
                        "first": "Micah",
                        "middle": [],
                        "last": "Goldblum",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Goldstein",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, and Tom Gold- stein. 2023. On the Reliability of Watermarks for Large Language Models. ArXiv abs/2306.04634 (2023).",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "Unveiling Safety Vulnerabilities of Large Language Models",
                "authors": [
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Kour",
                        "suffix": ""
                    },
                    {
                        "first": "Marcel",
                        "middle": [],
                        "last": "Zalmanovici",
                        "suffix": ""
                    },
                    {
                        "first": "Naama",
                        "middle": [],
                        "last": "Zwerdling",
                        "suffix": ""
                    },
                    {
                        "first": "Esther",
                        "middle": [],
                        "last": "Goldbraich",
                        "suffix": ""
                    },
                    {
                        "first": "Ora",
                        "middle": [
                            "Nova"
                        ],
                        "last": "Fandina",
                        "suffix": ""
                    },
                    {
                        "first": "Ateret",
                        "middle": [],
                        "last": "Anaby-Tavor",
                        "suffix": ""
                    },
                    {
                        "first": "Orna",
                        "middle": [],
                        "last": "Raz",
                        "suffix": ""
                    },
                    {
                        "first": "Eitan",
                        "middle": [],
                        "last": "Farchi",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich, Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz, and Eitan Farchi. 2023. Un- veiling Safety Vulnerabilities of Large Language Models. ArXiv abs/2311.04124 (2023).",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense",
                "authors": [
                    {
                        "first": "Kalpesh",
                        "middle": [],
                        "last": "Krishna",
                        "suffix": ""
                    },
                    {
                        "first": "Yixiao",
                        "middle": [],
                        "last": "Song",
                        "suffix": ""
                    },
                    {
                        "first": "Marzena",
                        "middle": [],
                        "last": "Karpinska",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Wieting",
                        "suffix": ""
                    },
                    {
                        "first": "Mohit",
                        "middle": [],
                        "last": "Iyyer",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. 2023. Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense. ArXiv abs/2303.13408 (2023).",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Validating large language models with relm",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Kuchnik",
                        "suffix": ""
                    },
                    {
                        "first": "Virginia",
                        "middle": [],
                        "last": "Smith",
                        "suffix": ""
                    },
                    {
                        "first": "George",
                        "middle": [],
                        "last": "Amvrosiadis",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Proceedings of Machine Learning and Systems",
                "volume": "5",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Kuchnik, Virginia Smith, and George Amvrosiadis. 2023. Validating large language models with relm. Proceedings of Machine Learning and Systems 5 (2023).",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond",
                "authors": [
                    {
                        "first": "Philippe",
                        "middle": [],
                        "last": "Laban",
                        "suffix": ""
                    },
                    {
                        "first": "Wojciech",
                        "middle": [],
                        "last": "Kryscinski",
                        "suffix": ""
                    },
                    {
                        "first": "Divyansh",
                        "middle": [],
                        "last": "Agarwal",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "R"
                        ],
                        "last": "Fabbri",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "R",
                        "middle": [],
                        "last": "Shafiq",
                        "suffix": ""
                    },
                    {
                        "first": "Chien-Sheng",
                        "middle": [],
                        "last": "Joty",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Philippe Laban, Wojciech Kryscinski, Divyansh Agarwal, Alexander R. Fab- bri, Caiming Xiong, Shafiq R. Joty, and Chien-Sheng Wu. 2023. LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond. ArXiv abs/2305.14540 (2023).",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Solving Quantitative Reasoning Problems with Language Models",
                "authors": [
                    {
                        "first": "Aitor",
                        "middle": [],
                        "last": "Lewkowycz",
                        "suffix": ""
                    },
                    {
                        "first": "Anders",
                        "middle": [],
                        "last": "Andreassen",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Dohan",
                        "suffix": ""
                    },
                    {
                        "first": "Ethan",
                        "middle": [],
                        "last": "Dyer",
                        "suffix": ""
                    },
                    {
                        "first": "Henryk",
                        "middle": [],
                        "last": "Michalewski",
                        "suffix": ""
                    },
                    {
                        "first": "Vinay",
                        "middle": [],
                        "last": "Ramasesh",
                        "suffix": ""
                    },
                    {
                        "first": "Ambrose",
                        "middle": [],
                        "last": "Slone",
                        "suffix": ""
                    },
                    {
                        "first": "Cem",
                        "middle": [],
                        "last": "Anil",
                        "suffix": ""
                    },
                    {
                        "first": "Imanol",
                        "middle": [],
                        "last": "Schlag",
                        "suffix": ""
                    },
                    {
                        "first": "Theo",
                        "middle": [],
                        "last": "Gutman-Solo",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhuai",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Behnam",
                        "middle": [],
                        "last": "Neyshabur",
                        "suffix": ""
                    },
                    {
                        "first": "Guy",
                        "middle": [],
                        "last": "Gur-Ari",
                        "suffix": ""
                    },
                    {
                        "first": "Sara",
                        "middle": [],
                        "last": "Vedant",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Abdali",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Anarfi",
                        "suffix": ""
                    },
                    {
                        "first": "\u2020",
                        "middle": [],
                        "last": "Cj Barberan",
                        "suffix": ""
                    },
                    {
                        "first": "Jia",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "\u2020",
                        "middle": [],
                        "last": "Misra",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2206.14858[cs.CL]"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant , , Sara Abdali, Richard Anarfi, CJ Barberan \u2020 , and Jia He \u2020 Misra. 2022. Solving Quantitative Reasoning Problems with Language Models. arXiv:2206.14858 [cs.CL]",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Deepfake Text Detection in the Wild",
                "authors": [
                    {
                        "first": "Yafu",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Qintong",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Leyang",
                        "middle": [],
                        "last": "Cui",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Bi",
                        "suffix": ""
                    },
                    {
                        "first": "Longyue",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Linyi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Shuming",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang, Shum- ing Shi, and Yue Zhang. 2023. Deepfake Text Detection in the Wild. ArXiv abs/2305.13242 (2023).",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "GPT detectors are biased against non-native English writers",
                "authors": [
                    {
                        "first": "Weixin",
                        "middle": [],
                        "last": "Liang",
                        "suffix": ""
                    },
                    {
                        "first": "Mert",
                        "middle": [],
                        "last": "Yuksekgonul",
                        "suffix": ""
                    },
                    {
                        "first": "Yining",
                        "middle": [],
                        "last": "Mao",
                        "suffix": ""
                    },
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Zou",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Patterns",
                "volume": "4",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "DOI": [
                        "10.1016/j.patter.2023.100779"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. 2023. GPT detectors are biased against non-native English writers. Patterns 4, 7 (2023), 100779. https://doi.org/10.1016/j.patter.2023.100779",
                "links": null
            },
            "BIBREF31": {
                "ref_id": "b31",
                "title": "Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT",
                "authors": [
                    {
                        "first": "Zeyan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Zijun",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    },
                    {
                        "first": "Fengjun",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Bo",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zeyan Liu, Zijun Yao, Fengjun Li, and Bo Luo. 2023. Check Me If You Can: Detecting ChatGPT-Generated Academic Writing using CheckGPT. ArXiv abs/2306.05524 (2023).",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
                "authors": [
                    {
                        "first": "Ning",
                        "middle": [],
                        "last": "Lu",
                        "suffix": ""
                    },
                    {
                        "first": "Shengcai",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ning Lu, Shengcai Liu, Ruidan He, and Ke Tang. 2023. Large Language Models can be Guided to Evade AI-Generated Text Detection. ArXiv abs/2305.10847 (2023).",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Self-Refine: Iterative Refinement with Self-Feedback",
                "authors": [
                    {
                        "first": "Aman",
                        "middle": [],
                        "last": "Madaan",
                        "suffix": ""
                    },
                    {
                        "first": "Niket",
                        "middle": [],
                        "last": "Tandon",
                        "suffix": ""
                    },
                    {
                        "first": "Prakhar",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Skyler",
                        "middle": [],
                        "last": "Hallinan",
                        "suffix": ""
                    },
                    {
                        "first": "Luyu",
                        "middle": [],
                        "last": "Gao",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Wiegreffe",
                        "suffix": ""
                    },
                    {
                        "first": "Uri",
                        "middle": [],
                        "last": "Alon",
                        "suffix": ""
                    },
                    {
                        "first": "Nouha",
                        "middle": [],
                        "last": "Dziri",
                        "suffix": ""
                    },
                    {
                        "first": "Shrimai",
                        "middle": [],
                        "last": "Prabhumoye",
                        "suffix": ""
                    },
                    {
                        "first": "Yiming",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Shashank",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "Bodhisattwa",
                        "middle": [],
                        "last": "Prasad Majumder",
                        "suffix": ""
                    },
                    {
                        "first": "Katherine",
                        "middle": [],
                        "last": "Hermann",
                        "suffix": ""
                    },
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Welleck",
                        "suffix": ""
                    },
                    {
                        "first": "Amir",
                        "middle": [],
                        "last": "Yazdanbakhsh",
                        "suffix": ""
                    },
                    {
                        "first": "Peter",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.17651[cs.CL]"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine: Iterative Refinement with Self-Feedback. arXiv:2303.17651 [cs.CL]",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Mitchell",
                        "suffix": ""
                    },
                    {
                        "first": "Yoonho",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [],
                        "last": "Khazatsky",
                        "suffix": ""
                    },
                    {
                        "first": "Christopher",
                        "middle": [
                            "D"
                        ],
                        "last": "Manning",
                        "suffix": ""
                    },
                    {
                        "first": "Chelsea",
                        "middle": [],
                        "last": "Finn",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn. 2023. DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. ArXiv abs/2301.11305 (2023).",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities",
                "authors": [
                    {
                        "first": "Maximilian",
                        "middle": [],
                        "last": "Mozes",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanli",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Bennett",
                        "middle": [],
                        "last": "Kleinberg",
                        "suffix": ""
                    },
                    {
                        "first": "Lewis",
                        "middle": [
                            "D"
                        ],
                        "last": "Griffin",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D. Griffin. 2023. Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabil- ities. ArXiv abs/2308.12833 (2023).",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
                "authors": [
                    {
                        "first": "Shreya",
                        "middle": [],
                        "last": "Muneeswaran",
                        "suffix": ""
                    },
                    {
                        "first": "Siva",
                        "middle": [],
                        "last": "Saxena",
                        "suffix": ""
                    },
                    {
                        "first": "M V Sai",
                        "middle": [],
                        "last": "Prasad",
                        "suffix": ""
                    },
                    {
                        "first": "Advaith",
                        "middle": [],
                        "last": "Prakash",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Shankar",
                        "suffix": ""
                    },
                    {
                        "first": "Vishal",
                        "middle": [],
                        "last": "Varun",
                        "suffix": ""
                    },
                    {
                        "first": "Saisubramaniam",
                        "middle": [],
                        "last": "Vaddina",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gopalakrishnan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "I Muneeswaran, Shreya Saxena, Siva Prasad, M V Sai Prakash, Advaith Shankar, V Varun, Vishal Vaddina, and Saisubramaniam Gopalakrishnan. 2023. Minimiz- ing Factual Inconsistency and Hallucination in Large Language Models. ArXiv abs/2311.13878 (2023).",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Show Your Work: Scratchpads for Intermediate Computation with Language Models",
                "authors": [
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Nye",
                        "suffix": ""
                    },
                    {
                        "first": "Anders",
                        "middle": [],
                        "last": "Johan Andreassen",
                        "suffix": ""
                    },
                    {
                        "first": "Guy",
                        "middle": [],
                        "last": "Gur-Ari",
                        "suffix": ""
                    },
                    {
                        "first": "Henryk",
                        "middle": [],
                        "last": "Michalewski",
                        "suffix": ""
                    },
                    {
                        "first": "Jacob",
                        "middle": [],
                        "last": "Austin",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Bieber",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Dohan",
                        "suffix": ""
                    },
                    {
                        "first": "Aitor",
                        "middle": [],
                        "last": "Lewkowycz",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Bosma",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Luan",
                        "suffix": ""
                    },
                    {
                        "first": "Charles",
                        "middle": [],
                        "last": "Sutton",
                        "suffix": ""
                    },
                    {
                        "first": "Augustus",
                        "middle": [],
                        "last": "Odena",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena. 2022. Show Your Work: Scratchpads for Intermediate Computation with Language Models. https://openreview.net/forum?id=iedYJm92o0a",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "On the Risk of Misinformation Pollution with Large Language Models",
                "authors": [
                    {
                        "first": "Yikang",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Liangming",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Preslav",
                        "middle": [],
                        "last": "Nakov",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Wang. 2023. On the Risk of Misinformation Pollution with Large Lan- guage Models.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "On the Risk of Misinformation Pollution with Large Language Models",
                "authors": [
                    {
                        "first": "Yikang",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Liangming",
                        "middle": [],
                        "last": "Pan",
                        "suffix": ""
                    },
                    {
                        "first": "Wenhu",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Preslav",
                        "middle": [],
                        "last": "Nakov",
                        "suffix": ""
                    },
                    {
                        "first": "Min-Yen",
                        "middle": [],
                        "last": "Kan",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023",
                "volume": "",
                "issue": "",
                "pages": "1389--1403",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yikang Pan, Liangming Pan, Wenhu Chen, Preslav Nakov, Min-Yen Kan, and William Wang. 2023. On the Risk of Misinformation Pollution with Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 1389-1403. https://aclanthology.org/2023.findings-emnlp.97",
                "links": null
            },
            "BIBREF40": {
                "ref_id": "b40",
                "title": "Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark",
                "authors": [
                    {
                        "first": "Wenjun",
                        "middle": [],
                        "last": "Peng",
                        "suffix": ""
                    },
                    {
                        "first": "Jingwei",
                        "middle": [],
                        "last": "Yi",
                        "suffix": ""
                    },
                    {
                        "first": "Fangzhao",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Shangxi",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Bin",
                        "middle": [],
                        "last": "Benjamin Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Lingjuan",
                        "middle": [],
                        "last": "Lyu",
                        "suffix": ""
                    },
                    {
                        "first": "Binxing",
                        "middle": [],
                        "last": "Jiao",
                        "suffix": ""
                    },
                    {
                        "first": "Tong",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Guangzhong",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Xing",
                        "middle": [],
                        "last": "Xie",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "ACL 2023",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Benjamin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, and Xing Xie. 2023. Are You Copying My Model? Protecting the Copyright of Large Language Mod- els for EaaS via Backdoor Watermark. In ACL 2023.",
                "links": null
            },
            "BIBREF41": {
                "ref_id": "b41",
                "title": "Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level",
                "authors": [
                    {
                        "first": "Mujahid",
                        "middle": [],
                        "last": "Ali Quidwai",
                        "suffix": ""
                    },
                    {
                        "first": "Chun",
                        "middle": [
                            "Xing"
                        ],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Parijat",
                        "middle": [],
                        "last": "Dube",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mujahid Ali Quidwai, Chun Xing Li, and Parijat Dube. 2023. Beyond Black Box AI-Generated Plagiarism Detection: From Sentence to Document Level. ArXiv abs/2306.08122 (2023).",
                "links": null
            },
            "BIBREF42": {
                "ref_id": "b42",
                "title": "Explain Yourself! Leveraging Language Models for Commonsense Reasoning",
                "authors": [
                    {
                        "first": "Nazneen",
                        "middle": [],
                        "last": "Fatema Rajani",
                        "suffix": ""
                    },
                    {
                        "first": "Bryan",
                        "middle": [],
                        "last": "Mccann",
                        "suffix": ""
                    },
                    {
                        "first": "Caiming",
                        "middle": [],
                        "last": "Xiong",
                        "suffix": ""
                    },
                    {
                        "first": "Richard",
                        "middle": [],
                        "last": "Socher",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "4932--4942",
                "other_ids": {
                    "DOI": [
                        "10.18653/v1/P19-1487"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Explain Yourself! Leveraging Language Models for Commonsense Rea- soning. In Proceedings of the 57th Annual Meeting of the Association for Compu- tational Linguistics. Association for Computational Linguistics, Florence, Italy, 4932-4942. https://doi.org/10.18653/v1/P19-1487",
                "links": null
            },
            "BIBREF43": {
                "ref_id": "b43",
                "title": "Can AI-Generated Text be Reliably Detected?",
                "authors": [
                    {
                        "first": "Aounon",
                        "middle": [],
                        "last": "Vinu Sankar Sadasivan",
                        "suffix": ""
                    },
                    {
                        "first": "S",
                        "middle": [],
                        "last": "Kumar",
                        "suffix": ""
                    },
                    {
                        "first": "Wenxiao",
                        "middle": [],
                        "last": "Balasubramanian",
                        "suffix": ""
                    },
                    {
                        "first": "Soheil",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Feizi",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Vinu Sankar Sadasivan, Aounon Kumar, S. Balasubramanian, Wenxiao Wang, and Soheil Feizi. 2023. Can AI-Generated Text be Reliably Detected? ArXiv abs/2303.11156 (2023).",
                "links": null
            },
            "BIBREF44": {
                "ref_id": "b44",
                "title": "Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks",
                "authors": [
                    {
                        "first": "Avi",
                        "middle": [],
                        "last": "Schwarzschild",
                        "suffix": ""
                    },
                    {
                        "first": "Micah",
                        "middle": [],
                        "last": "Goldblum",
                        "suffix": ""
                    },
                    {
                        "first": "Arjun",
                        "middle": [],
                        "last": "Gupta",
                        "suffix": ""
                    },
                    {
                        "first": "John",
                        "middle": [
                            "P"
                        ],
                        "last": "Dickerson",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Goldstein",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P. Dickerson, and Tom Goldstein. 2020. Just How Toxic is Data Poisoning? A Unified Benchmark for Backdoor and Data Poisoning Attacks. ArXiv abs/2006.12557 (2020).",
                "links": null
            },
            "BIBREF45": {
                "ref_id": "b45",
                "title": "Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI",
                "authors": [
                    {
                        "first": "Chamalke",
                        "middle": [],
                        "last": "Damith",
                        "suffix": ""
                    },
                    {
                        "first": "Julia",
                        "middle": [],
                        "last": "Senadeera",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Ive",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Damith Chamalke Senadeera and Julia Ive. 2022. Controlled Text Generation using T5 based Encoder-Decoder Soft Prompt Tuning and Analysis of the Utility of Generated Text in AI. ArXiv abs/2212.02924 (2022).",
                "links": null
            },
            "BIBREF46": {
                "ref_id": "b46",
                "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
                "authors": [
                    {
                        "first": "Erfan",
                        "middle": [],
                        "last": "Shayegani",
                        "suffix": ""
                    },
                    {
                        "first": "Md",
                        "middle": [],
                        "last": "Abdullah Al Mamun",
                        "suffix": ""
                    },
                    {
                        "first": "Yu",
                        "middle": [],
                        "last": "Fu",
                        "suffix": ""
                    },
                    {
                        "first": "Pedram",
                        "middle": [],
                        "last": "Zaree",
                        "suffix": ""
                    },
                    {
                        "first": "Yue",
                        "middle": [],
                        "last": "Dong",
                        "suffix": ""
                    },
                    {
                        "first": "B",
                        "middle": [],
                        "last": "Nael",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Abu-Ghazaleh",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Erfan Shayegani, Md. Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael B. Abu-Ghazaleh. 2023. Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks. ArXiv abs/2310.10844 (2023).",
                "links": null
            },
            "BIBREF47": {
                "ref_id": "b47",
                "title": "Reflexion: Language Agents with Verbal Reinforcement Learning",
                "authors": [
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Shinn",
                        "suffix": ""
                    },
                    {
                        "first": "Federico",
                        "middle": [],
                        "last": "Cassano",
                        "suffix": ""
                    },
                    {
                        "first": "Beck",
                        "middle": [],
                        "last": "Labash",
                        "suffix": ""
                    },
                    {
                        "first": "Ashwin",
                        "middle": [],
                        "last": "Gopinath",
                        "suffix": ""
                    },
                    {
                        "first": "Karthik",
                        "middle": [],
                        "last": "Narasimhan",
                        "suffix": ""
                    },
                    {
                        "first": "Shunyu",
                        "middle": [],
                        "last": "Yao",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2303.11366[cs.AI]"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Noah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. arXiv:2303.11366 [cs.AI]",
                "links": null
            },
            "BIBREF48": {
                "ref_id": "b48",
                "title": "Release Strategies and the Social Impacts of Language Models",
                "authors": [
                    {
                        "first": "Irene",
                        "middle": [],
                        "last": "Solaiman",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Brundage",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Ariel",
                        "middle": [],
                        "last": "Herbert-Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Jasmine",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, and Jasmine Wang. 2019. Release Strategies and the Social Impacts of Language Models. ArXiv abs/1908.09203 (2019).",
                "links": null
            },
            "BIBREF49": {
                "ref_id": "b49",
                "title": "AI bot ChatGPT writes smart essays-should academics worry?",
                "authors": [
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Stokel-Walker",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Nature",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Chris Stokel-Walker. 2022. AI bot ChatGPT writes smart essays-should aca- demics worry? Nature (2022).",
                "links": null
            },
            "BIBREF50": {
                "ref_id": "b50",
                "title": "De-tectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
                "authors": [
                    {
                        "first": "Jinyan",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Terry",
                        "middle": [],
                        "last": "Yue Zhuo",
                        "suffix": ""
                    },
                    {
                        "first": "Di",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Preslav",
                        "middle": [],
                        "last": "Nakov",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jinyan Su, Terry Yue Zhuo, Di Wang, and Preslav Nakov. 2023. De- tectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine- Generated Text. ArXiv abs/2306.05540 (2023).",
                "links": null
            },
            "BIBREF52": {
                "ref_id": "b52",
                "title": "HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis",
                "authors": [
                    {
                        "first": "Christoforos",
                        "middle": [],
                        "last": "Vasilatos",
                        "suffix": ""
                    },
                    {
                        "first": "Manaar",
                        "middle": [],
                        "last": "Alam",
                        "suffix": ""
                    },
                    {
                        "first": "Talal",
                        "middle": [],
                        "last": "Rahwan",
                        "suffix": ""
                    },
                    {
                        "first": "Yasir",
                        "middle": [],
                        "last": "Zaki",
                        "suffix": ""
                    },
                    {
                        "first": "Michail",
                        "middle": [],
                        "last": "Maniatakos",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki, and Michail Ma- niatakos. 2023. HowkGPT: Investigating the Detection of ChatGPT-generated University Student Homework through Context-Aware Perplexity Analysis. ArXiv abs/2305.18226 (2023).",
                "links": null
            },
            "BIBREF53": {
                "ref_id": "b53",
                "title": "Disinformation Capabilities of Large Language Models",
                "authors": [
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Vykopal",
                        "suffix": ""
                    },
                    {
                        "first": "'",
                        "middle": [],
                        "last": "Mat",
                        "suffix": ""
                    },
                    {
                        "first": "Ivan",
                        "middle": [],
                        "last": "Pikuliak",
                        "suffix": ""
                    },
                    {
                        "first": "R\u00f3bert",
                        "middle": [],
                        "last": "Srba",
                        "suffix": ""
                    },
                    {
                        "first": "Dominik",
                        "middle": [],
                        "last": "M\u00f3ro",
                        "suffix": ""
                    },
                    {
                        "first": "M\u00e1ria",
                        "middle": [],
                        "last": "Macko",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Bielikov\u00e1",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Ivan Vykopal, Mat'uvs Pikuliak, Ivan Srba, R\u00f3bert M\u00f3ro, Dominik Macko, and M\u00e1ria Bielikov\u00e1. 2023. Disinformation Capabilities of Large Language Models. ArXiv abs/2311.08838 (2023).",
                "links": null
            },
            "BIBREF54": {
                "ref_id": "b54",
                "title": "Bot or Human? Detecting ChatGPT Imposters with A Single Question",
                "authors": [
                    {
                        "first": "Hong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xuan",
                        "middle": [],
                        "last": "Luo",
                        "suffix": ""
                    },
                    {
                        "first": "Weizhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Xifeng",
                        "middle": [],
                        "last": "Yan",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hong Wang, Xuan Luo, Weizhi Wang, and Xifeng Yan. 2023. Bot or Human? Detecting ChatGPT Imposters with A Single Question. ArXiv abs/2305.06424 (2023).",
                "links": null
            },
            "BIBREF55": {
                "ref_id": "b55",
                "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
                "authors": [
                    {
                        "first": "Xuezhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Dale",
                        "middle": [],
                        "last": "Schuurmans",
                        "suffix": ""
                    },
                    {
                        "first": "V",
                        "middle": [],
                        "last": "Quoc",
                        "suffix": ""
                    },
                    {
                        "first": "Ed",
                        "middle": [
                            "H"
                        ],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Sharan",
                        "middle": [],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Aakanksha",
                        "middle": [],
                        "last": "Narang",
                        "suffix": ""
                    },
                    {
                        "first": "Denny",
                        "middle": [],
                        "last": "Chowdhery",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "The Eleventh International Conference on Learning Representations",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self- Consistency Improves Chain of Thought Reasoning in Language Mod- els. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=1PL1NIMMrw",
                "links": null
            },
            "BIBREF56": {
                "ref_id": "b56",
                "title": "Osama Mohammed Afzal, Tarek Mahmoud, Alham Fikri Aji, and Preslav Nakov. 2023. M4: Multi-generator, Multidomain, and Multi-lingual Black-Box Machine-Generated Text Detection",
                "authors": [
                    {
                        "first": "Yuxia",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jonibek",
                        "middle": [],
                        "last": "Mansurov",
                        "suffix": ""
                    },
                    {
                        "first": "Petar",
                        "middle": [],
                        "last": "Ivanov",
                        "suffix": ""
                    },
                    {
                        "first": "Jinyan",
                        "middle": [],
                        "last": "Su",
                        "suffix": ""
                    },
                    {
                        "first": "Artem",
                        "middle": [],
                        "last": "Shelmanov",
                        "suffix": ""
                    },
                    {
                        "first": "Akim",
                        "middle": [],
                        "last": "Tsvigun",
                        "suffix": ""
                    },
                    {
                        "first": "Chenxi",
                        "middle": [],
                        "last": "Whitehouse",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Yuxia Wang, Jonibek Mansurov, Petar Ivanov, Jinyan Su, Artem Shelmanov, Akim Tsvigun, Chenxi Whitehouse, Osama Mohammed Afzal, Tarek Mah- moud, Alham Fikri Aji, and Preslav Nakov. 2023. M4: Multi-generator, Multi- domain, and Multi-lingual Black-Box Machine-Generated Text Detection. ArXiv abs/2305.14902 (2023).",
                "links": null
            },
            "BIBREF57": {
                "ref_id": "b57",
                "title": "Testing of detection tools for AI-generated text",
                "authors": [
                    {
                        "first": "Debora",
                        "middle": [],
                        "last": "Weber-Wulff",
                        "suffix": ""
                    },
                    {
                        "first": "Alla",
                        "middle": [],
                        "last": "Anohina-Naumeca",
                        "suffix": ""
                    },
                    {
                        "first": "Sonja",
                        "middle": [],
                        "last": "Bjelobaba",
                        "suffix": ""
                    },
                    {
                        "first": "Tom'a\u0161",
                        "middle": [],
                        "last": "Folt\u00fdnek",
                        "suffix": ""
                    },
                    {
                        "first": "Jean",
                        "middle": [],
                        "last": "Gabriel Guerrero-Dib",
                        "suffix": ""
                    },
                    {
                        "first": "Olumide",
                        "middle": [],
                        "last": "Popoola",
                        "suffix": ""
                    },
                    {
                        "first": "Petr",
                        "middle": [],
                        "last": "Sigut",
                        "suffix": ""
                    },
                    {
                        "first": "Lorna",
                        "middle": [],
                        "last": "Waddington",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "International Journal for Educational Integrity",
                "volume": "19",
                "issue": "",
                "pages": "1--39",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tom'a\u0161 Folt\u00fdnek, Jean Gabriel Guerrero-Dib, Olumide Popoola, Petr Sigut, and Lorna Waddington. 2023. Testing of detection tools for AI-generated text. International Journal for Educational Integrity 19 (2023), 1-39.",
                "links": null
            },
            "BIBREF58": {
                "ref_id": "b58",
                "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
                "authors": [
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Wei",
                        "suffix": ""
                    },
                    {
                        "first": "Xuezhi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Dale",
                        "middle": [],
                        "last": "Schuurmans",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Bosma",
                        "suffix": ""
                    },
                    {
                        "first": "Fei",
                        "middle": [],
                        "last": "Xia",
                        "suffix": ""
                    },
                    {
                        "first": "Ed",
                        "middle": [
                            "H"
                        ],
                        "last": "Chi",
                        "suffix": ""
                    },
                    {
                        "first": "Denny",
                        "middle": [],
                        "last": "Quoc V Le",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le, and Denny Zhou. 2022. Chain of Thought Prompt- ing Elicits Reasoning in Large Language Models. In Advances in Neural Infor- mation Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=_VjQlMeSB_J",
                "links": null
            },
            "BIBREF59": {
                "ref_id": "b59",
                "title": "Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from Language Models",
                "authors": [
                    {
                        "first": "Laura",
                        "middle": [],
                        "last": "Weidinger",
                        "suffix": ""
                    },
                    {
                        "first": "F",
                        "middle": [
                            "J"
                        ],
                        "last": "John",
                        "suffix": ""
                    },
                    {
                        "first": "Maribeth",
                        "middle": [],
                        "last": "Mellor",
                        "suffix": ""
                    },
                    {
                        "first": "Conor",
                        "middle": [],
                        "last": "Rauh",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Griffin",
                        "suffix": ""
                    },
                    {
                        "first": "Po-Sen",
                        "middle": [],
                        "last": "Uesato",
                        "suffix": ""
                    },
                    {
                        "first": "Myra",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    },
                    {
                        "first": "Mia",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Borja",
                        "middle": [],
                        "last": "Glaese",
                        "suffix": ""
                    },
                    {
                        "first": "Atoosa",
                        "middle": [],
                        "last": "Balle",
                        "suffix": ""
                    },
                    {
                        "first": "Zachary",
                        "middle": [],
                        "last": "Kasirzadeh",
                        "suffix": ""
                    },
                    {
                        "first": "Sande",
                        "middle": [
                            "Minnich"
                        ],
                        "last": "Kenton",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "T"
                        ],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Hawkins",
                        "suffix": ""
                    },
                    {
                        "first": "Courtney",
                        "middle": [],
                        "last": "Stepleton",
                        "suffix": ""
                    },
                    {
                        "first": "Abeba",
                        "middle": [],
                        "last": "Biles",
                        "suffix": ""
                    },
                    {
                        "first": "Julia",
                        "middle": [],
                        "last": "Birhane",
                        "suffix": ""
                    },
                    {
                        "first": "Laura",
                        "middle": [],
                        "last": "Haas",
                        "suffix": ""
                    },
                    {
                        "first": "Lisa",
                        "middle": [
                            "Anne"
                        ],
                        "last": "Rimell",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [
                            "S"
                        ],
                        "last": "Hendricks",
                        "suffix": ""
                    },
                    {
                        "first": "Sean",
                        "middle": [],
                        "last": "Isaac",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Legassick",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Laura Weidinger, John F. J. Mellor, Maribeth Rauh, Conor Griffin, Jonathan Ue- sato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zachary Kenton, Sande Minnich Brown, William T. Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William S. Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Eth- ical and social risks of harm from Language Models. ArXiv abs/2112.04359 (2021).",
                "links": null
            },
            "BIBREF60": {
                "ref_id": "b60",
                "title": "Unveiling the Implicit Toxicity in Large Language Models",
                "authors": [
                    {
                        "first": "Jiaxin",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Pei",
                        "middle": [],
                        "last": "Ke",
                        "suffix": ""
                    },
                    {
                        "first": "Hao",
                        "middle": [],
                        "last": "Sun",
                        "suffix": ""
                    },
                    {
                        "first": "Zhexin",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Chengfei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Jinfeng",
                        "middle": [],
                        "last": "Bai",
                        "suffix": ""
                    },
                    {
                        "first": "Minlie",
                        "middle": [],
                        "last": "Huang",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, and Minlie Huang. 2023. Unveiling the Implicit Toxicity in Large Language Models. In Conference on Empirical Methods in Natural Language Processing.",
                "links": null
            },
            "BIBREF61": {
                "ref_id": "b61",
                "title": "Attacking Neural Text Detectors",
                "authors": [
                    {
                        "first": "Max",
                        "middle": [],
                        "last": "Wolff",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Max Wolff. 2020. Attacking Neural Text Detectors. ArXiv abs/2002.11768 (2020).",
                "links": null
            },
            "BIBREF62": {
                "ref_id": "b62",
                "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
                "authors": [
                    {
                        "first": "Tianci",
                        "middle": [],
                        "last": "Xue",
                        "suffix": ""
                    },
                    {
                        "first": "Ziqi",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Zhenhailong",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Chi",
                        "middle": [],
                        "last": "Han",
                        "suffix": ""
                    },
                    {
                        "first": "Pengfei",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Heng",
                        "middle": [],
                        "last": "Ji",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tianci Xue, Ziqi Wang, Zhenhailong Wang, Chi Han, Pengfei Yu, and Heng Ji. 2023. RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought. ArXiv abs/2305.11499 (2023).",
                "links": null
            },
            "BIBREF63": {
                "ref_id": "b63",
                "title": "Xuancheng Ren, Xu Sun, and Bin He. 2021. Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models",
                "authors": [
                    {
                        "first": "Wenkai",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Lei",
                        "middle": [],
                        "last": "Li",
                        "suffix": ""
                    },
                    {
                        "first": "Zhiyuan",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren, Xu Sun, and Bin He. 2021. Be Careful about Poisoned Word Embeddings: Exploring the Vulnerability of the Embedding Layers in NLP Models. ArXiv abs/2103.15543 (2021).",
                "links": null
            },
            "BIBREF64": {
                "ref_id": "b64",
                "title": "Watermarking Text Generated by Black-Box Language Models",
                "authors": [
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Kejiang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Weiming",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Chang",
                        "middle": [],
                        "last": "Rui Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuang",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Jie",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Han",
                        "middle": [],
                        "last": "Fang",
                        "suffix": ""
                    },
                    {
                        "first": "Neng",
                        "middle": [
                            "H"
                        ],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xi Yang, Kejiang Chen, Weiming Zhang, Chang rui Liu, Yuang Qi, Jie Zhang, Han Fang, and Neng H. Yu. 2023. Watermarking Text Generated by Black-Box Language Models. ArXiv abs/2305.08883 (2023).",
                "links": null
            },
            "BIBREF65": {
                "ref_id": "b65",
                "title": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text",
                "authors": [
                    {
                        "first": "Xianjun",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [],
                        "last": "Cheng",
                        "suffix": ""
                    },
                    {
                        "first": "Linda",
                        "middle": [],
                        "last": "Petzold",
                        "suffix": ""
                    },
                    {
                        "first": "William",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Wang",
                        "middle": [],
                        "last": "",
                        "suffix": ""
                    },
                    {
                        "first": "Haifeng",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xianjun Yang, Wei Cheng, Linda Petzold, William Yang Wang, and Haifeng Chen. 2023. DNA-GPT: Divergent N-Gram Analysis for Training-Free Detec- tion of GPT-Generated Text. ArXiv abs/2305.17359 (2023).",
                "links": null
            },
            "BIBREF66": {
                "ref_id": "b66",
                "title": "GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance",
                "authors": [
                    {
                        "first": "Xiao",
                        "middle": [],
                        "last": "Yu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuang",
                        "middle": [],
                        "last": "Qi",
                        "suffix": ""
                    },
                    {
                        "first": "Kejiang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Guoqiang",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Xi",
                        "middle": [],
                        "last": "Yang",
                        "suffix": ""
                    },
                    {
                        "first": "Pengyuan",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Weiming",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Neng",
                        "middle": [
                            "H"
                        ],
                        "last": "Yu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Xiao Yu, Yuang Qi, Kejiang Chen, Guoqiang Chen, Xi Yang, Pengyuan Zhu, Weiming Zhang, and Neng H. Yu. 2023. GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance. ArXiv abs/2305.12519 (2023).",
                "links": null
            },
            "BIBREF67": {
                "ref_id": "b67",
                "title": "BERT-CoQAC: BERT-Based Conversational Question Answering in Context",
                "authors": [
                    {
                        "first": "Munazza",
                        "middle": [],
                        "last": "Zaib",
                        "suffix": ""
                    },
                    {
                        "first": "Dai",
                        "middle": [],
                        "last": "Hoang Tran",
                        "suffix": ""
                    },
                    {
                        "first": "Subhash",
                        "middle": [],
                        "last": "Sagar",
                        "suffix": ""
                    },
                    {
                        "first": "Adnan",
                        "middle": [],
                        "last": "Mahmood",
                        "suffix": ""
                    },
                    {
                        "first": "Wei",
                        "middle": [
                            "Emma"
                        ],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Quan",
                        "middle": [
                            "Z"
                        ],
                        "last": "Sheng",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "International Symposium on Parallel Architectures, Algorithms and Programming",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Munazza Zaib, Dai Hoang Tran, Subhash Sagar, Adnan Mahmood, Wei Emma Zhang, and Quan Z. Sheng. 2021. BERT-CoQAC: BERT-Based Conversational Question Answering in Context. In International Symposium on Parallel Archi- tectures, Algorithms and Programming.",
                "links": null
            },
            "BIBREF68": {
                "ref_id": "b68",
                "title": "STaR: Bootstrapping Reasoning With Reasoning",
                "authors": [
                    {
                        "first": "Eric",
                        "middle": [],
                        "last": "Zelikman",
                        "suffix": ""
                    },
                    {
                        "first": "Yuhuai",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Jesse",
                        "middle": [],
                        "last": "Mu",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [],
                        "last": "Goodman",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Advances in Neural Information Processing Systems",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrapping Reasoning With Reasoning. In Advances in Neural Informa- tion Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=_3ELRdg2sgI",
                "links": null
            },
            "BIBREF69": {
                "ref_id": "b69",
                "title": "G3Detector: General GPT-Generated Text Detector",
                "authors": [
                    {
                        "first": "Haolan",
                        "middle": [],
                        "last": "Zhan",
                        "suffix": ""
                    },
                    {
                        "first": "Xuanli",
                        "middle": [],
                        "last": "He",
                        "suffix": ""
                    },
                    {
                        "first": "Qiongkai",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxiang",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Pontus",
                        "middle": [],
                        "last": "Stenetorp",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Haolan Zhan, Xuanli He, Qiongkai Xu, Yuxiang Wu, and Pontus Stenetorp. 2023. G3Detector: General GPT-Generated Text Detector. ArXiv abs/2305.12680 (2023).",
                "links": null
            },
            "BIBREF70": {
                "ref_id": "b70",
                "title": "Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models",
                "authors": [
                    {
                        "first": "Hanlin",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Benjamin",
                        "middle": [
                            "L"
                        ],
                        "last": "Edelman",
                        "suffix": ""
                    },
                    {
                        "first": "Danilo",
                        "middle": [],
                        "last": "Francati",
                        "suffix": ""
                    },
                    {
                        "first": "Daniele",
                        "middle": [],
                        "last": "Venturi",
                        "suffix": ""
                    },
                    {
                        "first": "Giuseppe",
                        "middle": [],
                        "last": "Ateniese",
                        "suffix": ""
                    },
                    {
                        "first": "Boaz",
                        "middle": [],
                        "last": "Barak",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, and Boaz Barak. 2023. Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models. ArXiv abs/2311.04378 (2023).",
                "links": null
            },
            "BIBREF71": {
                "ref_id": "b71",
                "title": "Relying on the Unreliable: The Impact of Language Models",
                "authors": [
                    {
                        "first": "Kaitlyn",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jena",
                        "middle": [
                            "D"
                        ],
                        "last": "Hwang",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    },
                    {
                        "first": "Maarten",
                        "middle": [],
                        "last": "Sap",
                        "suffix": ""
                    }
                ],
                "year": 2024,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, and Maarten Sap. 2024. Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncer- tainty.",
                "links": null
            },
            "BIBREF72": {
                "ref_id": "b72",
                "title": "Fine-Tuning Language Models from Human Preferences",
                "authors": [
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Daniel",
                        "suffix": ""
                    },
                    {
                        "first": "Nisan",
                        "middle": [],
                        "last": "Ziegler",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Stiennon",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [
                            "B"
                        ],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Dario",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Paul",
                        "middle": [],
                        "last": "Amodei",
                        "suffix": ""
                    },
                    {
                        "first": "Geoffrey",
                        "middle": [],
                        "last": "Christiano",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Irving",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daniel M. Ziegler, Nisan Stiennon, Jeff Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019. Fine-Tuning Language Models from Human Preferences. ArXiv abs/1909.08593 (2019). https://api.semanticscholar.org/CorpusID:202660943",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": "1",
                "text": "Figure 1: An overview of responsible AI-generated text study, with an emphasize on detection techniques and their challenges.",
                "type_str": "figure",
                "num": null
            },
            "TABREF0": {
                "text": "AI-generated text detection techniques and their vulnerabilities.",
                "content": "<table><tr><td>Related papers</td><td>Method</td><td>Main Idea</td><td>Vulnerabilities</td></tr><tr><td>[2, 3, 30, 49, 70]</td><td>Supervised detection</td><td>To fine-tune a model on sets of AI and human generated texts.</td><td/></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            },
            "TABREF1": {
                "text": "Interestingly, Chakraborty et al. demonstrate that the AU-ROC curve, as proposed by Sadasivan et al., might be overly conservative for practical detection scenarios. They introduce a hid-",
                "content": "<table><tr><td colspan=\"2\">den possibility by replacing</td><td colspan=\"2\">(M , H ) with</td><td>(M</td><td>, H</td><td>)</td></tr><tr><td colspan=\"2\">in AUROC equation, where</td><td>:=</td><td/><td>\u2022 \u2022 \u2022</td><td>(n times)</td></tr><tr><td colspan=\"6\">denotes the product distribution over sample set S := { }, \u2208</td></tr><tr><td>{1, . . . }, as does \u210e</td><td>. Since</td><td>(M</td><td>, H</td><td/></tr></table>",
                "type_str": "table",
                "html": null,
                "num": null
            }
        }
    }
}