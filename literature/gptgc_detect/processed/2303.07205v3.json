{
    "paper_id": "2303",
    "header": {
        "generated_with": "S2ORC 1.0.0",
        "date_generated": "2024-09-19T13:38:33.139474Z"
    },
    "title": "The Science of Detecting LLM-Generated Texts",
    "authors": [
        {
            "first": "Ruixiang",
            "middle": [],
            "last": "Tang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Rice University Houston",
                "location": {
                    "country": "USA"
                }
            },
            "email": ""
        },
        {
            "first": "Yu-Neng",
            "middle": [],
            "last": "Chuang",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Rice University Houston",
                "location": {
                    "country": "USA"
                }
            },
            "email": "ynchuang@rice.edu"
        },
        {
            "first": "Xia",
            "middle": [],
            "last": "Hu",
            "suffix": "",
            "affiliation": {
                "laboratory": "",
                "institution": "Rice University Houston",
                "location": {
                    "country": "USA"
                }
            },
            "email": "xia.hu@rice.edu"
        }
    ],
    "year": "",
    "venue": null,
    "identifiers": {},
    "abstract": "The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLMgenerated text detection.",
    "pdf_parse": {
        "paper_id": "2303",
        "_pdf_hash": "",
        "abstract": [
            {
                "text": "The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLMgenerated text detection.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Abstract",
                "sec_num": null
            }
        ],
        "body_text": [
            {
                "text": "Recent advancements in natural language generation (NLG) technology have significantly improved the diversity, control, and quality of LLM-generated texts. A notable example is OpenAI's ChatGPT, which demonstrates exceptional performance in tasks such as answering questions, composing emails, essays, and codes. However, this newfound capability to produce human-like texts at high efficiency also raises concerns about detecting and preventing misuse of LLMs in tasks such as phishing, disinformation, and academic dishonesty. For instance, many schools banned ChatGPT due to concerns over cheating in assignments [11] , and media outlets have raised the alarm over fake news generated by LLMs [14] . These concerns about the misuse of LLMs have hindered the NLG application in important domains such as media and education.",
                "cite_spans": [
                    {
                        "start": 616,
                        "end": 620,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    },
                    {
                        "start": 696,
                        "end": 700,
                        "text": "[14]",
                        "ref_id": "BIBREF13"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "The ability to accurately detect LLM-generated texts is critical for realizing the full potential of NLG while minimizing serious consequences. From the perspective of endusers, LLM-generated text detection could increase trust in NLG systems and encourage adoption. For machine learning system developers and researchers, the detector can aid in tracing generated texts and preventing unauthorized use. Given its significance, there has been a growing interest in academia and industry to pursue research on LLM-generated text detection and to deepen our understanding of its underlying mechanisms.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "While there is a rising discussion on whether LLM-generated texts could be properly detected and how this can be done, we provide a comprehensive technical introduction of existing detection methods which can be roughly grouped into two categories: black-box detection and white-box detection. Black-box detection methods are limited to API-level access to LLMs. They rely on collecting text samples from human and machine sources, respectively, to train a classification model that can be used to discriminate between LLM-and human-generated texts. Black-box detectors work well because current LLM-generated texts often show linguistic or statistical patterns. However, as LLMs evolve and improve, black-box methods are becoming less effective. An alternative is white-box detection, in this scenario, the detector has full access to the LLMs and can control the model's generation behavior for traceability purposes. In practice, black-box detectors are commonly constructed by external entities, whereas white-box detection is generally carried out by LLM developers.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "This article is to discuss the timely topic from a data mining and natural language processing perspective. Specifically, we first outline the black-box detection methods in terms of a data analytic life cycle, including data collection, feature selection, and classification model design. We then delve into more recent advancements in white-box detection methods, such as post-hoc watermarks and inference time watermarks. Finally, we present the limitations and concerns of current detection studies and suggest potential future research avenues. We aim to unleash the potential of powerful LLMs by providing fundamental concepts, algorithms, and case studies for detecting LLM-generated texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Introduction",
                "sec_num": "1"
            },
            {
                "text": "Recent advancements in LLMs, such as OpenAI's ChatGPT, have emphasized the potential impacts of this technology on individuals and society. Demonstrated through its performance on challenging tests, such as the MBA exams at Wharton Business School [31] , the capabilities of ChatGPT suggest its potential to provide professional assistance across various disciplines. Specifically in the healthcare domain, the applications of ChatGPT extend far beyond simple enhancements in efficiency. The ChatGPT not only optimizes documentation procedures, facilitating the generation of medical records, progress reports, and discharge summaries, it aids in the collection and analysis of patient data, facilitating medical professionals in making informed decisions regarding patient care. Recent research has also indicated the potential of LLMs in generating synthetic data for the healthcare field, thereby potentially addressing common privacy issues during data collection. LLMs' influence is also felt in the legal sector, where they are reshaping traditional practices such as contract generation and litigation procedures. The improved efficiency and effectiveness of these models are changing how we do things across a multitude of domains. As a result, when we talk about LLMs, we're not just measuring their technical competency, but also looking at their broader societal and professional implications.",
                "cite_spans": [
                    {
                        "start": 248,
                        "end": 252,
                        "text": "[31]",
                        "ref_id": "BIBREF30"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prevalence and Impact",
                "sec_num": "2"
            },
            {
                "text": "The introduction of LLMs in education has elicited huge concerns. While convenient, their potential to provide quick answers threatens to undermine the development of critical thinking and problem-solving skills, which are essential for academic and life-long success. Further, there's the issue of academic honesty, as students might be tempted to use these tools inappropriately. In response, New York City Public Schools have prohibited the use of ChatGPT [11] . While the impact of LLMs on education is significant, it is imperative to extend this discourse to other domains. For instance, in journalism, the emergence of AI-generated \"deepfake\" news articles can threaten the credibility of news outlets and misinform the public. In the legal sector, the potential misuse of LLMs could have repercussions on the justice system, from contract generation to litigation processes. In cybersecurity, LLMs could be weaponized to create more convincing phishing emails or social engineering attacks.",
                "cite_spans": [
                    {
                        "start": 459,
                        "end": 463,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prevalence and Impact",
                "sec_num": "2"
            },
            {
                "text": "In an attempt to mitigate the potential misuse of LLMs, detection systems for LLM-generated text are emerging as a significant countermeasure. These systems offer the capability to differentiate AI-generated content from humanauthored text, thereby playing a pivotal role in preserving the integrity of various domains. In the realm of academia, such tools can facilitate the identification of academic misconduct. Within the field of journalism, these systems may assist in separating legitimate news from AI-generated misinformation. Furthermore, in cybersecurity, they have the potential to strengthen spam filters to better identify and flag AI-aided threats. A recent incident at Texas A&M University underscores the urgent need for effective LLM detection tools [10] . An instructor suspected students of using ChatGPT to complete their final assignments. Lacking a detection tool, the instructor resorted to pasting the student's responses into ChatGPT, asking the ChatGPT if it had generated the text. This ad-hoc method sparked substantial debate online, illustrating the pressing need for more sophisticated and reliable ways to detect LLM-generated content. While the current detection tools may not be flawless, they nonetheless symbolize a proactive effort to maintain ethical standards in the face of rapid AI advancements. The surge of interest in research focused on LLM-generated text detection testifies to the importance of these tools in mitigating the societal impact of LLMs. As such, we must conduct more extensive discussions on the detection of LLM-generated text. Particularly, we must explore its potential to safeguard the integrity of various domains against the risks posed by LLM misuse.",
                "cite_spans": [
                    {
                        "start": 768,
                        "end": 772,
                        "text": "[10]",
                        "ref_id": "BIBREF9"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Prevalence and Impact",
                "sec_num": "2"
            },
            {
                "text": "In the domain of black-box detection, external entities are restricted to API-level access to the LLM, as depicted in Figure 1 . To develop a proficient detector, black-box approaches necessitate gathering text samples originating from both human and machine-generated sources. Subsequently, a classifier is then designed to distinguish between the two categories by identifying and leveraging relevant features. We highlight the three essential components of black-box text detection: data acquisition, feature selection, and the execution of the classification model.",
                "cite_spans": [],
                "ref_spans": [
                    {
                        "start": 125,
                        "end": 126,
                        "text": "1",
                        "ref_id": "FIGREF0"
                    }
                ],
                "eq_spans": [],
                "section": "Black-box Detection",
                "sec_num": "3"
            },
            {
                "text": "The effectiveness of black-box detection models is heavily dependent on the quality and diversity of the acquired data. Recently, a growing body of research has concentrated on amassing responses generated by LLMs and comparing them with human-composed texts spanning a wide range of domains. This section delves into the various strategies for obtaining data from both human and machine sources. [17] , the word ranking is obtained from the GPT-2 small model. Words that rank within the top 10 are highlighted in green, top 100 in yellow, top 1,000 in red, and the rest in purple. There is a notable difference between the two texts. The human-authored texts are from Chalkbeat New York [11] .",
                "cite_spans": [
                    {
                        "start": 397,
                        "end": 401,
                        "text": "[17]",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 688,
                        "end": 692,
                        "text": "[11]",
                        "ref_id": "BIBREF10"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "based on the preceding words. Recent advancements in natural language generation have led to the development of LLMs for various domains, including question-answering, news generation, and story creation. Prior to acquiring LMgenerated texts, it is essential to delineate target domains and generation models. Typically, a detection model is constructed to recognize text generated from a specific LM across multiple domains. In order to enhance detection generalizability, the minimax strategy suggests that detectors should minimize worst-case performance, which entails enhancing detection capabilities for the most challenging instances where the quality of LM-generated text closely resembles human-authored content [27] .",
                "cite_spans": [
                    {
                        "start": 721,
                        "end": 725,
                        "text": "[27]",
                        "ref_id": "BIBREF26"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "Generating high-quality text in a particular domain can be achieved by fine-tuning LMs on task-related data, which substantially improves the quality of the generated texts. For example, Solaiman et al. fine-tuned the GPT-2 model on Amazon product reviews, producing reviews with a style consistent with those found on Amazon [34] . Moreover, LMs are known to produce artifacts such as repetitiveness, which can negatively impact the generalizability of the detection model. To mitigate these artifacts, researchers can provide domain-specific prompts or constraints before generating outputs. For instance, Clark et al. [6] randomly selected 50 articles from Newspaper3k to use as prompts for the GPT-3 model for news generation and applied filtering constraints on the models with the phrase \"Once upon a time\" for story creation. The token sampling strategy also significantly influences the generated text quality and style. While deterministic greedy algorithms like beam search [35] generate the most probable sequence, they may restrict creativity and language diversity. Conversely, stochastic algorithms such as nucleus sampling [19] maintain a degree of randomness while excluding inferior candidates, making them more suitable for a free-form generation. In conclusion, it is crucial for researchers to carefully consider the target domain, generation models, and sampling strategies when collecting LM-generated text to ensure the production of high-quality, diverse, and domain-appropriate content.",
                "cite_spans": [
                    {
                        "start": 326,
                        "end": 330,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 621,
                        "end": 624,
                        "text": "[6]",
                        "ref_id": "BIBREF5"
                    },
                    {
                        "start": 984,
                        "end": 988,
                        "text": "[35]",
                        "ref_id": "BIBREF34"
                    },
                    {
                        "start": 1138,
                        "end": 1142,
                        "text": "[19]",
                        "ref_id": "BIBREF18"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "3.1.2 Human-Authored Data. Manual composition by humans serves as a natural method for obtaining authentic, human-authored data. For example, in a study conducted by Dugan et al. [8] , the authors sought to evaluate the quality of natural language generation systems and gauge human perceptions of the generated texts. To achieve this, they employed 200 Amazon Mechanical Turk workers to complete 10 annotations on a website, accompanied by a natural language rationale for their choices. However, manually collecting data through human effort can be both time-consuming and financially impractical for larger datasets. An alternative strategy involves extracting text directly from humanauthored sources, such as websites and scholarly articles. For instance, we can readily amass thousands of descriptions of computer science concepts from Wikipedia, penned by knowledgeable human experts [18] . Moreover, numerous publicly accessible benchmark datasets, like ELI5 [13] , which comprises 270K threads from the Reddit forum \"Explain Like I'm Five\", already offer human-authored texts in an organized format. Utilizing these readily available sources can considerably decrease the time and expense involved in collecting human-authored texts. Nevertheless, it is essential to address potential sampling biases and ensure topic diversity by including texts from various groups of people, as well as non-native speakers.",
                "cite_spans": [
                    {
                        "start": 179,
                        "end": 182,
                        "text": "[8]",
                        "ref_id": "BIBREF7"
                    },
                    {
                        "start": 891,
                        "end": 895,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 967,
                        "end": 971,
                        "text": "[13]",
                        "ref_id": "BIBREF12"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Data Acquisition",
                "sec_num": "3.1"
            },
            {
                "text": "Prior research has offered valuable perspectives on differentiating LLM-generated texts from human-authored texts through human evaluations. Initial observations indicate that LLM-generated texts are less emotional and objective compared to humanauthored text, which often uses punctuation and grammar to convey subjective feelings [18] . For example, human authors frequently use exclamation marks, question marks, and ellipsis to express their emotions, while LLMs generate answers that are more formal and structured. However, it is crucial to acknowledge that LLM-generated texts may not always be accurate or beneficial, as they can contain fabricated information [18, 33] . At the sentence level, research has shown that human-authored texts are more coherent than LLM-generated text, which tends to repeat terms within a paragraph [9, 28] . These observations suggest that LLMs may leave some distinctive signals in their generated text, allowing for the selection of suitable features to distinguish between LLM and human-authored texts.",
                "cite_spans": [
                    {
                        "start": 332,
                        "end": 336,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 669,
                        "end": 673,
                        "text": "[18,",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 674,
                        "end": 677,
                        "text": "33]",
                        "ref_id": "BIBREF32"
                    },
                    {
                        "start": 838,
                        "end": 841,
                        "text": "[9,",
                        "ref_id": "BIBREF8"
                    },
                    {
                        "start": 842,
                        "end": 845,
                        "text": "28]",
                        "ref_id": "BIBREF27"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Human Evaluation Findings:",
                "sec_num": "3.1.3"
            },
            {
                "text": "How can we discern between LM-generated texts and humanauthored texts? This section will discuss possible detection features from multiple angles, including statistical disparities, linguistic patterns, and fact verification.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Detection Feature Selection",
                "sec_num": "3.2"
            },
            {
                "text": "The detection of statistical disparities between LLM-generated and human-authored texts can be accomplished by employing various statistical metrics. For example, the Zipfian coefficient measures the text's conformity to an exponential curve, which is described by Zipf's law [29] . A visualization tool, GLTR [17] , has been developed to detect generation artifacts across prevalent sampling methods, as demonstrated in Figure 2 . The underlying assumption is that most systems sample from the head of the distribution, thus word ranking information of the language model can be used to distinguish LLM-generated text. Perplexity serves as another widely used metric for LLMgenerated text detection. It measures the degree of uncertainty or surprise in predicting the next word in a sequence, based on the preceding words, by calculating the negative average log-likelihood of the texts under the language model [5] . Research indicates that language models tend to concentrate on common patterns in the texts they were trained on, leading to low perplexity scores for LLM-generated text. In contrast, human authors possess the capacity to express themselves in a wide range of styles, making prediction more difficult for language models and resulting in higher perplexity values for human-authored texts. However, it is important to recognize that these statistical disparities are constrained by the necessity for document-level text, which inevitably diminishes the detection resolution, as depicted in Figure 3 .",
                "cite_spans": [
                    {
                        "start": 276,
                        "end": 280,
                        "text": "[29]",
                        "ref_id": "BIBREF28"
                    },
                    {
                        "start": 310,
                        "end": 314,
                        "text": "[17]",
                        "ref_id": "BIBREF16"
                    },
                    {
                        "start": 913,
                        "end": 916,
                        "text": "[5]",
                        "ref_id": "BIBREF4"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 428,
                        "end": 429,
                        "text": "2",
                        "ref_id": null
                    },
                    {
                        "start": 1515,
                        "end": 1516,
                        "text": "3",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Statistical Disparities.",
                "sec_num": "3.2.1"
            },
            {
                "text": "Various contextual properties can be employed to analyze linguistic patterns in human and LLM-generated texts, such as vocabulary features, part-ofspeech, dependency parsing, sentiment analysis, and stylistic features. The vocabulary features offer insight into the queried text's word usage patterns by analyzing characteristics such as average word length, vocabulary size, and word density. Previous studies on ChatGPT have shown that human-authored texts tend to have a more diverse vocabulary but shorter length [18] . Part-of-speech analysis emphasizes the dominance of nouns in ChatGPT texts, implying argumentativeness and objectivity, while the dependency parsing analysis shows that ChatGPT texts use more determiners, conjunctions, and auxiliary relations [18] . Sentiment analysis, on the other hand, provides a measure of the emotional tone and mood expressed in the text. Unlike humans, large language models tend to be neutral by default and lack emotional expression. Research has shown that Chat-GPT expresses significantly less negative emotion and hate speech compared to human-authored texts. Stylistic features or stylometry, including repetitiveness, lack of purpose, and readability, are also known to harbor valuable signals for detecting LLM-generated texts [15] . In addition to analyzing single texts, numerous linguistic patterns can be found in multi-turn conversations [3] . These linguistic patterns are a reflection of the training data and strategies of LLMs and serve as valuable features for detecting LLM-generated text.",
                "cite_spans": [
                    {
                        "start": 517,
                        "end": 521,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 767,
                        "end": 771,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 1283,
                        "end": 1287,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 1399,
                        "end": 1402,
                        "text": "[3]",
                        "ref_id": "BIBREF2"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistic Patterns.",
                "sec_num": "3.2.2"
            },
            {
                "text": "However, it is important to note that LLMs can substantially alter their linguistic patterns in response to prompts. For instance, incorporating a prompt like \"Please respond with humor\" can change the sentiment and style of the LLM's response, impacting the robustness of linguistic patterns.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Linguistic Patterns.",
                "sec_num": "3.2.2"
            },
            {
                "text": "Large Language models often rely on likelihood maximization objectives during training, which can result in the generation of nonsensical or inconsistent text, a phenomenon known as hallucination. This emphasizes the significance of fact-verification as a crucial feature for detection [40] . For instance, OpenAI's ChatGPT has been reported to generate false scientific abstracts and post misleading news opinions. Studies also revealed that popular decoding methods, such as top-k and nucleus sampling, resulted in more diverse and less repetitive generations. However, they also produce texts that are less verifiable [25] . These findings underscore the potential for using fact verification to detect LLM-generated texts.",
                "cite_spans": [
                    {
                        "start": 286,
                        "end": 290,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 621,
                        "end": 625,
                        "text": "[25]",
                        "ref_id": "BIBREF24"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fact Verification.",
                "sec_num": "3.2.3"
            },
            {
                "text": "Prior research has advanced the development of tools and algorithms for conducting fact verification, which entails retrieving evidence for claims, evaluating consistency and relevance, and detecting inconsistencies in texts. One strategy employs sentence-level evidence, such as extracting facts from Wikipedia, to directly verify facts of a sentence [25] . Another approach involves analyzing document-level evidence via graph structures, which capture the factual structure of the document as an entity graph. This graph is utilized to learn sentence representations with a graph neural network, followed by the composition of sentence representations into a document representation for fact verification [40] . Some studies also use knowledge graphs constructed from truth sources, such as Wikipedia, to conduct fact verification [33] . These methods evaluate consistency by querying subgraphs and identify non-factual information by iterating through entities and relations. Given that human-authored texts may also contain misinformation, it is vital to supplement the detection results with other features in order to accurately distinguish texts generated by LLMs.",
                "cite_spans": [
                    {
                        "start": 352,
                        "end": 356,
                        "text": "[25]",
                        "ref_id": "BIBREF24"
                    },
                    {
                        "start": 708,
                        "end": 712,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    },
                    {
                        "start": 834,
                        "end": 838,
                        "text": "[33]",
                        "ref_id": "BIBREF32"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Fact Verification.",
                "sec_num": "3.2.3"
            },
            {
                "text": "The detection task is typically approached as a binary classification problem, aiming to capture textual features that differentiate between human-authored and LLM-generated texts. This section provides an overview of the primary categories of classification models. GPT-2, GPT-3, and Grover models [15] . Similarly, Solaiman et al. achieved solid performance in identifying texts generated by GPT-2 through a combination of TF-IDF unigram and bigram features with a logistic regression model [34] . In addition, studies have also shown that using pre-trained language models to extract semantic textual features, followed by SVM for classification, can outperform the use of statistical features alone [7] . One advantage of these algorithms is their interpretability, allowing researchers to analyze the importance of input features and understand why the model classifies texts as LLM-generated or not.",
                "cite_spans": [
                    {
                        "start": 299,
                        "end": 303,
                        "text": "[15]",
                        "ref_id": "BIBREF14"
                    },
                    {
                        "start": 493,
                        "end": 497,
                        "text": "[34]",
                        "ref_id": "BIBREF33"
                    },
                    {
                        "start": 703,
                        "end": 706,
                        "text": "[7]",
                        "ref_id": "BIBREF6"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Classification Model",
                "sec_num": "3.3"
            },
            {
                "text": "Approaches. In addition to employing explicitly extracted features for detection, recent studies have explored leveraging language models, such as RoBERTa [24] , as a foundation. This approach entails fine-tuning these language models on a mixture of humanauthored and LLM-generated texts, enabling the implicit capture of textual distinctions. The majority of studies adopt the supervised learning paradigm for training the language model, as demonstrated by Ippolito et al. [20] , who fine-tuned the BERT model on a curated dataset of generated-text pairs. Their study revealed that human raters have significantly lower accuracy than automatic discriminators in identifying LLM-generated text. In a low-resource scenario, Rodriguez et al. [30] showed that a few hundred labeled in-domain authentic and synthetic texts suffice for robust performance, even without complete information about the LLM text generation pipeline. Despite the strong performance under the supervised learning paradigms, obtaining annotations for detection data can be challenging in real-world applications, leading the supervised paradigms inapplicable in some cases. Recent research [16] addresses this issue by detecting LLMgenerated documents using repeated higher-order n-grams, which can be trained under unsupervised learning paradigms without requiring LLM-generated datasets as training data. Besides using the language model as the backbone, recent research finds that contextual structure can be viewed as a graph containing entities mentioned in the texts and the semantically relevant relations, which utilizes a deep graph neural network to capture the structure feature of a document for LLM-generated news detection [40] . While deep learning approaches often yield superior detection outcomes, their black-box nature severely restricts interpretability. Consequently, researchers typically rely on interpretation tools to comprehend the rationale behind the model's decisions.",
                "cite_spans": [
                    {
                        "start": 155,
                        "end": 159,
                        "text": "[24]",
                        "ref_id": "BIBREF23"
                    },
                    {
                        "start": 476,
                        "end": 480,
                        "text": "[20]",
                        "ref_id": "BIBREF19"
                    },
                    {
                        "start": 742,
                        "end": 746,
                        "text": "[30]",
                        "ref_id": "BIBREF29"
                    },
                    {
                        "start": 1164,
                        "end": 1168,
                        "text": "[16]",
                        "ref_id": "BIBREF15"
                    },
                    {
                        "start": 1711,
                        "end": 1715,
                        "text": "[40]",
                        "ref_id": "BIBREF39"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Deep Learning",
                "sec_num": "3.3.2"
            },
            {
                "text": "In white-box detection, the detector processes complete access to the target language model, facilitating the integration of concealed watermarks into its outputs to monitor suspicious or unauthorized activities. In this section, we initially outline the three prerequisites for watermarks in NLG. Subsequently, we provide a synopsis of the two primary classifications of white-box watermarking strategies: post-hoc watermarking and inference-time watermarking.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "White-box Detection",
                "sec_num": "4"
            },
            {
                "text": "Building on prior research in traditional digital watermarking, we put forth three crucial requirements for NLG watermarking (1) Effectiveness: The watermark must be effectively embedded into the generated texts and verifiable while preserving the quality of the generated texts. (2) Secrecy: To achieve stealthiness, the watermark should be designed without introducing conspicuous alterations that could be readily detected by automated classifiers. Ideally, it should be indistinguishable from non-watermarked texts. (3) Robustness: The watermark ought to be resilient and difficult to remove through common modifications such as synonym substitution. To eliminate the watermark, attackers would need to implement significant modifications that render the texts unusable. These three requirements form the bedrock for NLG watermarking and guarantee the traceability of LLMgenerated texts.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Watermarking Requirements",
                "sec_num": "4.1"
            },
            {
                "text": "Given an LLM-generated text, post-hoc watermarks will embed a hidden message or identifier into the text. Verification of the watermark can be performed by recovering the hidden message from the suspicious text. There are two main categories of post-hoc watermarking methods: rule-based and neural-based approaches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Post-hoc Watermarking",
                "sec_num": "4.2"
            },
            {
                "text": "Approaches. Initially, nature language researchers adapted techniques from multimedia watermarking, which were non-linguistic in nature and relied heavily on character changes. For example, the line-shift watermark method involves moving a line of text upward or downward (or left or right) based on the binary signal (watermark) to be inserted [4] . However, these \"printed text\" watermarking approaches had limited applicability and were not robust against text reformatting. Later research shifted towards using the syntactic structure for watermarking. A study by Atallah et al. [2] embedded watermarks in parsed syntactic A random seed is generated by hashing the previously predicted token \"a\", splitting the whole vocabulary into \"green list\" and \"red list\". The next token \"carpet\" is chosen from the green list. tree structures, preserving the meaning of the original texts and rendering watermarks indecipherable to those without knowledge of the modified tree structure. Additionally, syntactic tree structures are difficult to remove through editing and remain effective when the text is translated into other languages. Further improvements were made in a series of works, which proposed variants of the method that embedded watermarks based on synonym tables instead of just parse trees [21] . Along with syntactic structure, researchers have also leveraged the semantic structure of text to embed watermarks. This includes exploiting features such as verbs, nouns, prepositions, spelling, acronyms, grammar rules, etc. For instance, a synonym substitution approach was proposed in which watermarks are embedded by replacing certain words with their synonyms without altering the context of the text [36] . Generally, rule-based methods utilize fixed rulebased substitutions, which may systematically change the text statistics, compromising the watermark's secrecy and allowing adversaries to detect and remove the watermark.",
                "cite_spans": [
                    {
                        "start": 345,
                        "end": 348,
                        "text": "[4]",
                        "ref_id": "BIBREF3"
                    },
                    {
                        "start": 583,
                        "end": 586,
                        "text": "[2]",
                        "ref_id": "BIBREF1"
                    },
                    {
                        "start": 1301,
                        "end": 1305,
                        "text": "[21]",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 1714,
                        "end": 1718,
                        "text": "[36]",
                        "ref_id": "BIBREF35"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Rule-based",
                "sec_num": "4.2.1"
            },
            {
                "text": "In contrast to the rulebased methods that demand significant engineering efforts for design, neural-based approaches envision the informationhiding process as an end-to-end learning process. Typically, these approaches involve three components: a watermark encoder network, a watermark decoder network, and a discriminator network [1] . Given a target text and a secret message (e.g., random binary bits), the watermark encoder network generates a modified text that incorporates the secret message. The watermark decoder network then endeavors to retrieve the secret message from the modified text. One challenge is that the watermark encoder network may significantly alter the language statistics. To address this problem, the framework employs an adversarial training strategy and includes the discriminator network. The discriminator network takes the target texts and watermarked texts as input and aims to differentiate between them, while the watermark encoder network aims to make them indistinguishable. The training process continues until the three components achieve a satisfactory level of performance. For watermarking LLM-generated text, developers can use the watermark encoder network to embed a pre-set secret message into LLMs' outputs, and the watermark decoder network to verify suspicious texts. Although neural-based approaches eliminate the need for manual rule design, their inherent lack of interpretability raises concerns regarding their truthfulness and the absence of mathematical guarantees for the watermark's effectiveness, secrecy, and robustness.",
                "cite_spans": [
                    {
                        "start": 331,
                        "end": 334,
                        "text": "[1]",
                        "ref_id": "BIBREF0"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Neural-based Approaches.",
                "sec_num": "4.2.2"
            },
            {
                "text": "Inference-time watermarking targets the LLM decoding process, as opposed to post-hoc watermarks, which are applied after text generation. The language model produces a probability distribution for the subsequent word in a sequence based on preceding words. A decoding strategy, which is an algorithm that chooses words from this distribution to create a sequence, offers an opportunity to embed the watermark by modifying the word selection process.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference Time Watermark",
                "sec_num": "4.3"
            },
            {
                "text": "A representative example of this method can be found in research conducted by Kirchenbauer et al. [23] . During the next token generation, a hash code is generated based on the previously generated token, which is then used to seed a random number generator. This seed randomly divides the whole vocabulary into a \"green list\" and a \"red list\" of equal size. The next token is subsequently generated from the green list. In this way, the watermark is embedded into every generated word, as depicted in Figure . 4. To detect the watermark, a third party with knowledge of the hash function and random number generator can reproduce the red list for each token and count the number of violations of the red list rule, thus verifying the authenticity of the text. The probability that a natural source produces \ud835\udc41 tokens without violating the red list rule is only 1/2 \ud835\udc41 , which is vanishingly small even for text fragments with a few dozen words. To remove the watermark, adversaries need to modify at least half of the document's tokens. However, one concern with these inference-time watermarks is that the controlled sampling process may significantly impact the quality of the generated text. One solution is to relax the watermarking constraints, e.g., increasing the green list vocabulary size, and aim for a balance between watermarking and text quality.",
                "cite_spans": [
                    {
                        "start": 98,
                        "end": 102,
                        "text": "[23]",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Inference Time Watermark",
                "sec_num": "4.3"
            },
            {
                "text": "This section introduces several noteworthy benchmarking datasets for detecting LLM-generated texts. One prominent example is the work by Guo et al. [18] , where the authors constructed the dataset Human ChatGPT Comparison Corpus (HC3), specifically designed to distinguish text generated by ChatGPT in question-answering tasks in both English and Chinese languages. The dataset comprises 37,175 questions across diverse domains, such as open-domain, computer science, finance, medicine, law, and psychology. These questions and corresponding human answers were sourced from publicly available question-answering datasets and wiki text. The researchers obtained responses generated by ChatGPT by presenting the same questions to the model and collecting its outputs. The evaluation results demonstrated that a RoBERTa-based detector achieved the highest performance, with F1 scores of 99.79% for paragraph-level detection and 98.43% for sentence-level detection of English text. Table 1 presents more representative public benchmarking datasets for detecting different LLMs across various domains.",
                "cite_spans": [
                    {
                        "start": 148,
                        "end": 152,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 984,
                        "end": 985,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Benchmarking Datasets",
                "sec_num": "5"
            },
            {
                "text": "Despite numerous LLM-generated text detection studies, there is no comprehensive benchmarking dataset for performance comparison. This gap arises from the divergence in detection targets across different studies, focusing on distinct LLMs in various domains like news, question-answering, coding, and storytelling. Moreover, the rapid evolution of LLMs must be acknowledged. These models are being developed and released at an accelerating pace, with new models emerging monthly. As a result, it is increasingly challenging for researchers to keep up with these advancements and create datasets that accurately reflect each new model. Thus, the ongoing challenge in the field lies in establishing comprehensive and adaptable benchmarking datasets to accommodate the rapid influx of new LLMs.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Benchmarking Datasets",
                "sec_num": "5"
            },
            {
                "text": "LLMs Domain HC3 [18] ChatGPT Question Answering Neural Fake News [39] Grover News TweepFake [12] GPT2 Tweets GPT2-Output [26] GPT2 WebText TURINGBENCH [37] GPT1,2,3 News Table 1 . Representative Benchmarking Datasets",
                "cite_spans": [
                    {
                        "start": 16,
                        "end": 20,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    },
                    {
                        "start": 65,
                        "end": 69,
                        "text": "[39]",
                        "ref_id": "BIBREF38"
                    },
                    {
                        "start": 92,
                        "end": 96,
                        "text": "[12]",
                        "ref_id": "BIBREF11"
                    },
                    {
                        "start": 121,
                        "end": 125,
                        "text": "[26]",
                        "ref_id": "BIBREF25"
                    },
                    {
                        "start": 151,
                        "end": 155,
                        "text": "[37]",
                        "ref_id": "BIBREF36"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 176,
                        "end": 177,
                        "text": "1",
                        "ref_id": null
                    }
                ],
                "eq_spans": [],
                "section": "Dataset",
                "sec_num": null
            },
            {
                "text": "The previous section delineated both white-box and blackbox detection methodologies. This section pivots towards adversarial perspectives, delving into potential adaptive attack strategies capable of breaching existing detection approaches. A representative work is from Sadasivan et al. [32] . The authors empirically demonstrated that a paraphrasing attack could break a wide array of detectors, including both white-box and black-box approaches. This attack is premised on a straightforward yet potent assumption: given a sentence \ud835\udc60, we denote \ud835\udc43 (\ud835\udc60) as the set of all paraphrased sentences that have similar meanings to \ud835\udc60. Furthermore, let \ud835\udc3f(\ud835\udc60) represent the set of sentences that the LLM could generate with a similar meaning to \ud835\udc60. We assume that a detector can only identify sentences within \ud835\udc3f(\ud835\udc60). If the size of \ud835\udc3f(\ud835\udc60) is much smaller than \ud835\udc43 (\ud835\udc60), i.e., |\ud835\udc3f(\ud835\udc60)| \u226a |\ud835\udc43 (\ud835\udc60)|, attackers can randomly sample a sentence from \ud835\udc43 (\ud835\udc60) to evade the detector with a high probability. Based on this assumption, the author utilizes a different language model to paraphrase the output of the LLM, thereby simulating the sampling process from \ud835\udc43 (\ud835\udc60), as depicted in Figure 5 . The empirical result shows that the paraphrasing attack is effective for the inference time watermark attack [23] . Utilizing the PEGASUS-based paraphrasing, the author succeeded in reducing the green list tokens from 58% to 44%. As a result, the detector accuracy drops from 97% to 80%. The attack also adversely affected black-box detection methods, causing the true positive rate of OpenAI's RoBERTa-Large-Detector to decline from 100% to roughly 80% with a practical false positive rate of 1%. Future research will inevitably encounter a proliferation of attack strategies designed to dupe detection systems. Developing robust detection systems capable of withstanding such potential attacks poses a formidable challenge to researchers.",
                "cite_spans": [
                    {
                        "start": 288,
                        "end": 292,
                        "text": "[32]",
                        "ref_id": null
                    },
                    {
                        "start": 1271,
                        "end": 1275,
                        "text": "[23]",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [
                    {
                        "start": 1158,
                        "end": 1159,
                        "text": "5",
                        "ref_id": "FIGREF4"
                    }
                ],
                "eq_spans": [],
                "section": "Adaptive Attacks for Detection Systems",
                "sec_num": "6"
            },
            {
                "text": "7 Authors' Concerns",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Adaptive Attacks for Detection Systems",
                "sec_num": "6"
            },
            {
                "text": "Bias in Collected Datasets. Data collection plays a vital role in the development of black-box detectors, as these systems rely on the data they are trained on to learn how to identify detection signals. However, it is important to note that the data collection process can introduce biases that can negatively impact the performance and generalization of the detector [28] . These biases can take several forms. For example, many existing studies tend to focus on only one or a few specific tasks, such as question-answering or news generation, which can lead to an imbalanced distribution of topics in the data and limit the detector's ability to generalize. Additionally, human artifacts can easily be introduced during data acquisition, as seen in the study conducted by Guo et al. [18] , where the lack of style instruction in collecting LLM-generated answers led to ChatGPT producing answers with a neutral sentiment. These spurious correlations can be captured and even amplified by the detector, leading to poor generalization performance in real-world applications. Confidence Calibration. In the development of real-world detection systems, it's crucial not only to have accurate classifications but also to provide an indication of the likelihood of being incorrect. For instance, a text with a 98% probability of being generated by an LLM should be considered more likely to be machine-generated than one with a 90% probability. In other words, the predicted class probabilities should reflect its ground truth correctness likelihood. Accurate confidence scores are of paramount importance for assessing system trustworthiness, as they offer valuable information for users to establish trust in the system, particularly for neural networks whose decisions can be challenging to interpret. Although neural networks exhibit greater accuracy than traditional classification models, research on confidence score accuracy in LLM-generated text detection topics remains scarce. Therefore, it is essential to calibrate the confidence scores for black-box detection classifiers, which frequently employ neural-based models.",
                "cite_spans": [
                    {
                        "start": 369,
                        "end": 373,
                        "text": "[28]",
                        "ref_id": "BIBREF27"
                    },
                    {
                        "start": 786,
                        "end": 790,
                        "text": "[18]",
                        "ref_id": "BIBREF17"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of Black-box Detection",
                "sec_num": "7.1"
            },
            {
                "text": "In our opinion, while black-box detection works at present due to detectable signals left by language models in generated text, it will gradually become less viable as language model capabilities advance and ultimately become infeasible. In light of the rapid improvement in LLM-generated text quality, the future of reliable detection tools lies in white-box watermarking detection approaches.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of Black-box Detection",
                "sec_num": "7.1"
            },
            {
                "text": "The limitations of the white-box detection method primarily stem from two perspectives. Firstly, there exists a trade-off between the effectiveness of the watermark and the quality of the text, which applies to both post-hoc watermarks [1, 21] and inference time watermarks [32] . Achieving a more reliable watermark often requires significant modifications to the original text, potentially compromising its quality. The challenge lies in optimizing the trade-off between watermark effectiveness and text quality to identify an optimal balance. Moreover, as users accumulate a growing volume of watermarked text, there is a potential for adversaries to detect and reverse engineer the watermark. Sadasivan et al. [32] emphasized that when attackers query the LLMs, they can gain knowledge about the green list words proposed in the inference time watermark [23] . This enables them to generate text that fools the detection system. To address this issue, it is essential to quantify the detectability of the watermark and explore its robustness under different query budgets. These avenues present promising directions for future research.",
                "cite_spans": [
                    {
                        "start": 236,
                        "end": 239,
                        "text": "[1,",
                        "ref_id": "BIBREF0"
                    },
                    {
                        "start": 240,
                        "end": 243,
                        "text": "21]",
                        "ref_id": "BIBREF20"
                    },
                    {
                        "start": 274,
                        "end": 278,
                        "text": "[32]",
                        "ref_id": null
                    },
                    {
                        "start": 714,
                        "end": 718,
                        "text": "[32]",
                        "ref_id": null
                    },
                    {
                        "start": 858,
                        "end": 862,
                        "text": "[23]",
                        "ref_id": "BIBREF22"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Limitations of White-box Detection",
                "sec_num": "7.2"
            },
            {
                "text": "Existing studies often rely on metrics such as AUC or accuracy for evaluating detection performance. However, these metrics only consider an average case and are not enough for security analysis. Consider comparing two detectors: Detector A perfectly identify of 1% of the LLM-generated texts but succeeds with a random 50% chance on the rest. Detector B succeeds with 50.5% on all data. On average, two detectors have the same detection accuracy or AUC. However, detector A demonstrates exceptional potency, while detector B is practically ineffective. In order to know if the detector can reliably identify the LLM-generated text, researchers need to consider the low false-positive rate regime (FPR) and report a detector's True-Positive Rate (TPR) at a low false-positive rate. This objective of designing methods around low falsepositive regimes is widely used in the computer security domain [22] . This is especially crucial for populations who produce unusual text, such as non-native speakers. Such populations might be especially at risk for false-positive, which could lead to serious consequences if these detectors are used in our education systems.",
                "cite_spans": [
                    {
                        "start": 898,
                        "end": 902,
                        "text": "[22]",
                        "ref_id": "BIBREF21"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Lacking Comprehensive Evaluation Metrics",
                "sec_num": "7.3"
            },
            {
                "text": "Current detection methods are based on the assumption that the LLM is controlled by the developers and offered as a service to end-users, this one-to-many relationship is conducive to detection purposes. However, challenges arise when developers open-source their models or when hackers steal them. For instance, Meta's latest LLM, LLaMA, was initially accessible by request. But just a week after accepting access requests, it was leaked online via a 4chan torrent, raising concerns about the potential surge in personalized spam and phishing attempts [38] . Once the end user gets full access to the LLM, the ability to modify the LLMs' behavior hinders black-box detection from identifying generalized language signals. Embedding watermarks in LLM outputs is one potential solution, as developers can integrate concealed watermarks into LLM outputs before making the models open-source. However, it can still be defeated as users have full access to the model and can fine-tune it or change sampling strategies to erase existing watermarks. Furthermore, it is challenging to impose the requirement of embedding a traceable watermark on all open-source LLMs.",
                "cite_spans": [
                    {
                        "start": 553,
                        "end": 557,
                        "text": "[38]",
                        "ref_id": "BIBREF37"
                    }
                ],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Threats from Open-Source LLMs",
                "sec_num": "7.4"
            },
            {
                "text": "With a growing number of companies and developers engaging in open-source large language model projects, a future possibility emerges wherein individuals can own and customize their own language models. In this scenario, the detection of open-source LLMs becomes increasingly complex and challenging. As a result, the threat emanating from open-source LLMs demands careful consideration, given its potentially wide-ranging implications for the security and integrity of various sectors.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Threats from Open-Source LLMs",
                "sec_num": "7.4"
            },
            {
                "text": "The detection of LLM-generated texts is an expanding and dynamic field, with numerous newly developed techniques emerging continuously. This survey provides a precise categorization and in-depth examination of existing approaches to help the research community comprehend the strengths and limitations of each method. Despite the rapid advancements in LLM-generated text detection, significant challenges still need to be addressed. Further progress in this field will require developing innovative solutions to overcome these challenges.",
                "cite_spans": [],
                "ref_spans": [],
                "eq_spans": [],
                "section": "Conclusion",
                "sec_num": "8"
            }
        ],
        "back_matter": [],
        "bib_entries": {
            "BIBREF0": {
                "ref_id": "b0",
                "title": "Adversarial watermarking transformer: Towards tracing text provenance with data hiding",
                "authors": [
                    {
                        "first": "Sahar",
                        "middle": [],
                        "last": "Abdelnabi",
                        "suffix": ""
                    },
                    {
                        "first": "Mario",
                        "middle": [],
                        "last": "Fritz",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "2021 IEEE Symposium on Security and Privacy (SP)",
                "volume": "",
                "issue": "",
                "pages": "121--140",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sahar Abdelnabi and Mario Fritz. 2021. Adversarial watermarking transformer: Towards tracing text provenance with data hiding. In 2021 IEEE Symposium on Security and Privacy (SP). IEEE, Institute of Electrical and Electronics Engineers, Online, 121-140.",
                "links": null
            },
            "BIBREF1": {
                "ref_id": "b1",
                "title": "Natural language watermarking: Design, analysis, and a proof-ofconcept implementation",
                "authors": [
                    {
                        "first": "Victor",
                        "middle": [],
                        "last": "Mikhail J Atallah",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Raskin",
                        "suffix": ""
                    },
                    {
                        "first": "Christian",
                        "middle": [],
                        "last": "Crogan",
                        "suffix": ""
                    },
                    {
                        "first": "Florian",
                        "middle": [],
                        "last": "Hempelmann",
                        "suffix": ""
                    },
                    {
                        "first": "Dina",
                        "middle": [],
                        "last": "Kerschbaum",
                        "suffix": ""
                    },
                    {
                        "first": "Sanket",
                        "middle": [],
                        "last": "Mohamed",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Naik",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Information Hiding: 4th International Workshop",
                "volume": "4",
                "issue": "",
                "pages": "185--200",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Mikhail J Atallah, Victor Raskin, Michael Crogan, Christian Hempel- mann, Florian Kerschbaum, Dina Mohamed, and Sanket Naik. 2001. Natural language watermarking: Design, analysis, and a proof-of- concept implementation. In Information Hiding: 4th International Work- shop, IH 2001 Pittsburgh, PA, USA, April 25-27, 2001 Proceedings 4. Springer, Pittsburgh, 185-200.",
                "links": null
            },
            "BIBREF2": {
                "ref_id": "b2",
                "title": "Detecting Bot-Generated Text by Characterizing Linguistic Accommodation in Human-Bot Interactions",
                "authors": [
                    {
                        "first": "Paras",
                        "middle": [],
                        "last": "Bhatt",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [],
                        "last": "Rios",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021",
                "volume": "",
                "issue": "",
                "pages": "3235--3247",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Paras Bhatt and Anthony Rios. 2021. Detecting Bot-Generated Text by Characterizing Linguistic Accommodation in Human-Bot Interac- tions. In Findings of the Association for Computational Linguistics: ACL- IJCNLP 2021. Association for Computational Linguistics, Bangkok, Thailand, 3235-3247.",
                "links": null
            },
            "BIBREF3": {
                "ref_id": "b3",
                "title": "Electronic marking and identification techniques to discourage document copying",
                "authors": [
                    {
                        "first": "Steven",
                        "middle": [],
                        "last": "Jack T Brassil",
                        "suffix": ""
                    },
                    {
                        "first": "Nicholas",
                        "middle": [
                            "F"
                        ],
                        "last": "Low",
                        "suffix": ""
                    },
                    {
                        "first": "Lawrence O'",
                        "middle": [],
                        "last": "Maxemchuk",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Gorman",
                        "suffix": ""
                    }
                ],
                "year": 1995,
                "venue": "IEEE Journal on Selected Areas in Communications",
                "volume": "13",
                "issue": "",
                "pages": "1495--1504",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Jack T Brassil, Steven Low, Nicholas F. Maxemchuk, and Lawrence O'Gorman. 1995. Electronic marking and identification techniques to discourage document copying. IEEE Journal on Selected Areas in Communications 13, 8 (1995), 1495-1504.",
                "links": null
            },
            "BIBREF4": {
                "ref_id": "b4",
                "title": "An estimate of an upper bound for the entropy of English",
                "authors": [
                    {
                        "first": "Stephen",
                        "middle": [
                            "A"
                        ],
                        "last": "Peter F Brown",
                        "suffix": ""
                    },
                    {
                        "first": "Della",
                        "middle": [],
                        "last": "Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Vincent J Della",
                        "middle": [],
                        "last": "Pietra",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [
                            "C"
                        ],
                        "last": "Lai",
                        "suffix": ""
                    },
                    {
                        "first": "Robert",
                        "middle": [
                            "L"
                        ],
                        "last": "Mercer",
                        "suffix": ""
                    }
                ],
                "year": 1992,
                "venue": "Computational Linguistics",
                "volume": "18",
                "issue": "",
                "pages": "31--40",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Peter F Brown, Stephen A Della Pietra, Vincent J Della Pietra, Jennifer C Lai, and Robert L Mercer. 1992. An estimate of an upper bound for the entropy of English. Computational Linguistics 18, 1 (1992), 31-40.",
                "links": null
            },
            "BIBREF5": {
                "ref_id": "b5",
                "title": "All That's 'Human'Is Not Gold: Evaluating Human Evaluation of Generated Text",
                "authors": [
                    {
                        "first": "Elizabeth",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Tal",
                        "middle": [],
                        "last": "August",
                        "suffix": ""
                    },
                    {
                        "first": "Sofia",
                        "middle": [],
                        "last": "Serrano",
                        "suffix": ""
                    },
                    {
                        "first": "Nikita",
                        "middle": [],
                        "last": "Haduong",
                        "suffix": ""
                    },
                    {
                        "first": "Suchin",
                        "middle": [],
                        "last": "Gururangan",
                        "suffix": ""
                    },
                    {
                        "first": "Noah",
                        "middle": [
                            "A"
                        ],
                        "last": "Smith",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
                "volume": "1",
                "issue": "",
                "pages": "7282--7296",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A Smith. 2021. All That's 'Human'Is Not Gold: Evaluating Human Evaluation of Generated Text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguis- tics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Association for Computational Linguistics, Bangkok, Thailand, 7282-7296.",
                "links": null
            },
            "BIBREF6": {
                "ref_id": "b6",
                "title": "Adversarial Robustness of Neural-Statistical Features in Detection of Generative Transformers",
                "authors": [
                    {
                        "first": "Evan",
                        "middle": [],
                        "last": "Crothers",
                        "suffix": ""
                    },
                    {
                        "first": "Nathalie",
                        "middle": [],
                        "last": "Japkowicz",
                        "suffix": ""
                    },
                    {
                        "first": "Herna",
                        "middle": [],
                        "last": "Viktor",
                        "suffix": ""
                    },
                    {
                        "first": "Paula",
                        "middle": [],
                        "last": "Branco",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "2022 International Joint Conference on Neural Networks (IJCNN)",
                "volume": "",
                "issue": "",
                "pages": "1--8",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Evan Crothers, Nathalie Japkowicz, Herna Viktor, and Paula Branco. 2022. Adversarial Robustness of Neural-Statistical Features in Detec- tion of Generative Transformers. In 2022 International Joint Conference on Neural Networks (IJCNN). Institute of Electrical and Electronics Engineers, Padua, Italy, 1-8.",
                "links": null
            },
            "BIBREF7": {
                "ref_id": "b7",
                "title": "RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text",
                "authors": [
                    {
                        "first": "Liam",
                        "middle": [],
                        "last": "Dugan",
                        "suffix": ""
                    },
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "Arun",
                        "middle": [],
                        "last": "Kirubarajan",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "189--196",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liam Dugan, Daphne Ippolito, Arun Kirubarajan, and Chris Callison- Burch. 2020. RoFT: A Tool for Evaluating Human Detection of Machine- Generated Text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Asso- ciation for Computational Linguistics, online, 189-196.",
                "links": null
            },
            "BIBREF8": {
                "ref_id": "b8",
                "title": "Real or Fake Text? Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text",
                "authors": [
                    {
                        "first": "Liam",
                        "middle": [],
                        "last": "Dugan",
                        "suffix": ""
                    },
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "Arun",
                        "middle": [],
                        "last": "Kirubarajan",
                        "suffix": ""
                    },
                    {
                        "first": "Sherry",
                        "middle": [],
                        "last": "Shi",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Pei",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Andrew",
                        "middle": [],
                        "last": "Zhu",
                        "suffix": ""
                    },
                    {
                        "first": "Jennifer",
                        "middle": [],
                        "last": "Hu",
                        "suffix": ""
                    },
                    {
                        "first": "Jay",
                        "middle": [],
                        "last": "Pujara",
                        "suffix": ""
                    },
                    {
                        "first": "Xiang",
                        "middle": [],
                        "last": "Ren",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "The 37th AAAI Conference on Artificial Intelligence (AAAI 2023)",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Liam Dugan, Daphne Ippolito, Arun Kirubarajan, Sherry Shi, Chris Callison-Burch, Pei Zhou, Andrew Zhu, Jennifer Hu, Jay Pujara, Xiang Ren, et al. 2023. Real or Fake Text? Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text. In The 37th AAAI Conference on Artificial Intelligence (AAAI 2023). Association for the Advancement of Artificial Intelligence, Washington, USA, 104979.",
                "links": null
            },
            "BIBREF9": {
                "ref_id": "b9",
                "title": "College instructor put on blast for accusing students of using ChatGPT on final assignments",
                "authors": [
                    {
                        "first": "Uwa",
                        "middle": [],
                        "last": "Ede-Osifo",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Uwa Ede-Osifo. 2023. College instructor put on blast for accusing students of using ChatGPT on final assignments. Retrieved March 19, 2023 from https://www.nbcnews.com/tech/chatgpt-texas-college- instructor-backlash-rcna84888",
                "links": null
            },
            "BIBREF10": {
                "ref_id": "b10",
                "title": "NYC education department blocks ChatGPT on school devices, networks",
                "authors": [
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Elsen-Rooney",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Michael Elsen-Rooney. 2023. NYC education department blocks ChatGPT on school devices, networks. Retrieved Jan 25, 2023",
                "links": null
            },
            "BIBREF11": {
                "ref_id": "b11",
                "title": "TweepFake: About detecting deepfake tweets",
                "authors": [
                    {
                        "first": "Tiziano",
                        "middle": [],
                        "last": "Fagni",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrizio",
                        "middle": [],
                        "last": "Falchi",
                        "suffix": ""
                    },
                    {
                        "first": "Margherita",
                        "middle": [],
                        "last": "Gambini",
                        "suffix": ""
                    },
                    {
                        "first": "Antonio",
                        "middle": [],
                        "last": "Martella",
                        "suffix": ""
                    },
                    {
                        "first": "Maurizio",
                        "middle": [],
                        "last": "Tesconi",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "Plos one",
                "volume": "16",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Tiziano Fagni, Fabrizio Falchi, Margherita Gambini, Antonio Martella, and Maurizio Tesconi. 2021. TweepFake: About detecting deepfake tweets. Plos one 16, 5 (2021), e0251415.",
                "links": null
            },
            "BIBREF12": {
                "ref_id": "b12",
                "title": "ELI5: Long Form Question Answering",
                "authors": [
                    {
                        "first": "Angela",
                        "middle": [],
                        "last": "Fan",
                        "suffix": ""
                    },
                    {
                        "first": "Yacine",
                        "middle": [],
                        "last": "Jernite",
                        "suffix": ""
                    },
                    {
                        "first": "Ethan",
                        "middle": [],
                        "last": "Perez",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Grangier",
                        "suffix": ""
                    },
                    {
                        "first": "Jason",
                        "middle": [],
                        "last": "Weston",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Auli",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "3558--3567",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason We- ston, and Michael Auli. 2019. ELI5: Long Form Question Answering. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics. Association for Computational Linguistics, Florence, Italy, 3558-3567.",
                "links": null
            },
            "BIBREF13": {
                "ref_id": "b13",
                "title": "GPT-3: Its nature, scope, limits, and consequences. Minds and Machines",
                "authors": [
                    {
                        "first": "Luciano",
                        "middle": [],
                        "last": "Floridi",
                        "suffix": ""
                    },
                    {
                        "first": "Massimo",
                        "middle": [],
                        "last": "Chiriatti",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "",
                "volume": "30",
                "issue": "",
                "pages": "681--694",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines 30 (2020), 681-694.",
                "links": null
            },
            "BIBREF14": {
                "ref_id": "b14",
                "title": "Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover",
                "authors": [
                    {
                        "first": "Leon",
                        "middle": [],
                        "last": "Fr\u00f6hling",
                        "suffix": ""
                    },
                    {
                        "first": "Arkaitz",
                        "middle": [],
                        "last": "Zubiaga",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "PeerJ Computer Science",
                "volume": "7",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Leon Fr\u00f6hling and Arkaitz Zubiaga. 2021. Feature-based detection of automated language models: tackling GPT-2, GPT-3 and Grover. PeerJ Computer Science 7 (2021), e443.",
                "links": null
            },
            "BIBREF15": {
                "ref_id": "b15",
                "title": "Unsupervised and distributional detection of machine-generated text",
                "authors": [
                    {
                        "first": "Matthias",
                        "middle": [],
                        "last": "Gall\u00e9",
                        "suffix": ""
                    },
                    {
                        "first": "Jos",
                        "middle": [],
                        "last": "Rozen",
                        "suffix": ""
                    },
                    {
                        "first": "Germ\u00e1n",
                        "middle": [],
                        "last": "Kruszewski",
                        "suffix": ""
                    },
                    {
                        "first": "Hady",
                        "middle": [],
                        "last": "Elsahar",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2111.02878"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Matthias Gall\u00e9, Jos Rozen, Germ\u00e1n Kruszewski, and Hady Elsahar. 2021. Unsupervised and distributional detection of machine-generated text. arXiv preprint arXiv:2111.02878 (2021).",
                "links": null
            },
            "BIBREF16": {
                "ref_id": "b16",
                "title": "GLTR: Statistical Detection and Visualization of Generated Text",
                "authors": [
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Gehrmann",
                        "suffix": ""
                    },
                    {
                        "first": "Hendrik",
                        "middle": [],
                        "last": "Strobelt",
                        "suffix": ""
                    },
                    {
                        "first": "Alexander",
                        "middle": [
                            "M"
                        ],
                        "last": "Rush",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations",
                "volume": "",
                "issue": "",
                "pages": "111--116",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Sebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. 2019. GLTR: Statistical Detection and Visualization of Generated Text. In Proceedings of the 57th Annual Meeting of the Association for Computa- tional Linguistics: System Demonstrations. 111-116.",
                "links": null
            },
            "BIBREF17": {
                "ref_id": "b17",
                "title": "How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection",
                "authors": [
                    {
                        "first": "Biyang",
                        "middle": [],
                        "last": "Guo",
                        "suffix": ""
                    },
                    {
                        "first": "Xin",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Ziyuan",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Minqi",
                        "middle": [],
                        "last": "Jiang",
                        "suffix": ""
                    },
                    {
                        "first": "Jinran",
                        "middle": [],
                        "last": "Nie",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxuan",
                        "middle": [],
                        "last": "Ding",
                        "suffix": ""
                    },
                    {
                        "first": "Jianwei",
                        "middle": [],
                        "last": "Yue",
                        "suffix": ""
                    },
                    {
                        "first": "Yupeng",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2301.07597"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Biyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng Wu. 2023. How Close is ChatGPT to Human Experts? Comparison Corpus, Evaluation, and Detection. arXiv preprint arXiv:2301.07597 (2023).",
                "links": null
            },
            "BIBREF18": {
                "ref_id": "b18",
                "title": "The curious case of neural text degeneration",
                "authors": [
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Jan",
                        "middle": [],
                        "last": "Buys",
                        "suffix": ""
                    },
                    {
                        "first": "Li",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Maxwell",
                        "middle": [],
                        "last": "Forbes",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "1--16",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1904.09751"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751 (2019), 1-16.",
                "links": null
            },
            "BIBREF19": {
                "ref_id": "b19",
                "title": "Automatic Detection of Generated Text is Easiest when Humans are Fooled",
                "authors": [
                    {
                        "first": "Daphne",
                        "middle": [],
                        "last": "Ippolito",
                        "suffix": ""
                    },
                    {
                        "first": "Daniel",
                        "middle": [],
                        "last": "Duckworth",
                        "suffix": ""
                    },
                    {
                        "first": "Chris",
                        "middle": [],
                        "last": "Callison-Burch",
                        "suffix": ""
                    },
                    {
                        "first": "Douglas",
                        "middle": [],
                        "last": "Eck",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1808--1822",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Dou- glas Eck. 2020. Automatic Detection of Generated Text is Easiest when Humans are Fooled. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 1808-1822.",
                "links": null
            },
            "BIBREF20": {
                "ref_id": "b20",
                "title": "A review of digital watermarking techniques for text documents",
                "authors": [
                    {
                        "first": "Zunera",
                        "middle": [],
                        "last": "Jalil",
                        "suffix": ""
                    },
                    {
                        "first": "M",
                        "middle": [],
                        "last": "Anwar",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Mirza",
                        "suffix": ""
                    }
                ],
                "year": 2009,
                "venue": "2009 International Conference on Information and Multimedia Technology",
                "volume": "",
                "issue": "",
                "pages": "230--234",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Zunera Jalil and Anwar M Mirza. 2009. A review of digital watermark- ing techniques for text documents. In 2009 International Conference on Information and Multimedia Technology. IEEE, IEEE Computer Society, Washington DC, United States, 230-234.",
                "links": null
            },
            "BIBREF21": {
                "ref_id": "b21",
                "title": "Better malware ground truth: Techniques for weighting anti-virus vendor labels",
                "authors": [
                    {
                        "first": "Alex",
                        "middle": [],
                        "last": "Kantchelian",
                        "suffix": ""
                    },
                    {
                        "first": "Michael",
                        "middle": [],
                        "last": "Carl Tschantz",
                        "suffix": ""
                    },
                    {
                        "first": "Sadia",
                        "middle": [],
                        "last": "Afroz",
                        "suffix": ""
                    },
                    {
                        "first": "Brad",
                        "middle": [],
                        "last": "Miller",
                        "suffix": ""
                    },
                    {
                        "first": "Vaishaal",
                        "middle": [],
                        "last": "Shankar",
                        "suffix": ""
                    },
                    {
                        "first": "Rekha",
                        "middle": [],
                        "last": "Bachwani",
                        "suffix": ""
                    },
                    {
                        "first": "Anthony",
                        "middle": [
                            "D"
                        ],
                        "last": "Joseph",
                        "suffix": ""
                    },
                    {
                        "first": "Doug",
                        "middle": [],
                        "last": "Tygar",
                        "suffix": ""
                    }
                ],
                "year": 2015,
                "venue": "Proceedings of the 8th ACM Workshop on Artificial Intelligence and Security",
                "volume": "",
                "issue": "",
                "pages": "45--56",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Alex Kantchelian, Michael Carl Tschantz, Sadia Afroz, Brad Miller, Vaishaal Shankar, Rekha Bachwani, Anthony D Joseph, and J Doug Tygar. 2015. Better malware ground truth: Techniques for weighting anti-virus vendor labels. In Proceedings of the 8th ACM Workshop on Ar- tificial Intelligence and Security. Association for Computing Machinery, Denver, Colorado, USA, 45-56.",
                "links": null
            },
            "BIBREF22": {
                "ref_id": "b22",
                "title": "A Watermark for Large Language Models",
                "authors": [
                    {
                        "first": "John",
                        "middle": [],
                        "last": "Kirchenbauer",
                        "suffix": ""
                    },
                    {
                        "first": "Jonas",
                        "middle": [],
                        "last": "Geiping",
                        "suffix": ""
                    },
                    {
                        "first": "Yuxin",
                        "middle": [],
                        "last": "Wen",
                        "suffix": ""
                    },
                    {
                        "first": "Jonathan",
                        "middle": [],
                        "last": "Katz",
                        "suffix": ""
                    },
                    {
                        "first": "Ian",
                        "middle": [],
                        "last": "Miers",
                        "suffix": ""
                    },
                    {
                        "first": "Tom",
                        "middle": [],
                        "last": "Goldstein",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {
                    "arXiv": [
                        "arXiv:2301.10226"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. 2023. A Watermark for Large Language Models. arXiv preprint arXiv:2301.10226 (2023).",
                "links": null
            },
            "BIBREF23": {
                "ref_id": "b23",
                "title": "Roberta: A robustly optimized bert pretraining approach",
                "authors": [
                    {
                        "first": "Yinhan",
                        "middle": [],
                        "last": "Liu",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Naman",
                        "middle": [],
                        "last": "Goyal",
                        "suffix": ""
                    },
                    {
                        "first": "Jingfei",
                        "middle": [],
                        "last": "Du",
                        "suffix": ""
                    },
                    {
                        "first": "Mandar",
                        "middle": [],
                        "last": "Joshi",
                        "suffix": ""
                    },
                    {
                        "first": "Danqi",
                        "middle": [],
                        "last": "Chen",
                        "suffix": ""
                    },
                    {
                        "first": "Omer",
                        "middle": [],
                        "last": "Levy",
                        "suffix": ""
                    },
                    {
                        "first": "Mike",
                        "middle": [],
                        "last": "Lewis",
                        "suffix": ""
                    },
                    {
                        "first": "Luke",
                        "middle": [],
                        "last": "Zettlemoyer",
                        "suffix": ""
                    },
                    {
                        "first": "Veselin",
                        "middle": [],
                        "last": "Stoyanov",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "1--13",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1907.11692"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019), 1-13.",
                "links": null
            },
            "BIBREF24": {
                "ref_id": "b24",
                "title": "How Decoding Strategies Affect the Verifiability of Generated Text",
                "authors": [
                    {
                        "first": "Luca",
                        "middle": [],
                        "last": "Massarelli",
                        "suffix": ""
                    },
                    {
                        "first": "Fabio",
                        "middle": [],
                        "last": "Petroni",
                        "suffix": ""
                    },
                    {
                        "first": "Aleksandra",
                        "middle": [],
                        "last": "Piktus",
                        "suffix": ""
                    },
                    {
                        "first": "Myle",
                        "middle": [],
                        "last": "Ott",
                        "suffix": ""
                    },
                    {
                        "first": "Tim",
                        "middle": [],
                        "last": "Rockt\u00e4schel",
                        "suffix": ""
                    },
                    {
                        "first": "Vassilis",
                        "middle": [],
                        "last": "Plachouras",
                        "suffix": ""
                    },
                    {
                        "first": "Fabrizio",
                        "middle": [],
                        "last": "Silvestri",
                        "suffix": ""
                    },
                    {
                        "first": "Sebastian",
                        "middle": [],
                        "last": "Riedel",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020",
                "volume": "",
                "issue": "",
                "pages": "223--235",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rock- t\u00e4schel, Vassilis Plachouras, Fabrizio Silvestri, and Sebastian Riedel. 2020. How Decoding Strategies Affect the Verifiability of Gener- ated Text. In Findings of the Association for Computational Linguistics: EMNLP 2020. Association for Computational Linguistics, Online, 223- 235.",
                "links": null
            },
            "BIBREF25": {
                "ref_id": "b25",
                "title": "2021. gpt-2-output-dataset",
                "authors": [
                    {
                        "first": "",
                        "middle": [],
                        "last": "Openai",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "OpenAI. 2021. gpt-2-output-dataset. Retrieved March 19, 2023 from https://github.com/openai/gpt-2-output-dataset",
                "links": null
            },
            "BIBREF26": {
                "ref_id": "b26",
                "title": "Threat Scenarios and Best Practices to Detect Neural Fake News",
                "authors": [
                    {
                        "first": "Artidoro",
                        "middle": [],
                        "last": "Pagnoni",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Graciarena",
                        "suffix": ""
                    },
                    {
                        "first": "Yulia",
                        "middle": [],
                        "last": "Tsvetkov",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1233--1249",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Artidoro Pagnoni, Martin Graciarena, and Yulia Tsvetkov. 2022. Threat Scenarios and Best Practices to Detect Neural Fake News. In Proceed- ings of the 29th International Conference on Computational Linguistics. International Committee on Computational Linguistics, Gyeongju, Republic of Korea, 1233-1249.",
                "links": null
            },
            "BIBREF27": {
                "ref_id": "b27",
                "title": "Threat Scenarios and Best Practices to Detect Neural Fake News",
                "authors": [
                    {
                        "first": "Artidoro",
                        "middle": [],
                        "last": "Pagnoni",
                        "suffix": ""
                    },
                    {
                        "first": "Martin",
                        "middle": [],
                        "last": "Graciarena",
                        "suffix": ""
                    },
                    {
                        "first": "Yulia",
                        "middle": [],
                        "last": "Tsvetkov",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 29th International Conference on Computational Linguistics",
                "volume": "",
                "issue": "",
                "pages": "1233--1249",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Artidoro Pagnoni, Martin Graciarena, and Yulia Tsvetkov. 2022. Threat Scenarios and Best Practices to Detect Neural Fake News. In Proceed- ings of the 29th International Conference on Computational Linguistics. 1233-1249.",
                "links": null
            },
            "BIBREF28": {
                "ref_id": "b28",
                "title": "Zipf's word frequency law in natural language: A critical review and future directions",
                "authors": [
                    {
                        "first": "T",
                        "middle": [],
                        "last": "Steven",
                        "suffix": ""
                    },
                    {
                        "first": "",
                        "middle": [],
                        "last": "Piantadosi",
                        "suffix": ""
                    }
                ],
                "year": 2014,
                "venue": "Psychonomic bulletin & review",
                "volume": "21",
                "issue": "",
                "pages": "1112--1130",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Steven T Piantadosi. 2014. Zipf's word frequency law in natural language: A critical review and future directions. Psychonomic bulletin & review 21 (2014), 1112-1130.",
                "links": null
            },
            "BIBREF29": {
                "ref_id": "b29",
                "title": "Cross-Domain Detection of GPT-2-Generated Technical Text",
                "authors": [
                    {
                        "first": "Juan",
                        "middle": [],
                        "last": "Rodriguez",
                        "suffix": ""
                    },
                    {
                        "first": "Todd",
                        "middle": [],
                        "last": "Hay",
                        "suffix": ""
                    },
                    {
                        "first": "David",
                        "middle": [],
                        "last": "Gros",
                        "suffix": ""
                    },
                    {
                        "first": "Zain",
                        "middle": [],
                        "last": "Shamsi",
                        "suffix": ""
                    },
                    {
                        "first": "Ravi",
                        "middle": [],
                        "last": "Srinivasan",
                        "suffix": ""
                    }
                ],
                "year": 2022,
                "venue": "Proceedings of the 2022 Conference of the North American Chapter",
                "volume": "",
                "issue": "",
                "pages": "1213--1233",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Juan Rodriguez, Todd Hay, David Gros, Zain Shamsi, and Ravi Srini- vasan. 2022. Cross-Domain Detection of GPT-2-Generated Techni- cal Text. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Lan- guage Technologies. Association for Computational Linguistics, Seattle, United States, 1213-1233.",
                "links": null
            },
            "BIBREF30": {
                "ref_id": "b30",
                "title": "ChatGPT passes MBA exam given by a Wharton professor",
                "authors": [
                    {
                        "first": "Kalhan",
                        "middle": [],
                        "last": "Rosenblatt",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Kalhan Rosenblatt. 2023. ChatGPT passes MBA exam given by a Wharton professor. Retrieved Jan 25, 2023 from https://www.nbcnews.com/tech/tech-news/chatgpt-passes-mba- exam-wharton-professor-rcna67036",
                "links": null
            },
            "BIBREF32": {
                "ref_id": "b32",
                "title": "Fake news detection and fact verification using knowledge graphs and machine learning",
                "authors": [
                    {
                        "first": "Danish",
                        "middle": [],
                        "last": "Shakeel",
                        "suffix": ""
                    },
                    {
                        "first": "Nitin",
                        "middle": [],
                        "last": "Jain",
                        "suffix": ""
                    }
                ],
                "year": 2021,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "1--7",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Danish Shakeel and Nitin Jain. 2021. Fake news detection and fact ver- ification using knowledge graphs and machine learning. no. February (2021), 1-7.",
                "links": null
            },
            "BIBREF33": {
                "ref_id": "b33",
                "title": "Release strategies and the social impacts of language models",
                "authors": [
                    {
                        "first": "Irene",
                        "middle": [],
                        "last": "Solaiman",
                        "suffix": ""
                    },
                    {
                        "first": "Miles",
                        "middle": [],
                        "last": "Brundage",
                        "suffix": ""
                    },
                    {
                        "first": "Jack",
                        "middle": [],
                        "last": "Clark",
                        "suffix": ""
                    },
                    {
                        "first": "Amanda",
                        "middle": [],
                        "last": "Askell",
                        "suffix": ""
                    },
                    {
                        "first": "Ariel",
                        "middle": [],
                        "last": "Herbert-Voss",
                        "suffix": ""
                    },
                    {
                        "first": "Jeff",
                        "middle": [],
                        "last": "Wu",
                        "suffix": ""
                    },
                    {
                        "first": "Alec",
                        "middle": [],
                        "last": "Radford",
                        "suffix": ""
                    },
                    {
                        "first": "Gretchen",
                        "middle": [],
                        "last": "Krueger",
                        "suffix": ""
                    },
                    {
                        "first": "Jong",
                        "middle": [
                            "Wook"
                        ],
                        "last": "Kim",
                        "suffix": ""
                    },
                    {
                        "first": "Sarah",
                        "middle": [],
                        "last": "Kreps",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "1--46",
                "other_ids": {
                    "arXiv": [
                        "arXiv:1908.09203"
                    ]
                },
                "num": null,
                "urls": [],
                "raw_text": "Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec Radford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. 2019. Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203 (2019), 1-46.",
                "links": null
            },
            "BIBREF34": {
                "ref_id": "b34",
                "title": "Word reordering and a dynamic programming beam search algorithm for statistical machine translation",
                "authors": [
                    {
                        "first": "Christoph",
                        "middle": [],
                        "last": "Tillmann",
                        "suffix": ""
                    },
                    {
                        "first": "Hermann",
                        "middle": [],
                        "last": "Ney",
                        "suffix": ""
                    }
                ],
                "year": 2003,
                "venue": "Computational linguistics",
                "volume": "29",
                "issue": "",
                "pages": "97--133",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Christoph Tillmann and Hermann Ney. 2003. Word reordering and a dynamic programming beam search algorithm for statistical machine translation. Computational linguistics 29, 1 (2003), 97-133.",
                "links": null
            },
            "BIBREF35": {
                "ref_id": "b35",
                "title": "The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions",
                "authors": [
                    {
                        "first": "Umut",
                        "middle": [],
                        "last": "Topkara",
                        "suffix": ""
                    },
                    {
                        "first": "Mercan",
                        "middle": [],
                        "last": "Topkara",
                        "suffix": ""
                    },
                    {
                        "first": "Mikhail",
                        "middle": [
                            "J"
                        ],
                        "last": "Atallah",
                        "suffix": ""
                    }
                ],
                "year": 2006,
                "venue": "Proceedings of the 8th workshop on Multimedia and security",
                "volume": "",
                "issue": "",
                "pages": "164--174",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Umut Topkara, Mercan Topkara, and Mikhail J Atallah. 2006. The hid- ing virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In Proceedings of the 8th workshop on Multimedia and security. Association for Computing Machinery, Geneva, Switzerland, 164-174.",
                "links": null
            },
            "BIBREF36": {
                "ref_id": "b36",
                "title": "TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation",
                "authors": [
                    {
                        "first": "Adaku",
                        "middle": [],
                        "last": "Uchendu",
                        "suffix": ""
                    },
                    {
                        "first": "Zeyu",
                        "middle": [],
                        "last": "Ma",
                        "suffix": ""
                    },
                    {
                        "first": "Thai",
                        "middle": [],
                        "last": "Le",
                        "suffix": ""
                    },
                    {
                        "first": "Rui",
                        "middle": [],
                        "last": "Zhang",
                        "suffix": ""
                    },
                    {
                        "first": "Dongwon",
                        "middle": [],
                        "last": "Lee",
                        "suffix": ""
                    }
                ],
                "year": 2001,
                "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, and Dongwon Lee. 2021. TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation. In Findings of the Association for Computational Linguistics: EMNLP 2021. 2001-2016.",
                "links": null
            },
            "BIBREF37": {
                "ref_id": "b37",
                "title": "Meta's powerful AI language model has leaked online -what happens now",
                "authors": [
                    {
                        "first": "James",
                        "middle": [],
                        "last": "Vincent",
                        "suffix": ""
                    }
                ],
                "year": 2023,
                "venue": "",
                "volume": "",
                "issue": "",
                "pages": "",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "James Vincent. 2023. Meta's powerful AI language model has leaked online -what happens now? Retrieved March 19, 2023 from https://www.theverge.com/2023/3/8/23629362/meta-ai- language-model-llama-leak-online-misuse",
                "links": null
            },
            "BIBREF38": {
                "ref_id": "b38",
                "title": "Defending against neural fake news",
                "authors": [
                    {
                        "first": "Rowan",
                        "middle": [],
                        "last": "Zellers",
                        "suffix": ""
                    },
                    {
                        "first": "Ari",
                        "middle": [],
                        "last": "Holtzman",
                        "suffix": ""
                    },
                    {
                        "first": "Hannah",
                        "middle": [],
                        "last": "Rashkin",
                        "suffix": ""
                    },
                    {
                        "first": "Yonatan",
                        "middle": [],
                        "last": "Bisk",
                        "suffix": ""
                    },
                    {
                        "first": "Ali",
                        "middle": [],
                        "last": "Farhadi",
                        "suffix": ""
                    },
                    {
                        "first": "Franziska",
                        "middle": [],
                        "last": "Roesner",
                        "suffix": ""
                    },
                    {
                        "first": "Yejin",
                        "middle": [],
                        "last": "Choi",
                        "suffix": ""
                    }
                ],
                "year": 2019,
                "venue": "Advances in neural information processing systems",
                "volume": "32",
                "issue": "",
                "pages": "9054--9065",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. Advances in neural information processing systems 32 (2019), 9054-9065.",
                "links": null
            },
            "BIBREF39": {
                "ref_id": "b39",
                "title": "Neural Deepfake Detection with Factual Structure of Text",
                "authors": [
                    {
                        "first": "Wanjun",
                        "middle": [],
                        "last": "Zhong",
                        "suffix": ""
                    },
                    {
                        "first": "Duyu",
                        "middle": [],
                        "last": "Tang",
                        "suffix": ""
                    },
                    {
                        "first": "Zenan",
                        "middle": [],
                        "last": "Xu",
                        "suffix": ""
                    },
                    {
                        "first": "Ruize",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Nan",
                        "middle": [],
                        "last": "Duan",
                        "suffix": ""
                    },
                    {
                        "first": "Ming",
                        "middle": [],
                        "last": "Zhou",
                        "suffix": ""
                    },
                    {
                        "first": "Jiahai",
                        "middle": [],
                        "last": "Wang",
                        "suffix": ""
                    },
                    {
                        "first": "Jian",
                        "middle": [],
                        "last": "Yin",
                        "suffix": ""
                    }
                ],
                "year": 2020,
                "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
                "volume": "",
                "issue": "",
                "pages": "2461--2470",
                "other_ids": {},
                "num": null,
                "urls": [],
                "raw_text": "Wanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, and Jian Yin. 2020. Neural Deepfake Detection with Factual Structure of Text. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Minneapolis, Minnesota, 2461-2470.",
                "links": null
            }
        },
        "ref_entries": {
            "FIGREF0": {
                "uris": null,
                "fig_num": "1",
                "text": "Figure 1. An overview of the LLM-generated text detection.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF1": {
                "uris": null,
                "fig_num": "12",
                "text": "Figure 2. Visualization results of GLTR[17], the word ranking is obtained from the GPT-2 small model. Words that rank within the top 10 are highlighted in green, top 100 in yellow, top 1,000 in red, and the rest in purple. There is a notable difference between the two texts. The human-authored texts are from Chalkbeat New York[11].",
                "type_str": "figure",
                "num": null
            },
            "FIGREF2": {
                "uris": null,
                "fig_num": "313",
                "text": "Figure 3. A taxonomy of LLM-generated text detection.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF3": {
                "uris": null,
                "fig_num": "4",
                "text": "Figure 4. Illustration of inference time watermark.A random seed is generated by hashing the previously predicted token \"a\", splitting the whole vocabulary into \"green list\" and \"red list\". The next token \"carpet\" is chosen from the green list.",
                "type_str": "figure",
                "num": null
            },
            "FIGREF4": {
                "uris": null,
                "fig_num": "5",
                "text": "Figure 5. Illustration of paraphrasing attack.",
                "type_str": "figure",
                "num": null
            }
        }
    }
}